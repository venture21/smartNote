"""
ì˜ìƒ/ì˜¤ë””ì˜¤ ê²€ìƒ‰ ì—”ì§„

ë‘ ê°€ì§€ ëª¨ë“œ ì œê³µ:
1. ì˜ìƒ ê²€ìƒ‰ ì—”ì§„: YouTube ë§í¬ ì…ë ¥ â†’ ë‹¤ìš´ë¡œë“œ â†’ STT â†’ íšŒì˜ë¡
2. ì˜¤ë””ì˜¤ ê²€ìƒ‰ ì—”ì§„: ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ â†’ STT â†’ íšŒì˜ë¡

ê¸°ëŠ¥:
- YouTube ì˜ìƒ ë‹¤ìš´ë¡œë“œ ë° MP3 ë³€í™˜
- ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ ì§€ì› (mp3, wav, m4a, flac, ogg)
- STT (Gemini)
- VectorStore ê¸°ë°˜ íšŒì˜ë¡ ì €ì¥ ë° ê²€ìƒ‰ (ChromaDB + Gemini Embedding)
- íšŒì˜ë¡ ìš”ì•½ ë° AI ì±„íŒ… (RAG ê¸°ë°˜)
- CSVë¡œ ì‘ì—… ì´ë ¥ ê´€ë¦¬ (ìºì‹±)
"""

from flask import Flask, render_template, request, jsonify, send_from_directory
import os
import pandas as pd
import json
import subprocess
from werkzeug.utils import secure_filename
from dotenv import load_dotenv
from google import genai
from google.genai import types
import secrets
import yt_dlp
import logging
from datetime import datetime
import threading
import time
import hashlib
from mutagen import File as MutagenFile
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from langchain_community.vectorstores.utils import filter_complex_metadata
from langchain_text_splitters import RecursiveCharacterTextSplitter

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Flask ì•± ìƒì„± (main.pyì—ì„œ ì¬ì‚¬ìš©)
app = Flask(__name__)
app.secret_key = secrets.token_hex(16)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')

# ì„¤ì •
MP4_FOLDER = "mp4"
MP3_FOLDER = "mp3"
CSV_FOLDER = "csv"
UPLOADS_FOLDER = "uploads"
CHROMA_DB_FOLDER = "chroma_db"
YOUTUBE_HISTORY_CSV = os.path.join(CSV_FOLDER, "youtube_history.csv")
AUDIO_HISTORY_CSV = os.path.join(CSV_FOLDER, "audio_history.csv")
STT_PROCESSING_LOG = os.path.join(CSV_FOLDER, "stt_processing_log.json")

app.config["MAX_CONTENT_LENGTH"] = 500 * 1024 * 1024  # 500MB max

# í—ˆìš©ëœ ì˜¤ë””ì˜¤ íŒŒì¼ í™•ì¥ì
ALLOWED_AUDIO_EXTENSIONS = {
    "mp3",
    "wav",
    "m4a",
    "flac",
    "ogg",
    "mp4",
    "avi",
    "mov",
    "mkv",
}

# í´ë” ìƒì„±
for folder in [MP4_FOLDER, MP3_FOLDER, CSV_FOLDER, UPLOADS_FOLDER, CHROMA_DB_FOLDER]:
    os.makedirs(folder, exist_ok=True)

# ì„¸ì…˜ë³„ ë°ì´í„° ì €ì¥
session_data = {}

# ì§„í–‰ ìƒí™© ì €ì¥
progress_data = {}

# LangChain Embeddings ì´ˆê¸°í™”
embeddings = None

# LangChain VectorStore (YouTube, Audio, Summary ë¶„ë¦¬)
youtube_vectorstore = None
audio_vectorstore = None
summary_vectorstore = None


def allowed_file(filename):
    """í—ˆìš©ëœ íŒŒì¼ í™•ì¥ìì¸ì§€ í™•ì¸"""
    return (
        "." in filename
        and filename.rsplit(".", 1)[1].lower() in ALLOWED_AUDIO_EXTENSIONS
    )


def calculate_file_hash(file_path):
    """íŒŒì¼ì˜ MD5 í•´ì‹œë¥¼ ê³„ì‚°í•˜ì—¬ ê³ ìœ  ID ìƒì„±"""
    hasher = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hasher.update(chunk)
    return hasher.hexdigest()


def get_audio_duration(file_path):
    """ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ê¸¸ì´ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë°˜í™˜"""
    try:
        audio = MutagenFile(file_path)
        if audio is not None and hasattr(audio.info, "length"):
            duration = audio.info.length
            logging.info(f"ğŸµ ì˜¤ë””ì˜¤ íŒŒì¼ ê¸¸ì´: {duration:.2f}ì´ˆ ({duration/60:.2f}ë¶„)")
            return duration
        else:
            logging.warning(f"âš ï¸ ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return 0.0
    except Exception as e:
        logging.error(f"âŒ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return 0.0


def update_progress(task_id, step, progress, message, estimated_time=None, elapsed_time=None):
    """
    ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ (ì‹œê°„ ì •ë³´ í¬í•¨)

    Args:
        task_id: ì‘ì—… ID
        step: ë‹¨ê³„ (ì˜ˆ: 'stt', 'download', 'vectorstore')
        progress: ì§„í–‰ë¥  (0-100)
        message: ë©”ì‹œì§€
        estimated_time: ì˜ˆìƒ ì†Œìš” ì‹œê°„ (ì´ˆ)
        elapsed_time: ì‹¤ì œ ê²½ê³¼ ì‹œê°„ (ì´ˆ)
    """
    if task_id not in progress_data:
        progress_data[task_id] = {}

    progress_data[task_id][step] = {
        "progress": progress,
        "message": message,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    }

    # ì‹œê°„ ì •ë³´ ì¶”ê°€
    if estimated_time is not None:
        progress_data[task_id][step]["estimated_time"] = estimated_time

    if elapsed_time is not None:
        progress_data[task_id][step]["elapsed_time"] = elapsed_time

        # ë‚¨ì€ ì˜ˆìƒ ì‹œê°„ ê³„ì‚°
        if estimated_time is not None:
            remaining_time = max(0, estimated_time - elapsed_time)
            progress_data[task_id][step]["remaining_time"] = remaining_time

    # ë¡œê¹… (ì‹œê°„ ì •ë³´ í¬í•¨)
    log_msg = f"[{task_id}] {step}: {progress}% - {message}"
    if estimated_time is not None and elapsed_time is not None:
        remaining = max(0, estimated_time - elapsed_time)
        log_msg += f" (ì˜ˆìƒ: {estimated_time:.1f}ì´ˆ, ê²½ê³¼: {elapsed_time:.1f}ì´ˆ, ë‚¨ìŒ: {remaining:.1f}ì´ˆ)"
    logging.info(log_msg)


# YouTube ì´ë ¥ ë¡œë“œ
def load_youtube_history():
    """SQLiteì—ì„œ YouTube ë‹¤ìš´ë¡œë“œ ì´ë ¥ì„ ë¡œë“œí•©ë‹ˆë‹¤ (modules/database.py ì‚¬ìš©)"""
    from modules.database import load_youtube_history as db_load_youtube
    return db_load_youtube()


def save_youtube_history(df):
    """YouTube ì´ë ¥ì„ SQLiteì— ì €ì¥í•©ë‹ˆë‹¤ (modules/database.py ì‚¬ìš©)"""
    from modules.database import save_youtube_history as db_save_youtube
    db_save_youtube(df)


# ì˜¤ë””ì˜¤ ì´ë ¥ ë¡œë“œ
def load_audio_history():
    """SQLiteì—ì„œ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì´ë ¥ì„ ë¡œë“œí•©ë‹ˆë‹¤ (modules/database.py ì‚¬ìš©)"""
    from modules.database import load_audio_history as db_load_audio
    return db_load_audio()


def save_audio_history(df):
    """ì˜¤ë””ì˜¤ ì´ë ¥ì„ SQLiteì— ì €ì¥í•©ë‹ˆë‹¤ (modules/database.py ì‚¬ìš©)"""
    from modules.database import save_audio_history as db_save_audio
    db_save_audio(df)


def load_stt_processing_log():
    """STT ì²˜ë¦¬ ì‹œê°„ ë¡œê·¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if os.path.exists(STT_PROCESSING_LOG):
        try:
            with open(STT_PROCESSING_LOG, 'r', encoding='utf-8') as f:
                logs = json.load(f)
            return logs
        except Exception as e:
            logging.error(f"STT ë¡œê·¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return []
    else:
        return []


def save_stt_processing_log(logs):
    """STT ì²˜ë¦¬ ì‹œê°„ ë¡œê·¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        with open(STT_PROCESSING_LOG, 'w', encoding='utf-8') as f:
            json.dump(logs, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logging.error(f"STT ë¡œê·¸ ì €ì¥ ì˜¤ë¥˜: {e}")


def add_stt_processing_record(audio_duration, processing_time, source_type="audio"):
    """
    STT ì²˜ë¦¬ ê¸°ë¡ì„ ë¡œê·¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.

    Args:
        audio_duration: ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
        processing_time: ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
        source_type: ì†ŒìŠ¤ íƒ€ì… ("audio" ë˜ëŠ” "youtube")
    """
    logs = load_stt_processing_log()

    # ì²˜ë¦¬ ë¹„ìœ¨ ê³„ì‚°
    ratio = processing_time / audio_duration if audio_duration > 0 else 0

    # ìƒˆ ê¸°ë¡ ì¶”ê°€ (ë” ë§ì€ ë©”íƒ€ë°ì´í„°)
    logs.append({
        "audio_duration": float(audio_duration),
        "processing_time": float(processing_time),
        "ratio": float(ratio),
        "source_type": source_type,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    })

    # ìµœê·¼ 200ê°œë§Œ ìœ ì§€ (ë” ë§ì€ ë°ì´í„°ë¡œ ì •í™•ë„ í–¥ìƒ)
    if len(logs) > 200:
        logs = logs[-200:]

    save_stt_processing_log(logs)
    logging.info(f"ğŸ“Š STT ì²˜ë¦¬ ê¸°ë¡ ì¶”ê°€: {audio_duration:.2f}ì´ˆ â†’ {processing_time:.2f}ì´ˆ (ë¹„ìœ¨: {ratio:.3f})")


def estimate_stt_processing_time(audio_duration):
    """
    ê³¼ê±° ë¡œê·¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ STT ì²˜ë¦¬ ì‹œê°„ì„ ì •í™•íˆ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

    ê°œì„  ì‚¬í•­:
    - ê°€ì¤‘ í‰ê· : ìµœê·¼ ë°ì´í„°ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    - ì´ìƒì¹˜ ì œê±°: í‘œì¤€í¸ì°¨ ê¸°ë°˜ í•„í„°ë§
    - êµ¬ê°„ë³„ ë¶„ì„: ì˜¤ë””ì˜¤ ê¸¸ì´ë³„ë¡œ ë‹¤ë¥¸ ë¹„ìœ¨ ì ìš©

    Args:
        audio_duration: ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)

    Returns:
        ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
    """
    logs = load_stt_processing_log()

    if not logs:
        # ë¡œê·¸ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’: ì˜¤ë””ì˜¤ ê¸¸ì´ì˜ 20% (ê²½í—˜ì  ì¶”ì •)
        estimated = audio_duration * 0.2
        logging.info(f"â±ï¸ STT ì˜ˆìƒ ì‹œê°„ (ê¸°ë³¸ê°’): {estimated:.2f}ì´ˆ")
        return estimated

    # 1. ì˜¤ë””ì˜¤ ê¸¸ì´ë³„ êµ¬ê°„ ë¶„ë¥˜
    # - ì§§ì€ ì˜¤ë””ì˜¤: 0~300ì´ˆ (5ë¶„)
    # - ì¤‘ê°„ ì˜¤ë””ì˜¤: 300~900ì´ˆ (5~15ë¶„)
    # - ê¸´ ì˜¤ë””ì˜¤: 900ì´ˆ ì´ìƒ (15ë¶„ ì´ìƒ)
    if audio_duration < 300:
        duration_range = "short"
        target_logs = [log for log in logs if log.get("audio_duration", 0) < 300]
    elif audio_duration < 900:
        duration_range = "medium"
        target_logs = [log for log in logs if 300 <= log.get("audio_duration", 0) < 900]
    else:
        duration_range = "long"
        target_logs = [log for log in logs if log.get("audio_duration", 0) >= 900]

    # êµ¬ê°„ë³„ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©
    if len(target_logs) < 5:
        target_logs = logs
        logging.info(f"â±ï¸ êµ¬ê°„ë³„ ë°ì´í„° ë¶€ì¡±, ì „ì²´ ë¡œê·¸ ì‚¬ìš© ({len(logs)}ê°œ)")

    # 2. ìµœê·¼ ë°ì´í„°ë§Œ ì„ íƒ (ìµœëŒ€ 50ê°œ)
    recent_logs = target_logs[-50:]

    # 3. ë¹„ìœ¨ ì¶”ì¶œ ë° ì´ìƒì¹˜ ì œê±°
    ratios = []
    for log in recent_logs:
        audio_dur = log.get("audio_duration", 0)
        proc_time = log.get("processing_time", 0)

        if audio_dur > 0:
            # ê¸°ì¡´ ratio í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê³„ì‚°
            ratio = log.get("ratio", proc_time / audio_dur)
            ratios.append(ratio)

    if not ratios:
        # ë¹„ìœ¨ ê³„ì‚° ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
        estimated = audio_duration * 0.2
        logging.info(f"â±ï¸ STT ì˜ˆìƒ ì‹œê°„ (ê¸°ë³¸ê°’): {estimated:.2f}ì´ˆ")
        return estimated

    # 4. ì´ìƒì¹˜(outlier) ì œê±° (í‘œì¤€í¸ì°¨ ê¸°ë°˜)
    import statistics

    if len(ratios) >= 3:
        mean_ratio = statistics.mean(ratios)
        stdev_ratio = statistics.stdev(ratios)

        # í‰ê·  Â± 2 í‘œì¤€í¸ì°¨ ë²”ìœ„ ë‚´ì˜ ê°’ë§Œ ì‚¬ìš©
        filtered_ratios = [
            r for r in ratios
            if abs(r - mean_ratio) <= 2 * stdev_ratio
        ]

        if filtered_ratios:
            ratios = filtered_ratios
            logging.info(f"ğŸ“Š ì´ìƒì¹˜ ì œê±°: {len(recent_logs)}ê°œ â†’ {len(ratios)}ê°œ")

    # 5. ê°€ì¤‘ í‰ê·  ê³„ì‚° (ìµœê·¼ ë°ì´í„°ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
    weights = []
    weighted_sum = 0
    weight_total = 0

    for i, ratio in enumerate(ratios):
        # ì§€ìˆ˜ ê°€ì¤‘ì¹˜: ìµœê·¼ ë°ì´í„°ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜ (1.0 ~ 2.0)
        weight = 1.0 + (i / len(ratios))  # ì²« ë²ˆì§¸: 1.0, ë§ˆì§€ë§‰: 2.0
        weighted_sum += ratio * weight
        weight_total += weight
        weights.append(weight)

    weighted_avg_ratio = weighted_sum / weight_total if weight_total > 0 else 0.2

    # 6. ì˜ˆìƒ ì‹œê°„ ê³„ì‚°
    estimated = audio_duration * weighted_avg_ratio

    # 7. ì˜ˆì¸¡ ì‹ ë¢°ë„ ê³„ì‚°
    if len(ratios) >= 3:
        stdev = statistics.stdev(ratios)
        confidence = max(0, 100 - (stdev * 100))  # í‘œì¤€í¸ì°¨ê°€ ë‚®ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ
    else:
        confidence = 50  # ë°ì´í„° ë¶€ì¡± ì‹œ ì¤‘ê°„ ì‹ ë¢°ë„

    logging.info(
        f"â±ï¸ STT ì˜ˆìƒ ì‹œê°„: {estimated:.2f}ì´ˆ "
        f"(êµ¬ê°„: {duration_range}, ìƒ˜í”Œ: {len(ratios)}ê°œ, "
        f"ê°€ì¤‘í‰ê·  ë¹„ìœ¨: {weighted_avg_ratio:.3f}, ì‹ ë¢°ë„: {confidence:.0f}%)"
    )

    return estimated


def analyze_stt_prediction_accuracy():
    """
    STT ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

    Returns:
        dict: í†µê³„ ì •ë³´ (í‰ê·  ì˜¤ì°¨ìœ¨, í‘œì¤€í¸ì°¨ ë“±)
    """
    logs = load_stt_processing_log()

    if len(logs) < 5:
        return {
            "total_records": len(logs),
            "message": "ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 5ê°œ í•„ìš”)"
        }

    import statistics

    # ê° êµ¬ê°„ë³„ í†µê³„
    stats_by_range = {
        "short": {"ratios": [], "errors": []},   # 0~5ë¶„
        "medium": {"ratios": [], "errors": []},  # 5~15ë¶„
        "long": {"ratios": [], "errors": []}     # 15ë¶„ ì´ìƒ
    }

    all_ratios = []

    for log in logs:
        audio_dur = log.get("audio_duration", 0)
        proc_time = log.get("processing_time", 0)

        if audio_dur > 0:
            ratio = log.get("ratio", proc_time / audio_dur)
            all_ratios.append(ratio)

            # êµ¬ê°„ ë¶„ë¥˜
            if audio_dur < 300:
                duration_range = "short"
            elif audio_dur < 900:
                duration_range = "medium"
            else:
                duration_range = "long"

            stats_by_range[duration_range]["ratios"].append(ratio)

    # ì „ì²´ í†µê³„
    if all_ratios:
        mean_ratio = statistics.mean(all_ratios)
        median_ratio = statistics.median(all_ratios)
        stdev_ratio = statistics.stdev(all_ratios) if len(all_ratios) >= 2 else 0

        result = {
            "total_records": len(logs),
            "overall": {
                "mean_ratio": round(mean_ratio, 4),
                "median_ratio": round(median_ratio, 4),
                "stdev_ratio": round(stdev_ratio, 4),
                "min_ratio": round(min(all_ratios), 4),
                "max_ratio": round(max(all_ratios), 4),
            },
            "by_duration": {}
        }

        # êµ¬ê°„ë³„ í†µê³„
        for duration_range, data in stats_by_range.items():
            ratios = data["ratios"]
            if len(ratios) >= 2:
                result["by_duration"][duration_range] = {
                    "count": len(ratios),
                    "mean_ratio": round(statistics.mean(ratios), 4),
                    "median_ratio": round(statistics.median(ratios), 4),
                    "stdev_ratio": round(statistics.stdev(ratios), 4),
                }
            elif len(ratios) == 1:
                result["by_duration"][duration_range] = {
                    "count": 1,
                    "mean_ratio": round(ratios[0], 4),
                    "median_ratio": round(ratios[0], 4),
                    "stdev_ratio": 0,
                }

        return result
    else:
        return {
            "total_records": len(logs),
            "message": "ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
        }


def get_gemini_client():
    """Gemini í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    api_key = os.environ.get("GOOGLE_API_KEY")
    if api_key:
        return genai.Client(api_key=api_key)
    else:
        return genai.Client()


def initialize_collections():
    """LangChain VectorStore ì´ˆê¸°í™” (OpenAI Embeddings ì‚¬ìš©)"""
    global embeddings, youtube_vectorstore, audio_vectorstore, summary_vectorstore

    try:
        # OpenAI Embeddings ì´ˆê¸°í™”
        openai_api_key = os.environ.get("OPENAI_API_KEY")
        embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small", openai_api_key=openai_api_key
        )
        logging.info("âœ… OpenAI Embeddings ì‚¬ìš©")

        # YouTube VectorStore
        youtube_vectorstore = Chroma(
            collection_name="youtube_transcripts",
            embedding_function=embeddings,
            persist_directory=CHROMA_DB_FOLDER,
        )

        # Audio VectorStore
        audio_vectorstore = Chroma(
            collection_name="audio_transcripts",
            embedding_function=embeddings,
            persist_directory=CHROMA_DB_FOLDER,
        )

        # Summary VectorStore (ë³„ë„ ì €ì¥ì†Œ)
        summary_vectorstore = Chroma(
            collection_name="summaries",
            embedding_function=embeddings,
            persist_directory=CHROMA_DB_FOLDER,
        )

        logging.info(f"âœ… LangChain VectorStore ì´ˆê¸°í™” ì™„ë£Œ")
        logging.info(f"   - YouTube VectorStore ì´ˆê¸°í™”ë¨")
        logging.info(f"   - Audio VectorStore ì´ˆê¸°í™”ë¨")
        logging.info(f"   - Summary VectorStore ì´ˆê¸°í™”ë¨")
    except Exception as e:
        logging.error(f"âŒ LangChain VectorStore ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()


def store_segments_in_vectordb(
    segments, source_id, source_type="youtube", filename=None, title=None, use_chunking=True, chunk_size=500, chunk_overlap=100
):
    """
    ì„¸ê·¸ë¨¼íŠ¸ë¥¼ VectorDBì— ì €ì¥ (LangChain ë°©ì‹)

    Args:
        segments: STTë¡œ ì¶”ì¶œëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
        source_id: YouTube video_id ë˜ëŠ” audio file_hash
        source_type: "youtube" ë˜ëŠ” "audio"
        filename: ì˜¤ë””ì˜¤ íŒŒì¼ëª… (ì˜¤ë””ì˜¤ì¼ ê²½ìš°)
        title: ì œëª© (ì‚¬ìš©ì ì…ë ¥ ë˜ëŠ” ìë™ ì¶”ì¶œ)
        use_chunking: Trueì´ë©´ í† í° ê¸°ë°˜ ì²­í‚¹ ì‚¬ìš©, Falseì´ë©´ ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥ (ê¸°ë³¸ê°’: True)
        chunk_size: ì²­í‚¹ ì‹œ chunkë‹¹ ìµœëŒ€ ë¬¸ì ìˆ˜ (ê¸°ë³¸ê°’: 500)
        chunk_overlap: ì²­í‚¹ ì‹œ chunk ê°„ ì¤‘ë³µ ë¬¸ì ìˆ˜ (ê¸°ë³¸ê°’: 100)
    """
    try:
        vectorstore = (
            youtube_vectorstore if source_type == "youtube" else audio_vectorstore
        )

        if not vectorstore:
            logging.error("âŒ LangChain VectorStoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False

        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ê°™ì€ source_id)
        try:
            # LangChain Chromaì—ì„œ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            existing_docs = vectorstore.get(where={"source_id": source_id})
            if existing_docs and existing_docs["ids"]:
                vectorstore.delete(ids=existing_docs["ids"])
                logging.info(
                    f"ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ: {len(existing_docs['ids'])}ê°œ ë¬¸ì„œ"
                )
        except Exception as e:
            logging.warning(f"ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

        # ì²­í‚¹ ì—¬ë¶€ì— ë”°ë¼ ì²˜ë¦¬
        if use_chunking:
            logging.info(f"ğŸ“¦ í† í° ê¸°ë°˜ ì²­í‚¹ ì‹œì‘ (chunk_size={chunk_size}, overlap={chunk_overlap})...")
            chunks = create_token_based_chunks(segments, chunk_size=chunk_size, chunk_overlap=chunk_overlap)

            if not chunks:
                logging.warning("âš ï¸ ì²­í‚¹ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ, ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤.")
                use_chunking = False  # í´ë°±: ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥
            else:
                # LangChain Document ê°ì²´ ìƒì„± (ì²­í¬ ê¸°ë°˜)
                documents = []
                doc_ids = []

                for chunk in chunks:
                    # Document (content)
                    text = chunk["text"]

                    # Metadata
                    metadata = {
                        "source_id": source_id,
                        "source_type": source_type,
                        "document_type": "chunk",  # ì²­í¬ì„ì„ í‘œì‹œ
                        "chunk_id": int(chunk["chunk_id"]),
                        "segment_ids": chunk["segment_ids"],  # ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ID ë¦¬ìŠ¤íŠ¸ (ë³µì¡í•œ ë©”íƒ€ë°ì´í„°)
                        "speakers": chunk["speakers"],  # í™”ì ë¦¬ìŠ¤íŠ¸ (ë³µì¡í•œ ë©”íƒ€ë°ì´í„°)
                        "start_time": float(chunk["start_time"]),
                        "end_time": float(chunk["end_time"]) if chunk["end_time"] is not None else None,
                        "confidence": float(chunk["confidence"]),
                    }

                    # ì œëª© ì¶”ê°€
                    if title:
                        metadata["title"] = title

                    if source_type == "audio" and filename:
                        metadata["filename"] = filename

                    # ID: source_id + chunk_id
                    doc_id = f"{source_id}_chunk_{chunk['chunk_id']}"
                    doc_ids.append(doc_id)

                    # LangChain Document ìƒì„±
                    doc = Document(page_content=text, metadata=metadata)
                    documents.append(doc)

                # ë³µì¡í•œ ë©”íƒ€ë°ì´í„° í•„í„°ë§ (segment_ids, speakersëŠ” ë¦¬ìŠ¤íŠ¸)
                logging.info(f"ğŸ”§ ë³µì¡í•œ ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì¤‘... (Document ìˆ˜: {len(documents)})")
                filtered_documents = filter_complex_metadata(documents)

                # LangChain VectorStoreì— ì €ì¥ (ìë™ìœ¼ë¡œ ì„ë² ë”© ìƒì„±ë¨)
                vectorstore.add_documents(
                    documents=filtered_documents,
                    ids=doc_ids,
                )

                logging.info(
                    f"âœ… VectorDB ì €ì¥ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ (ì›ë³¸ {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸, source: {source_id})"
                )
                return True

        # ì²­í‚¹ ë¯¸ì‚¬ìš© ë˜ëŠ” í´ë°±: ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥
        if not use_chunking:
            documents = []

            for idx, segment in enumerate(segments):
                # Document (content)
                text = segment["text"]

                # end_time ê³„ì‚° (ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ì˜ start_time ë˜ëŠ” None)
                if idx < len(segments) - 1:
                    end_time = float(segments[idx + 1]["start_time"])
                else:
                    # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ëŠ” end_timeì´ ì—†ìŒ (None)
                    end_time = None

                # Metadata
                metadata = {
                    "source_id": source_id,
                    "source_type": source_type,
                    "document_type": "segment",  # ëª…ì‹œì ìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ì„ì„ í‘œì‹œ
                    "speaker": str(segment["speaker"]),
                    "start_time": float(segment["start_time"]),
                    "end_time": end_time,
                    "confidence": float(segment.get("confidence", 0.0)),
                    "segment_id": int(segment["id"]),
                }

                # ì œëª© ì¶”ê°€
                if title:
                    metadata["title"] = title

                if source_type == "audio" and filename:
                    metadata["filename"] = filename

                # ID: source_id + segment_id
                doc_id = f"{source_id}_seg_{segment['id']}"

                # LangChain Document ìƒì„±
                doc = Document(page_content=text, metadata=metadata)
                documents.append(doc)

            # LangChain VectorStoreì— ì €ì¥ (ìë™ìœ¼ë¡œ ì„ë² ë”© ìƒì„±ë¨)
            vectorstore.add_documents(
                documents=documents,
                ids=[f"{source_id}_seg_{seg['id']}" for seg in segments],
            )

            logging.info(
                f"âœ… VectorDB ì €ì¥ ì™„ë£Œ: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ (source: {source_id})"
            )
            return True

    except Exception as e:
        logging.error(f"âŒ VectorDB ì €ì¥ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return False


def store_summary_in_vectordb(summary, source_id, source_type="youtube", filename=None):
    """
    ìš”ì•½ì„ ì†Œì£¼ì œë³„ë¡œ ë¶„í• í•˜ì—¬ Summary VectorDBì— ì €ì¥

    Args:
        summary: ìƒì„±ëœ ìš”ì•½ í…ìŠ¤íŠ¸ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)
        source_id: YouTube video_id ë˜ëŠ” audio file_hash
        source_type: "youtube" ë˜ëŠ” "audio"
        filename: ì˜¤ë””ì˜¤ íŒŒì¼ëª… (ì˜¤ë””ì˜¤ì¼ ê²½ìš°)
    """
    try:
        # ë””ë²„ê¹…: ì…ë ¥ íŒŒë¼ë¯¸í„° í™•ì¸
        logging.info(f"ğŸ“¥ store_summary_in_vectordb í˜¸ì¶œë¨ - source_id: {source_id}, source_type: {source_type}")
        logging.debug(f"ìš”ì•½ íƒ€ì…: {type(summary)}, ê¸¸ì´: {len(summary) if summary else 0}")
        logging.debug(f"ìš”ì•½ ë¯¸ë¦¬ë³´ê¸°: {summary[:200] if summary else 'None'}...")

        if not summary_vectorstore:
            logging.error("âŒ Summary VectorStoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False

        # ê¸°ì¡´ ìš”ì•½ ë°ì´í„° ì‚­ì œ (ê°™ì€ source_idì˜ summary)
        try:
            existing_docs = summary_vectorstore.get(where={"source_id": source_id})
            if existing_docs and existing_docs["ids"]:
                summary_vectorstore.delete(ids=existing_docs["ids"])
                logging.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ìš”ì•½ ì‚­ì œ: {len(existing_docs['ids'])}ê°œ")
        except Exception as e:
            logging.warning(f"ê¸°ì¡´ ìš”ì•½ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

        # ìš”ì•½ì„ ì†Œì£¼ì œë³„ë¡œ ë¶„í• 
        logging.info("ğŸ” ì†Œì£¼ì œ íŒŒì‹± ì‹œì‘...")
        subtopics = parse_summary_by_subtopics(summary)
        logging.info(f"ğŸ” ì†Œì£¼ì œ íŒŒì‹± ê²°ê³¼: {len(subtopics) if subtopics else 0}ê°œ")

        if not subtopics:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ì €ì¥ (fallback)
            logging.warning("âš ï¸ ì†Œì£¼ì œ íŒŒì‹± ì‹¤íŒ¨, ì „ì²´ ìš”ì•½ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ì €ì¥í•©ë‹ˆë‹¤.")
            metadata = {
                "source_id": source_id,
                "source_type": source_type,
                "document_type": "summary",
                "subtopic": "ì „ì²´",
                "subtopic_index": 0,
                "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }
            if source_type == "audio" and filename:
                metadata["filename"] = filename

            doc = Document(page_content=summary, metadata=metadata)
            doc_id = f"{source_id}_summary_0"

            # ë³µì¡í•œ ë©”íƒ€ë°ì´í„° í•„í„°ë§ (ì¼ê´€ì„±ì„ ìœ„í•´)
            filtered_docs = filter_complex_metadata([doc])
            summary_vectorstore.add_documents(documents=filtered_docs, ids=[doc_id])
            logging.info(
                f"âœ… ìš”ì•½ Summary VectorDB ì €ì¥ ì™„ë£Œ (ì „ì²´, source: {source_id})"
            )
            return True

        # ê° ì†Œì£¼ì œë¥¼ ë³„ë„ì˜ Documentë¡œ ì €ì¥
        documents = []
        doc_ids = []

        for idx, subtopic in enumerate(subtopics):
            # cited_chunk_ids ì¶”ì¶œ
            cited_chunk_ids = subtopic.get("cited_chunk_ids", [])

            metadata = {
                "source_id": source_id,
                "source_type": source_type,
                "document_type": "summary",
                "subtopic": subtopic["title"],
                "subtopic_index": idx,
                "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "cited_chunk_ids": cited_chunk_ids,  # citation ì •ë³´ ì €ì¥ (ì²­í¬ ë²ˆí˜¸)
            }

            if source_type == "audio" and filename:
                metadata["filename"] = filename

            # ì†Œì£¼ì œ ì œëª© + ë‚´ìš©ì„ í•¨ê»˜ ì €ì¥ (ê²€ìƒ‰ ì‹œ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€)
            content = f"**{subtopic['title']}**\n\n{subtopic['content']}"
            doc = Document(page_content=content, metadata=metadata)
            documents.append(doc)

            doc_id = f"{source_id}_summary_{idx}"
            doc_ids.append(doc_id)

            logging.debug(f"ğŸ“Œ ì†Œì£¼ì œ '{subtopic['title']}' - cited_chunk_ids: {cited_chunk_ids}")

        # ë³µì¡í•œ ë©”íƒ€ë°ì´í„° í•„í„°ë§ (ë¦¬ìŠ¤íŠ¸, ë”•ì…”ë„ˆë¦¬ ë“±ì„ ë¬¸ìì—´ë¡œ ë³€í™˜)
        logging.info(f"ğŸ”§ ë³µì¡í•œ ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì¤‘... (Document ìˆ˜: {len(documents)})")
        filtered_documents = filter_complex_metadata(documents)
        logging.info(f"âœ… ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì™„ë£Œ")

        # Summary VectorStoreì— ì¼ê´„ ì €ì¥
        summary_vectorstore.add_documents(documents=filtered_documents, ids=doc_ids)

        logging.info(
            f"âœ… ìš”ì•½ Summary VectorDB ì €ì¥ ì™„ë£Œ ({len(subtopics)}ê°œ ì†Œì£¼ì œ, source: {source_id})"
        )
        return True

    except Exception as e:
        logging.error(f"âŒ ìš”ì•½ VectorDB ì €ì¥ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return False


def parse_summary_by_subtopics(summary):
    """
    ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ìš”ì•½ì„ ì†Œì£¼ì œë³„ë¡œ íŒŒì‹±í•˜ê³  citation ì •ë³´ ì¶”ì¶œ

    ë‹¤ì–‘í•œ í˜•ì‹ì„ ì§€ì›:
    1. ### ì œëª© (ë§ˆí¬ë‹¤ìš´ í—¤ë”©)
    2. **ì œëª©** (ë³¼ë“œ)
    3. ë¹ˆ ì¤„ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ì§§ì€ í…ìŠ¤íŠ¸ (ì¼ë°˜ í…ìŠ¤íŠ¸ ì œëª©)

    Args:
        summary: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ìš”ì•½ í…ìŠ¤íŠ¸

    Returns:
        list: [{"title": "ì†Œì£¼ì œ ì œëª©", "content": "ì†Œì£¼ì œ ë‚´ìš©", "cited_segment_ids": [1, 2, 3]}, ...]
    """
    import re

    if not summary or not summary.strip():
        logging.warning("âš ï¸ ìš”ì•½ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return []

    lines = summary.split("\n")
    subtopics = []
    current_title = None
    current_content = []

    logging.info(f"ğŸ“ ìš”ì•½ íŒŒì‹± ì‹œì‘ (ì´ {len(lines)}ì¤„)")

    for idx, line in enumerate(lines):
        stripped = line.strip()

        # ë‹¤ì–‘í•œ í—¤ë” í˜•ì‹ ì§€ì›
        # 1. **ì œëª©** íŒ¨í„´ (ë§ˆí¬ë‹¤ìš´ ë³¼ë“œ)
        bold_match = re.match(r"^\*\*(.+?)\*\*\s*$", stripped)
        # 2. ### ì œëª© íŒ¨í„´ (ë§ˆí¬ë‹¤ìš´ í—¤ë”© 3)
        h3_match = re.match(r"^###[\s]+(.+?)[\s]*$", stripped)
        # 3. ## ì œëª© íŒ¨í„´ (ë§ˆí¬ë‹¤ìš´ í—¤ë”© 2)
        h2_match = re.match(r"^##[\s]+(.+?)[\s]*$", stripped)
        # 4. # ì œëª© íŒ¨í„´ (ë§ˆí¬ë‹¤ìš´ í—¤ë”© 1)
        h1_match = re.match(r"^#[\s]+(.+?)[\s]*$", stripped) if len(subtopics) > 0 else None

        # 5. ì¼ë°˜ í…ìŠ¤íŠ¸ ì œëª© ê°ì§€ (íœ´ë¦¬ìŠ¤í‹±)
        # - ë¹ˆ ì¤„ë¡œ ë‘˜ëŸ¬ì‹¸ì—¬ ìˆìŒ
        # - ì§§ì€ í…ìŠ¤íŠ¸ (100ì ì´í•˜)
        # - ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë‚˜ [cite:ë¡œ ì‹œì‘í•˜ì§€ ì•ŠìŒ
        # - ë¬¸ì¥ ë¶€í˜¸ë¡œ ëë‚˜ì§€ ì•ŠìŒ (ë˜ëŠ” ë¬¼ìŒí‘œë¡œë§Œ ëë‚¨)
        is_potential_title = False
        if stripped and len(stripped) < 100 and not stripped.startswith('*') and not stripped.startswith('[cite'):
            # ì´ì „ ì¤„ê³¼ ë‹¤ìŒ ì¤„ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            prev_line_empty = (idx == 0) or (idx > 0 and not lines[idx-1].strip())
            next_line_empty = (idx == len(lines)-1) or (idx < len(lines)-1 and not lines[idx+1].strip())

            # ë¬¸ì¥ ë¶€í˜¸ ì²´í¬ (ë§ˆì¹¨í‘œ, ì‰¼í‘œ, [cite: ë“±ìœ¼ë¡œ ëë‚˜ì§€ ì•ŠìŒ)
            ends_with_punct = stripped.endswith(('.', ',', '!', ':', ';')) or '[cite:' in stripped[-20:]

            if prev_line_empty and next_line_empty and not ends_with_punct:
                is_potential_title = True
                logging.debug(f"ğŸ” ì¼ë°˜ í…ìŠ¤íŠ¸ ì œëª© í›„ë³´ (ì¤„ {idx+1}): '{stripped}'")

        # í—¤ë” ë§¤ì¹­ ìš°ì„ ìˆœìœ„: h3 > h2 > bold > h1 > ì¼ë°˜ í…ìŠ¤íŠ¸
        title_match = h3_match or h2_match or bold_match or h1_match

        if title_match or is_potential_title:
            # ì´ì „ ì†Œì£¼ì œ ì €ì¥
            if current_title is not None:
                content_str = "\n".join(current_content).strip()
                if content_str:  # ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ ì €ì¥
                    # Citation ì¶”ì¶œ ([cite: 0, 1, 2] í˜•ì‹ - ì²­í¬ ë²ˆí˜¸)
                    cited_chunk_ids = extract_citations(content_str)

                    subtopics.append(
                        {
                            "title": current_title,
                            "content": content_str,
                            "cited_chunk_ids": cited_chunk_ids,
                        }
                    )
                    logging.debug(
                        f"âœ… ì†Œì£¼ì œ ì €ì¥: '{current_title}' (ë‚´ìš© ê¸¸ì´: {len(content_str)}, citations: {cited_chunk_ids})"
                    )
                else:
                    logging.warning(f"âš ï¸ ì†Œì£¼ì œ '{current_title}'ì— ë‚´ìš©ì´ ì—†ì–´ ì œì™¸ë¨")

            # ìƒˆ ì†Œì£¼ì œ ì‹œì‘
            if title_match:
                current_title = title_match.group(1).strip()
            else:
                current_title = stripped
            current_content = []
            logging.debug(f"ğŸ“Œ ìƒˆ ì†Œì£¼ì œ ë°œê²¬ (ì¤„ {idx+1}): '{current_title}'")

        elif current_title is not None and stripped:
            # í˜„ì¬ ì†Œì£¼ì œì˜ ë‚´ìš© ì¶”ê°€ (ë¹ˆ ì¤„ì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
            current_content.append(line)

    # ë§ˆì§€ë§‰ ì†Œì£¼ì œ ì €ì¥
    if current_title is not None:
        content_str = "\n".join(current_content).strip()
        if content_str:
            cited_chunk_ids = extract_citations(content_str)
            subtopics.append({
                "title": current_title,
                "content": content_str,
                "cited_chunk_ids": cited_chunk_ids,
            })
            logging.debug(
                f"âœ… ë§ˆì§€ë§‰ ì†Œì£¼ì œ ì €ì¥: '{current_title}' (ë‚´ìš© ê¸¸ì´: {len(content_str)}, citations: {cited_chunk_ids})"
            )
        else:
            logging.warning(f"âš ï¸ ë§ˆì§€ë§‰ ì†Œì£¼ì œ '{current_title}'ì— ë‚´ìš©ì´ ì—†ì–´ ì œì™¸ë¨")

    logging.info(f"âœ… íŒŒì‹± ì™„ë£Œ: {len(subtopics)}ê°œì˜ ì†Œì£¼ì œ ë°œê²¬")

    if len(subtopics) == 0:
        logging.warning(
            f"âš ï¸ ì†Œì£¼ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìš”ì•½ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:\n{summary[:500]}"
        )

    return subtopics


def extract_citations(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ [cite: X, Y, Z] í˜•ì‹ì˜ citationì„ ì¶”ì¶œí•˜ì—¬ chunk_id ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    Args:
        text: citationì´ í¬í•¨ëœ í…ìŠ¤íŠ¸

    Returns:
        list: ì¶”ì¶œëœ chunk_id ë¦¬ìŠ¤íŠ¸ (ì •ìˆ˜)
    """
    import re

    # [cite: 0, 1, 2] ë˜ëŠ” [cite: 0] í˜•ì‹ì˜ citation ì°¾ê¸° (ì²­í¬ ë²ˆí˜¸)
    citations = re.findall(r'\[cite:\s*(\d+(?:\s*,\s*\d+)*)\]', text)

    chunk_ids = []
    for citation in citations:
        # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ chunk_idë“¤ì„ ì¶”ì¶œ
        ids = [int(cid.strip()) for cid in citation.split(',')]
        chunk_ids.extend(ids)

    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    chunk_ids = sorted(list(set(chunk_ids)))

    return chunk_ids


def get_summary_from_vectordb(source_id, source_type="youtube"):
    """
    ë³„ë„ì˜ Summary VectorDBì—ì„œ ì €ì¥ëœ ìš”ì•½ ê°€ì ¸ì˜¤ê¸° (ëª¨ë“  ì†Œì£¼ì œ í¬í•¨)

    Args:
        source_id: YouTube video_id ë˜ëŠ” audio file_hash
        source_type: "youtube" ë˜ëŠ” "audio"

    Returns:
        ìš”ì•½ í…ìŠ¤íŠ¸ (ëª¨ë“  ì†Œì£¼ì œ í•©ì³ì§„ ê²ƒ) ë˜ëŠ” None (ì—†ìœ¼ë©´)
    """
    try:
        if not summary_vectorstore:
            logging.error("âŒ Summary VectorStoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None

        # ìš”ì•½ ë¬¸ì„œ ê²€ìƒ‰ (source_id ì¼ì¹˜)
        results = summary_vectorstore.get(where={"source_id": source_id})

        if results and results["documents"] and len(results["documents"]) > 0:
            # ëª¨ë“  ì†Œì£¼ì œë¥¼ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ í•©ì¹˜ê¸°
            documents = results["documents"]
            metadatas = results["metadatas"]

            # subtopic_indexë¡œ ì •ë ¬ (ì €ì¥ ìˆœì„œ ìœ ì§€)
            sorted_chunks = []
            for doc, meta in zip(documents, metadatas):
                subtopic_index = meta.get("subtopic_index", 0)
                sorted_chunks.append((subtopic_index, doc))

            sorted_chunks.sort(key=lambda x: x[0])

            # ëª¨ë“  ì†Œì£¼ì œë¥¼ í•©ì³ì„œ ë°˜í™˜
            summary = "\n\n".join([doc for _, doc in sorted_chunks])

            logging.info(
                f"âœ… Summary VectorDBì—ì„œ ìš”ì•½ ë¡œë“œ ì™„ë£Œ (source: {source_id}, {len(documents)}ê°œ ì†Œì£¼ì œ)"
            )
            return summary
        else:
            logging.info(
                f"â„¹ï¸ Summary VectorDBì— ì €ì¥ëœ ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤ (source: {source_id})"
            )
            return None

    except Exception as e:
        logging.error(f"âŒ Summary VectorDB ìš”ì•½ ë¡œë“œ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return None


def delete_from_vectorstore(source_id, source_type="youtube"):
    """
    VectorStoreì—ì„œ íŠ¹ì • source_idì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ (ì„¸ê·¸ë¨¼íŠ¸ + ìš”ì•½)

    Args:
        source_id: YouTube video_id ë˜ëŠ” audio file_hash
        source_type: "youtube" ë˜ëŠ” "audio"

    Returns:
        (ì„±ê³µ ì—¬ë¶€, ì‚­ì œëœ ë¬¸ì„œ ìˆ˜)
    """
    try:
        total_deleted = 0

        # 1. ì„¸ê·¸ë¨¼íŠ¸ ì‚­ì œ (youtube_vectorstore ë˜ëŠ” audio_vectorstore)
        vectorstore = youtube_vectorstore if source_type == "youtube" else audio_vectorstore

        if vectorstore:
            try:
                existing_docs = vectorstore.get(where={"source_id": source_id})
                if existing_docs and existing_docs["ids"]:
                    vectorstore.delete(ids=existing_docs["ids"])
                    deleted_count = len(existing_docs["ids"])
                    total_deleted += deleted_count
                    logging.info(f"ğŸ—‘ï¸ {source_type} VectorStoreì—ì„œ {deleted_count}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì‚­ì œ")
            except Exception as e:
                logging.warning(f"âš ï¸ {source_type} VectorStore ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")

        # 2. ìš”ì•½ ì‚­ì œ (summary_vectorstore)
        if summary_vectorstore:
            try:
                existing_summary = summary_vectorstore.get(where={"source_id": source_id})
                if existing_summary and existing_summary["ids"]:
                    summary_vectorstore.delete(ids=existing_summary["ids"])
                    summary_count = len(existing_summary["ids"])
                    total_deleted += summary_count
                    logging.info(f"ğŸ—‘ï¸ Summary VectorStoreì—ì„œ {summary_count}ê°œ ìš”ì•½ ì‚­ì œ")
            except Exception as e:
                logging.warning(f"âš ï¸ Summary VectorStore ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")

        logging.info(f"âœ… VectorStore ì‚­ì œ ì™„ë£Œ: ì´ {total_deleted}ê°œ ë¬¸ì„œ ì‚­ì œë¨")
        return True, total_deleted

    except Exception as e:
        logging.error(f"âŒ VectorStore ì‚­ì œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False, 0


def search_vectordb(query, source_id=None, source_type=None, n_results=5, document_type=None):
    """
    VectorDBì—ì„œ ê²€ìƒ‰ (LangChain Retriever ì‚¬ìš©)

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        source_id: íŠ¹ì • sourceë¡œ ì œí•œ (ì„ íƒ)
        source_type: "youtube", "audio", "summary" ë˜ëŠ” None (ì„ íƒ)
        n_results: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        document_type: "chunk", "segment" ë˜ëŠ” None (ì„ íƒ, chunkë§Œ ê²€ìƒ‰í•˜ë ¤ë©´ "chunk" ì§€ì •)

    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    try:
        # ê²€ìƒ‰í•  VectorStore ê²°ì •
        vectorstores_to_search = []
        if source_type == "youtube":
            vectorstores_to_search = [youtube_vectorstore]
            logging.info(f"ğŸ” ê²€ìƒ‰ ëŒ€ìƒ: YouTube VectorStoreë§Œ")
        elif source_type == "audio":
            vectorstores_to_search = [audio_vectorstore]
            logging.info(f"ğŸ” ê²€ìƒ‰ ëŒ€ìƒ: Audio VectorStoreë§Œ")
        elif source_type == "summary":
            vectorstores_to_search = [summary_vectorstore]
            logging.info(f"ğŸ” ê²€ìƒ‰ ëŒ€ìƒ: Summary VectorStoreë§Œ")
        else:
            vectorstores_to_search = [youtube_vectorstore, audio_vectorstore]
            logging.info(f"ğŸ” ê²€ìƒ‰ ëŒ€ìƒ: YouTube + Audio VectorStore (ì „ì²´ ê²€ìƒ‰)")

        all_results = []

        for idx, vectorstore in enumerate(vectorstores_to_search):
            if not vectorstore:
                logging.warning(f"âš ï¸ VectorStore #{idx}ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue

            # VectorStoreì— ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ í™•ì¸
            try:
                collection = vectorstore._collection
                total_docs = collection.count()
                logging.info(f"ğŸ“Š VectorStore #{idx} ë¬¸ì„œ ìˆ˜: {total_docs}ê°œ")
            except Exception as e:
                logging.warning(f"âš ï¸ VectorStore #{idx} ë¬¸ì„œ ìˆ˜ í™•ì¸ ì‹¤íŒ¨: {e}")

            # where í•„í„° êµ¬ì„±
            search_kwargs = {"k": n_results}
            filter_dict = {}

            if source_id:
                filter_dict["source_id"] = source_id

            if document_type:
                filter_dict["document_type"] = document_type
                logging.info(f"ğŸ“‹ document_type í•„í„°: {document_type}")

            if filter_dict:
                search_kwargs["filter"] = filter_dict

            # LangChain Retriever ìƒì„± ë° ê²€ìƒ‰
            retriever = vectorstore.as_retriever(
                search_type="similarity", search_kwargs=search_kwargs
            )

            # ê²€ìƒ‰ ìˆ˜í–‰ (similarity_search_with_score ì‚¬ìš©)
            logging.info(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{query}', k={n_results}, í•„í„°: {filter_dict if filter_dict else 'None'}")
            docs_with_scores = vectorstore.similarity_search_with_score(
                query=query, k=n_results, filter=search_kwargs.get("filter")
            )
            logging.info(
                f"âœ… VectorStore #{idx}ì—ì„œ {len(docs_with_scores)}ê°œ ê²°ê³¼ ë°œê²¬"
            )

            # ê²°ê³¼ íŒŒì‹±
            for doc, score in docs_with_scores:
                all_results.append(
                    {
                        "id": doc.metadata.get("segment_id", ""),
                        "document": doc.page_content,
                        "metadata": doc.metadata,
                        "distance": score,  # LangChainì€ ê±°ë¦¬(ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)ë¥¼ ë°˜í™˜
                    }
                )

        # ê±°ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)
        all_results.sort(key=lambda x: x.get("distance", float("inf")))

        # ìƒìœ„ n_resultsê°œë§Œ ë°˜í™˜
        return all_results[:n_results]

    except Exception as e:
        logging.error(f"âŒ VectorDB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return []


def download_youtube_audio_as_mp3(url, task_id=None):
    """
    YouTubeì—ì„œ ì˜¤ë””ì˜¤ë§Œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ mp3ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Returns:
        dict: {
            'video_id': str,
            'title': str,
            'channel': str,
            'view_count': int,
            'upload_date': str,
            'mp3_path': str,
            'success': bool,
            'error': str (optional)
        }
    """
    try:
        if task_id:
            update_progress(task_id, "download", 0, "YouTube ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹œì‘")

        logging.info(f"ğŸµ YouTube ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {url}")

        # ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
        def progress_hook(d):
            if task_id and d["status"] == "downloading":
                downloaded = d.get("downloaded_bytes", 0)
                total = d.get("total_bytes") or d.get("total_bytes_estimate", 0)

                if total > 0:
                    percent = int((downloaded / total) * 100)
                    speed = d.get("speed", 0)
                    eta = d.get("eta", 0)

                    # ì†ë„ í¬ë§·íŒ…
                    if speed:
                        speed_mb = speed / (1024 * 1024)
                        speed_str = f"{speed_mb:.1f} MB/s"
                    else:
                        speed_str = "ê³„ì‚° ì¤‘..."

                    # ETA í¬ë§·íŒ…
                    if eta:
                        eta_min = eta // 60
                        eta_sec = eta % 60
                        eta_str = f"{int(eta_min)}:{int(eta_sec):02d}"
                    else:
                        eta_str = "ê³„ì‚° ì¤‘..."

                    message = (
                        f"ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì¤‘... {speed_str} (ë‚¨ì€ ì‹œê°„: {eta_str})"
                    )
                    update_progress(task_id, "download", percent, message)
            elif task_id and d["status"] == "finished":
                update_progress(
                    task_id, "download", 90, "ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ, MP3 ë³€í™˜ ì¤‘..."
                )
            elif task_id and d["status"] == "processing":
                update_progress(task_id, "download", 95, "MP3 ë³€í™˜ ì¤‘...")

        ydl_opts = {
            "format": "bestaudio/best",
            "outtmpl": os.path.join(MP3_FOLDER, "%(title).50s-%(id)s.%(ext)s"),
            "postprocessors": [
                {
                    "key": "FFmpegExtractAudio",
                    "preferredcodec": "mp3",
                    "preferredquality": "192",
                }
            ],
            "progress_hooks": [progress_hook],
        }

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info_dict = ydl.extract_info(url, download=True)
            video_id = info_dict.get("id", None)
            video_title = info_dict.get("title", None)
            channel = info_dict.get("channel", info_dict.get("uploader", "Unknown"))
            view_count = info_dict.get("view_count", 0)
            upload_date = info_dict.get("upload_date", "")

            # upload_date í¬ë§· ë³€í™˜ (YYYYMMDD -> YYYY-MM-DD)
            if upload_date and len(upload_date) == 8:
                upload_date = f"{upload_date[:4]}-{upload_date[4:6]}-{upload_date[6:]}"

            # MP3 íŒŒì¼ ê²½ë¡œ ìƒì„± (yt-dlpê°€ ìƒì„±í•œ íŒŒì¼ëª… ê¸°ë°˜)
            # prepare_filenameì€ ì›ë³¸ í™•ì¥ìë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ .mp3ë¡œ êµì²´
            original_path = ydl.prepare_filename(info_dict)
            mp3_path = os.path.splitext(original_path)[0] + ".mp3"

        if not os.path.exists(mp3_path):
            if task_id:
                update_progress(task_id, "download", 0, "MP3 íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return {"success": False, "error": "MP3 íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        logging.info(f"âœ… YouTube ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {mp3_path}")

        if task_id:
            update_progress(task_id, "download", 100, "YouTube ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

        return {
            "success": True,
            "video_id": video_id,
            "title": video_title,
            "channel": channel,
            "view_count": view_count,
            "upload_date": upload_date,
            "mp3_path": mp3_path,
        }

    except Exception as e:
        logging.error(f"âŒ YouTube ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        if task_id:
            update_progress(task_id, "download", 0, f"ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return {"success": False, "error": str(e)}


def merge_consecutive_speaker_segments(segments):
    """ì—°ì†ì ìœ¼ë¡œ ë™ì¼í•œ í™”ìì˜ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹©ë‹ˆë‹¤."""
    if not segments:
        return []

    merged_segments = []
    current_segment = segments[0].copy()

    for next_segment in segments[1:]:
        if current_segment["speaker"] == next_segment["speaker"]:
            current_segment["text"] += " " + next_segment["text"]
        else:
            merged_segments.append(current_segment)
            current_segment = next_segment.copy()

    merged_segments.append(current_segment)
    return merged_segments


def get_segment_from_csv(source_id, source_type, segment_id):
    """
    CSVì—ì„œ íŠ¹ì • ì„¸ê·¸ë¨¼íŠ¸ë¥¼ segment_idë¡œ ì¡°íšŒ

    Args:
        source_id: YouTube video_id ë˜ëŠ” audio file_hash
        source_type: "youtube" ë˜ëŠ” "audio"
        segment_id: ì„¸ê·¸ë¨¼íŠ¸ ID

    Returns:
        segment dict ë˜ëŠ” None
    """
    try:
        if source_type == "youtube":
            history_df = load_youtube_history()
            row = history_df[history_df["video_id"] == source_id]
        else:  # audio
            history_df = load_audio_history()
            row = history_df[history_df["file_hash"] == source_id]

        if row.empty:
            logging.warning(f"âš ï¸ CSVì—ì„œ source_id={source_id} ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None

        # segments_json íŒŒì‹±
        segments_json_str = row.iloc[0].get("segments_json", "[]")
        segments = json.loads(segments_json_str)

        # segment_idë¡œ ê²€ìƒ‰
        for seg in segments:
            if seg.get("id") == segment_id:
                return seg

        logging.warning(f"âš ï¸ CSVì—ì„œ segment_id={segment_id} ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return None

    except Exception as e:
        logging.error(f"âŒ CSVì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ì¡°íšŒ ì‹¤íŒ¨ (source_id={source_id}, segment_id={segment_id}): {e}")
        return None


def create_token_based_chunks(segments, chunk_size=500, chunk_overlap=100):
    """
    í† í° ê¸°ë°˜ ì²­í‚¹: í™”ì ë¶„ë¦¬ëœ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ê³ ì • í¬ê¸°ì˜ chunkë¡œ ì¬êµ¬ì„±

    Args:
        segments: ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ [{"id": 1, "speaker": "1", "start_time": 0.0, "text": "...", "confidence": 0.95}, ...]
        chunk_size: chunkë‹¹ ìµœëŒ€ ë¬¸ì ìˆ˜ (í† í° ê·¼ì‚¬ì¹˜)
        chunk_overlap: chunk ê°„ ì¤‘ë³µ ë¬¸ì ìˆ˜

    Returns:
        chunks: [{"chunk_id": 0, "text": "...", "segment_ids": [1, 2, 3], "start_time": 0.0, "end_time": 30.5, "speakers": ["1", "2"]}, ...]
    """
    try:
        if not segments:
            logging.warning("âš ï¸ create_token_based_chunks: ë¹ˆ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸")
            return []

        # ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë§ˆì»¤ì™€ í•¨ê»˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        full_text_with_markers = ""
        segment_map = {}  # ë§ˆì»¤ ìœ„ì¹˜ â†’ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ë§¤í•‘

        for seg in segments:
            marker = f"[SEG_{seg['id']}]"
            full_text_with_markers += marker + seg["text"] + " "
            segment_map[seg['id']] = seg

        # RecursiveCharacterTextSplitterë¡œ ì²­í‚¹
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", "ã€‚ ", "! ", "? ", ", ", " ", ""],  # í•œêµ­ì–´/ì˜ì–´ ë¬¸ì¥ êµ¬ë¶„ì
            length_function=len,
        )

        chunks_text = splitter.split_text(full_text_with_markers)
        logging.info(f"ğŸ“¦ ì²­í‚¹ ì™„ë£Œ: {len(chunks_text)}ê°œ chunk ìƒì„± (chunk_size={chunk_size}, overlap={chunk_overlap})")

        # chunkì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ID ì¶”ì¶œ ë° ë©”íƒ€ë°ì´í„° êµ¬ì„±
        chunks = []
        import re

        for chunk_idx, chunk_text in enumerate(chunks_text):
            # [SEG_X] ë§ˆì»¤ì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ID ì¶”ì¶œ
            seg_ids = [int(x) for x in re.findall(r'\[SEG_(\d+)\]', chunk_text)]

            if not seg_ids:
                # ë§ˆì»¤ê°€ ì—†ëŠ” ê²½ìš° (ë“œë¬¼ì§€ë§Œ ì²˜ë¦¬)
                logging.warning(f"âš ï¸ Chunk {chunk_idx}: ì„¸ê·¸ë¨¼íŠ¸ ID ì—†ìŒ")
                continue

            # ë§ˆì»¤ ì œê±°í•˜ì—¬ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            clean_text = re.sub(r'\[SEG_\d+\]', '', chunk_text).strip()

            # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            cited_segments = [segment_map[sid] for sid in seg_ids if sid in segment_map]

            if not cited_segments:
                logging.warning(f"âš ï¸ Chunk {chunk_idx}: ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ ì—†ìŒ")
                continue

            # ì‹œì‘/ì¢…ë£Œ ì‹œê°„ ê³„ì‚°
            start_time = min(seg["start_time"] for seg in cited_segments)

            # end_time ê³„ì‚°: ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ì˜ end_time (ì—†ìœ¼ë©´ ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ì˜ start_time)
            last_seg = cited_segments[-1]
            last_seg_id = last_seg["id"]

            # ì›ë³¸ segmentsì—ì„œ last_seg_id ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì°¾ê¸°
            last_seg_idx = next((i for i, s in enumerate(segments) if s["id"] == last_seg_id), None)
            if last_seg_idx is not None and last_seg_idx + 1 < len(segments):
                end_time = segments[last_seg_idx + 1]["start_time"]
            else:
                # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ì¸ ê²½ìš° end_timeì€ None
                end_time = None

            # í™”ì ëª©ë¡ ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
            speakers = sorted(list(set(seg["speaker"] for seg in cited_segments)))

            # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
            avg_confidence = sum(seg.get("confidence", 0.0) for seg in cited_segments) / len(cited_segments)

            chunks.append({
                "chunk_id": chunk_idx,
                "text": clean_text,
                "segment_ids": seg_ids,  # ì¸ìš©ëœ ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ID ë¦¬ìŠ¤íŠ¸
                "start_time": float(start_time),
                "end_time": float(end_time) if end_time is not None else None,
                "speakers": speakers,  # ë³µìˆ˜ í™”ì ê°€ëŠ¥
                "confidence": float(avg_confidence),
            })

        logging.info(f"âœ… ì²­í‚¹ ê²°ê³¼: {len(chunks)}ê°œ chunk, í‰ê·  ê¸¸ì´: {sum(len(c['text']) for c in chunks) / len(chunks):.0f}ì")
        return chunks

    except Exception as e:
        logging.error(f"âŒ create_token_based_chunks ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return []


def parse_mmss_to_seconds(time_str):
    """'ë¶„:ì´ˆ:ë°€ë¦¬ì´ˆ' í˜•íƒœì˜ ë¬¸ìì—´ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    try:
        parts = time_str.split(":")
        if len(parts) == 3:
            minutes = int(parts[0])
            seconds = int(parts[1])
            milliseconds = int(parts[2])
            return minutes * 60 + seconds + milliseconds / 1000.0
        else:
            return 0.0
    except:
        return 0.0


def recognize_with_gemini(audio_path, task_id=None, audio_duration=None):
    """
    Google Gemini STT APIë¡œ ìŒì„± ì¸ì‹

    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        task_id: ì‘ì—… ID (í”„ë¡œê·¸ë ˆìŠ¤ ì—…ë°ì´íŠ¸ìš©)
        audio_duration: ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ) - ì˜ˆìƒ ì‹œê°„ ê³„ì‚°ìš©
    """
    start_time = time.time()

    # ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
    estimated_time = None
    if audio_duration:
        estimated_time = estimate_stt_processing_time(audio_duration)
        logging.info(f"â±ï¸ ì˜ˆìƒ STT ì²˜ë¦¬ ì‹œê°„: {estimated_time:.1f}ì´ˆ (ì˜¤ë””ì˜¤ ê¸¸ì´: {audio_duration:.1f}ì´ˆ)")

    try:
        if task_id:
            update_progress(
                task_id,
                "stt",
                0,
                "Gemini STT ì‹œì‘",
                estimated_time=estimated_time,
                elapsed_time=0
            )

        logging.info(f"ğŸ§ Gemini STT APIë¡œ ìŒì„± ì¸ì‹ ì¤‘: {audio_path}")

        # ë³„ë„ ìŠ¤ë ˆë“œë¡œ ê²½ê³¼ ì‹œê°„ ì—…ë°ì´íŠ¸ (ì‹œë®¬ë ˆì´ì…˜)
        stop_progress_update = threading.Event()

        def update_elapsed_time():
            """ê²½ê³¼ ì‹œê°„ì„ ì£¼ê¸°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸"""
            while not stop_progress_update.is_set():
                elapsed = time.time() - start_time

                if task_id and estimated_time:
                    # ì§„í–‰ë¥  ê³„ì‚° (ìµœëŒ€ 95%ê¹Œì§€ë§Œ)
                    progress_percent = min(95, int((elapsed / estimated_time) * 100))

                    update_progress(
                        task_id,
                        "stt",
                        progress_percent,
                        "ì˜¤ë””ì˜¤ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...",
                        estimated_time=estimated_time,
                        elapsed_time=elapsed
                    )

                time.sleep(1)  # 1ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸

        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ìŠ¤ë ˆë“œ ì‹œì‘
        if task_id and estimated_time:
            progress_thread = threading.Thread(target=update_elapsed_time, daemon=True)
            progress_thread.start()

        client = get_gemini_client()

        with open(audio_path, "rb") as f:
            file_bytes = f.read()

        file_ext = os.path.splitext(audio_path)[1].lower()
        mime_type_map = {
            ".wav": "audio/wav",
            ".mp3": "audio/mp3",
            ".m4a": "audio/mp4",
            ".flac": "audio/flac",
            ".ogg": "audio/ogg",
        }
        mime_type = mime_type_map.get(file_ext, "audio/mp3")

        prompt = """
ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ íšŒì˜ë¡ ì‘ì„±ìì…ë‹ˆë‹¤. ì œê³µëœ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë“£ê³  ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•´ ì£¼ì‹­ì‹œì˜¤:
1. ì „ì²´ ëŒ€í™”ë¥¼ ì •í™•í•˜ê²Œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
2. ê° ë°œí™”ì— ëŒ€í•´ í™”ìë¥¼ ìˆ«ìë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤. ë°œí™”ìì˜ ë“±ì¥ ìˆœì„œëŒ€ë¡œ ë²ˆí˜¸ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.
3. ê° ë°œí™”ì— ëŒ€í•´ ìŒì„± ì¸ì‹ì˜ ì‹ ë¢°ë„ë¥¼ 0.0~1.0 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
4. ìµœì¢… ê²°ê³¼ëŠ” ì•„ë˜ì˜ JSON í˜•ì‹ê³¼ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤. ê° JSON ê°ì²´ëŠ” 'speaker', 'start_time_mmss', 'confidence', 'text' í‚¤ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
5. start_time_mmssëŠ” "ë¶„:ì´ˆ:ë°€ë¦¬ì´ˆ" í˜•íƒœë¡œ ì¶œë ¥í•©ë‹ˆë‹¤. (ì˜ˆ: "0:05:200", "1:23:450")
6. ë°°ê²½ìŒì•…ê³¼ ë°œí™”ìì˜ ëª©ì†Œë¦¬ê°€ ì„ì¸ ê²½ìš° ëª©ì†Œë¦¬ë§Œ ì˜ êµ¬ë³„í•˜ì—¬ ê°€ì ¸ì˜¨ë‹¤.
7. speakerê°€ ë™ì¼í•œ ê²½ìš° í•˜ë‚˜ì˜ í–‰ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤. ë‹¨, ë¬¸ì¥ì´ 5ê°œë¥¼ ë„˜ì–´ê°ˆ ê²½ìš° ë‹¤ìŒ ëŒ€í™”ë¡œ ë¶„ë¦¬í•œë‹¤.


ì¶œë ¥ í˜•ì‹:
[
    {
        "speaker": 1,
        "start_time_mmss": "0:00:000",
        "confidence": 0.95,
        "text": "ì•ˆë…•í•˜ì„¸ìš”. íšŒì˜ë¥¼ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤."
    },
    {
        "speaker": 2,
        "start_time_mmss": "0:05:200",
        "confidence": 0.92,
        "text": "ë„¤, ì¢‹ìŠµë‹ˆë‹¤."
    }
]

JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
"""

        logging.info("ğŸ¤– Gemini 2.5 Proë¡œ ìŒì„± ì¸ì‹ ì¤‘...")

        response = client.models.generate_content(
            model="gemini-2.5-pro",
            contents=[
                prompt,
                types.Part.from_bytes(
                    data=file_bytes,
                    mime_type=mime_type,
                ),
            ],
        )

        logging.info("âœ… Gemini ìŒì„± ì¸ì‹ ì™„ë£Œ")

        cleaned_response = response.text.strip()
        cleaned_response = (
            cleaned_response.replace("```json", "").replace("```", "").strip()
        )

        result_list = json.loads(cleaned_response)

        normalized_segments = []
        for idx, segment in enumerate(result_list):
            time_mmss = segment.get("start_time_mmss", "0:00:000")
            start_time_seconds = parse_mmss_to_seconds(time_mmss)

            normalized_segments.append(
                {
                    "id": idx,
                    "speaker": segment.get("speaker", 1),
                    "start_time": start_time_seconds,
                    "confidence": segment.get("confidence", 0.0),
                    "text": segment.get("text", ""),
                }
            )

        end_time = time.time()
        processing_time = end_time - start_time

        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ìŠ¤ë ˆë“œ ì¤‘ì§€
        if task_id and estimated_time:
            stop_progress_update.set()
            if 'progress_thread' in locals():
                progress_thread.join(timeout=1)

        if task_id:
            update_progress(
                task_id,
                "stt",
                100,
                f"Gemini STT ì™„ë£Œ",
                estimated_time=estimated_time,
                elapsed_time=processing_time
            )

        logging.info(f"â±ï¸ Gemini STT ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")

        return {"segments": normalized_segments, "processing_time": processing_time}

    except Exception as e:
        end_time = time.time()
        processing_time = end_time - start_time

        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ìŠ¤ë ˆë“œ ì¤‘ì§€
        if task_id and estimated_time:
            stop_progress_update.set()
            if 'progress_thread' in locals():
                progress_thread.join(timeout=1)

        logging.error(f"âŒ Gemini ì˜¤ë¥˜ ë°œìƒ: {e}")
        if task_id:
            update_progress(
                task_id,
                "stt",
                0,
                "Gemini STT ì˜¤ë¥˜",
                estimated_time=estimated_time,
                elapsed_time=processing_time
            )
        import traceback

        traceback.print_exc()
        return {"segments": None, "processing_time": processing_time}


@app.route("/")
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return render_template("youtube_viewer.html")


@app.route("/api/process-youtube", methods=["POST"])
def process_youtube():
    """
    YouTube URLì„ ì²˜ë¦¬í•˜ì—¬ íšŒì˜ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ìºì‹± ê¸°ëŠ¥ í¬í•¨.
    """
    try:
        data = request.get_json()
        youtube_url = data.get("youtube_url", "").strip()

        if not youtube_url:
            return (
                jsonify({"success": False, "error": "YouTube URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}),
                400,
            )

        # ë¨¼ì € video_id ì¶”ì¶œ (ë‹¤ìš´ë¡œë“œ ì—†ì´ ì •ë³´ë§Œ)
        try:
            with yt_dlp.YoutubeDL({"quiet": True}) as ydl:
                info = ydl.extract_info(youtube_url, download=False)
                video_id = info.get("id", None)

                if not video_id:
                    return (
                        jsonify(
                            {
                                "success": False,
                                "error": "ìœ íš¨í•˜ì§€ ì•Šì€ YouTube URLì…ë‹ˆë‹¤.",
                            }
                        ),
                        400,
                    )
        except Exception as e:
            logging.error(f"URL íŒŒì‹± ì˜¤ë¥˜: {e}")
            return (
                jsonify(
                    {"success": False, "error": f"YouTube URL íŒŒì‹± ì‹¤íŒ¨: {str(e)}"}
                ),
                400,
            )

        # ì´ë ¥ì—ì„œ í™•ì¸
        history_df = load_youtube_history()

        # video_idë¡œ ìºì‹œ í™•ì¸ (URL í˜•ì‹ê³¼ ë¬´ê´€)
        existing = history_df[history_df["video_id"] == video_id]

        if not existing.empty:
            # ìºì‹œëœ ë°ì´í„° ë¡œë“œ
            row = existing.iloc[0]

            logging.info(f"ğŸ“‚ ìºì‹œëœ ë°ì´í„° ë¡œë“œ: {row['title']}")

            # segments JSON íŒŒì‹±
            segments = json.loads(row["segments_json"])

            # ì„¸ì…˜ì— ì €ì¥
            session_id = request.remote_addr + "_" + secrets.token_hex(8)
            session_data[session_id] = {
                "segments": segments,
                "chat_history": [],
                "video_id": row["video_id"],  # ìš”ì•½ ì €ì¥ ì‹œ ì‚¬ìš©
                "source_type": "youtube",
            }

            # NaN ê°’ ì•ˆì „ ì²˜ë¦¬
            view_count = row.get("view_count", 0)
            if pd.isna(view_count):
                view_count = 0
            else:
                view_count = int(view_count)

            stt_processing_time = row.get("stt_processing_time", 0.0)
            if pd.isna(stt_processing_time):
                stt_processing_time = 0.0
            else:
                stt_processing_time = float(stt_processing_time)

            # ìš”ì•½ ë¡œë“œ (CSV â†’ VectorStore ìˆœì„œë¡œ í™•ì¸)
            summary = row.get("summary", "")
            if not summary or pd.isna(summary) or summary.strip() == "":
                # CSVì— ìš”ì•½ì´ ì—†ìœ¼ë©´ VectorStoreì—ì„œ ê°€ì ¸ì˜¤ê¸°
                vectordb_summary = get_summary_from_vectordb(
                    source_id=row["video_id"], source_type="youtube"
                )
                if vectordb_summary:
                    summary = vectordb_summary
                    logging.info(f"ğŸ“¦ VectorDBì—ì„œ ìš”ì•½ ë¡œë“œ: {row['video_id']}")

            return jsonify(
                {
                    "success": True,
                    "cached": True,
                    "source_type": "youtube",
                    "message": f"âœ… ì €ì¥ëœ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤: {row['title']}",
                    "video_id": row["video_id"],
                    "title": row["title"],
                    "channel": row.get("channel", "Unknown"),
                    "view_count": view_count,
                    "upload_date": row.get("upload_date", ""),
                    "mp3_path": row.get("mp3_path", ""),
                    "segments": segments,
                    "total_segments": len(segments),
                    "stt_service": row["stt_service"],
                    "stt_processing_time": stt_processing_time,
                    "session_id": session_id,
                    "created_at": row["created_at"],
                    "summary": summary,
                }
            )

        # ìƒˆë¡œìš´ ì²˜ë¦¬
        logging.info(f"ğŸ†• ìƒˆë¡œìš´ YouTube URL ì²˜ë¦¬: {youtube_url}")

        # task_id ë° remote_addr ìƒì„± (request contextì—ì„œ ë¯¸ë¦¬ ì¶”ì¶œ)
        task_id = secrets.token_hex(16)
        remote_addr = request.remote_addr

        # progress_data ì´ˆê¸°í™” (í”„ë¡œê·¸ë ˆìŠ¤ ë°” 100% ë²„ê·¸ ë°©ì§€)
        progress_data[task_id] = {}

        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬í•  í•¨ìˆ˜
        def process_in_background():
            try:
                # 1. YouTube ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ (mp3ë¡œ ì§ì ‘ ë³€í™˜)
                download_result = download_youtube_audio_as_mp3(youtube_url, task_id)
                if not download_result["success"]:
                    update_progress(
                        task_id,
                        "error",
                        0,
                        f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {download_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}",
                    )
                    return

                video_id = download_result["video_id"]
                title = download_result["title"]
                channel = download_result["channel"]
                view_count = download_result["view_count"]
                upload_date = download_result["upload_date"]
                mp3_path = download_result["mp3_path"]

                # MP3 ì˜¤ë””ì˜¤ ê¸¸ì´ ì¶”ì¶œ (ì˜ˆìƒ ì‹œê°„ ê³„ì‚°ìš©)
                audio_duration = get_audio_duration(mp3_path)

                # 2. STT ì²˜ë¦¬ (Gemini)
                stt_processing_time = 0.0
                segments = None

                result = recognize_with_gemini(mp3_path, task_id, audio_duration)
                if result and isinstance(result, dict):
                    segments = result.get("segments")
                    stt_processing_time = result.get("processing_time", 0.0)

                if not segments:
                    # STT ì‹¤íŒ¨ ì‹œì—ë„ ì˜ìƒ ì •ë³´ëŠ” DBì— ì €ì¥ (ë¹ˆ ì„¸ê·¸ë¨¼íŠ¸ë¡œ)
                    logging.warning(f"âš ï¸ STT ì²˜ë¦¬ ì‹¤íŒ¨, ì˜ìƒ ì •ë³´ë§Œ DBì— ì €ì¥: {title}")

                    new_row = {
                        "youtube_url": youtube_url,
                        "video_id": video_id,
                        "title": title,
                        "channel": channel,
                        "view_count": view_count,
                        "upload_date": upload_date,
                        "mp3_path": mp3_path,
                        "segments_json": json.dumps([], ensure_ascii=False),  # ë¹ˆ ì„¸ê·¸ë¨¼íŠ¸
                        "stt_service": "gemini",
                        "stt_processing_time": 0.0,
                        "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "summary": "STT ì²˜ë¦¬ ì‹¤íŒ¨",
                    }

                    history_df = load_youtube_history()
                    history_df = pd.concat(
                        [history_df, pd.DataFrame([new_row])], ignore_index=True
                    )
                    save_youtube_history(history_df)

                    update_progress(
                        task_id,
                        "error",
                        0,
                        "Gemini STT ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    )
                    return

                # 3. ì´ë ¥ì— ì €ì¥
                new_row = {
                    "youtube_url": youtube_url,
                    "video_id": video_id,
                    "title": title,
                    "channel": channel,
                    "view_count": view_count,
                    "upload_date": upload_date,
                    "mp3_path": mp3_path,
                    "segments_json": json.dumps(segments, ensure_ascii=False),
                    "stt_service": "gemini",
                    "stt_processing_time": stt_processing_time,
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "summary": "",
                }

                history_df = load_youtube_history()
                history_df = pd.concat(
                    [history_df, pd.DataFrame([new_row])], ignore_index=True
                )
                save_youtube_history(history_df)

                # STT ì²˜ë¦¬ ì‹œê°„ ë¡œê·¸ì— ê¸°ë¡
                add_stt_processing_record(audio_duration, stt_processing_time, source_type="youtube")

                # ì„¸ì…˜ì— ì €ì¥
                session_id = remote_addr + "_" + secrets.token_hex(8)
                session_data[session_id] = {
                    "segments": segments,
                    "chat_history": [],
                    "video_id": video_id,
                    "source_type": "youtube",
                }

                # ì™„ë£Œ ìƒíƒœ ì €ì¥
                progress_data[task_id]["completed"] = True
                progress_data[task_id]["result"] = {
                    "success": True,
                    "source_type": "youtube",
                    "video_id": video_id,
                    "title": title,
                    "channel": channel,
                    "view_count": view_count,
                    "upload_date": upload_date,
                    "mp3_path": mp3_path,
                    "segments": segments,
                    "total_segments": len(segments),
                    "stt_service": "gemini",
                    "stt_processing_time": stt_processing_time,
                    "session_id": session_id,
                    "created_at": new_row["created_at"],
                }

                logging.info(f"âœ… ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì™„ë£Œ: {title}")

            except Exception as e:
                import traceback

                traceback.print_exc()
                update_progress(task_id, "error", 0, f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘
        thread = threading.Thread(target=process_in_background)
        thread.daemon = True
        thread.start()

        # ì¦‰ì‹œ task_id ë°˜í™˜
        return jsonify(
            {
                "success": True,
                "processing": True,
                "task_id": task_id,
                "message": "ì²˜ë¦¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ì§„í–‰ ìƒí™©ì„ í™•ì¸í•˜ì„¸ìš”.",
            }
        )

    except Exception as e:
        import traceback

        traceback.print_exc()
        return jsonify({"success": False, "error": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500


@app.route("/api/process-audio", methods=["POST"])
def process_audio():
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ íšŒì˜ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ìºì‹± ê¸°ëŠ¥ í¬í•¨.
    """
    try:
        # íŒŒì¼ í™•ì¸
        if "audio_file" not in request.files:
            return jsonify({"success": False, "error": "ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}), 400

        file = request.files["audio_file"]

        if file.filename == "":
            return (
                jsonify({"success": False, "error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}),
                400,
            )

        if not allowed_file(file.filename):
            return (
                jsonify(
                    {
                        "success": False,
                        "error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. í—ˆìš©ëœ í˜•ì‹: {', '.join(ALLOWED_AUDIO_EXTENSIONS)}",
                    }
                ),
                400,
            )

        # íŒŒì¼ëª… ì¤€ë¹„
        filename = secure_filename(file.filename)
        final_file_path = os.path.join(UPLOADS_FOLDER, filename)

        # ì„ì‹œ íŒŒì¼ë¡œ ë¨¼ì € ì €ì¥ (ë®ì–´ì“°ê¸° ë°©ì§€)
        temp_filename = f"temp_{secrets.token_hex(8)}_{filename}"
        temp_file_path = os.path.join(UPLOADS_FOLDER, temp_filename)
        file.save(temp_file_path)

        logging.info(f"ğŸ“ ì„ì‹œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {temp_file_path}")

        # íŒŒì¼ í•´ì‹œ ê³„ì‚°
        file_hash = calculate_file_hash(temp_file_path)
        file_size = os.path.getsize(temp_file_path)

        # ìµœì¢… íŒŒì¼ëª… ì¤‘ë³µ ì²´í¬
        if os.path.exists(final_file_path):
            # ê¸°ì¡´ íŒŒì¼ì˜ í•´ì‹œ ê³„ì‚°
            existing_file_hash = calculate_file_hash(final_file_path)

            if existing_file_hash != file_hash:
                # ë‹¤ë¥¸ íŒŒì¼ì¸ë° ì´ë¦„ì´ ê°™ìŒ â†’ ì—ëŸ¬
                os.remove(temp_file_path)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                return (
                    jsonify(
                        {
                            "success": False,
                            "error": f"ê°™ì€ ì´ë¦„ì˜ ë‹¤ë¥¸ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {filename}\në‹¤ë¥¸ íŒŒì¼ëª…ìœ¼ë¡œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.",
                        }
                    ),
                    400,
                )
            else:
                # ê°™ì€ íŒŒì¼ì„ â†’ ì„ì‹œ íŒŒì¼ ì‚­ì œí•˜ê³  ê¸°ì¡´ íŒŒì¼ ì‚¬ìš©
                os.remove(temp_file_path)
                file_path = final_file_path
                logging.info(f"âœ… ê¸°ì¡´ íŒŒì¼ ì¬ì‚¬ìš©: {filename}")
        else:
            # íŒŒì¼ì´ ì—†ìŒ â†’ ì„ì‹œ íŒŒì¼ì„ ìµœì¢… ìœ„ì¹˜ë¡œ ì´ë™
            os.rename(temp_file_path, final_file_path)
            file_path = final_file_path
            logging.info(f"âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_path}")

        # ì˜¤ë””ì˜¤ ê¸¸ì´ ì¶”ì¶œ
        audio_duration = get_audio_duration(file_path)

        # ì´ë ¥ì—ì„œ í™•ì¸ (íŒŒì¼ í•´ì‹œë¡œ ìºì‹œ í™•ì¸)
        history_df = load_audio_history()
        existing = history_df[history_df["file_hash"] == file_hash]

        if not existing.empty:
            # ìºì‹œëœ ë°ì´í„° ë¡œë“œ
            row = existing.iloc[0]

            logging.info(f"ğŸ“‚ ìºì‹œëœ ì˜¤ë””ì˜¤ ë°ì´í„° ë¡œë“œ: {row['filename']}")

            # segments JSON íŒŒì‹±
            segments = json.loads(row["segments_json"])

            # ì„¸ì…˜ì— ì €ì¥
            session_id = request.remote_addr + "_" + secrets.token_hex(8)
            session_data[session_id] = {
                "segments": segments,
                "chat_history": [],
                "file_hash": row["file_hash"],
                "filename": row["filename"],
                "source_type": "audio",
            }

            stt_processing_time = row.get("stt_processing_time", 0.0)
            if pd.isna(stt_processing_time):
                stt_processing_time = 0.0
            else:
                stt_processing_time = float(stt_processing_time)

            audio_duration = row.get("audio_duration", 0.0)
            if pd.isna(audio_duration):
                audio_duration = 0.0
            else:
                audio_duration = float(audio_duration)

            # ìš”ì•½ ë¡œë“œ (CSV â†’ VectorStore ìˆœì„œë¡œ í™•ì¸)
            summary = row.get("summary", "")
            if not summary or pd.isna(summary) or summary.strip() == "":
                # CSVì— ìš”ì•½ì´ ì—†ìœ¼ë©´ VectorStoreì—ì„œ ê°€ì ¸ì˜¤ê¸°
                vectordb_summary = get_summary_from_vectordb(
                    source_id=row["file_hash"], source_type="audio"
                )
                if vectordb_summary:
                    summary = vectordb_summary
                    logging.info(f"ğŸ“¦ VectorDBì—ì„œ ìš”ì•½ ë¡œë“œ: {row['filename']}")

            return jsonify(
                {
                    "success": True,
                    "cached": True,
                    "source_type": "audio",
                    "message": f"âœ… ì €ì¥ëœ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤: {row['filename']}",
                    "file_hash": row["file_hash"],
                    "filename": row["filename"],
                    "file_path": row["file_path"],
                    "file_size": int(row["file_size"]),
                    "audio_duration": audio_duration,
                    "segments": segments,
                    "total_segments": len(segments),
                    "stt_service": row["stt_service"],
                    "stt_processing_time": stt_processing_time,
                    "session_id": session_id,
                    "created_at": row["created_at"],
                    "summary": summary,
                }
            )

        # ìƒˆë¡œìš´ ì²˜ë¦¬
        logging.info(f"ğŸ†• ìƒˆë¡œìš´ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬: {filename}")

        # task_id ë° remote_addr ìƒì„±
        task_id = secrets.token_hex(16)
        remote_addr = request.remote_addr

        # progress_data ì´ˆê¸°í™” (í”„ë¡œê·¸ë ˆìŠ¤ ë°” 100% ë²„ê·¸ ë°©ì§€)
        progress_data[task_id] = {}

        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬í•  í•¨ìˆ˜
        def process_in_background():
            try:
                # STT ì²˜ë¦¬ (Gemini)
                stt_processing_time = 0.0
                segments = None

                result = recognize_with_gemini(file_path, task_id, audio_duration)
                if result and isinstance(result, tuple) and len(result) == 2:
                    segments, stt_processing_time = result
                elif result and isinstance(result, dict):
                    segments = result.get("segments")
                    stt_processing_time = result.get("processing_time", 0.0)

                if not segments:
                    # STT ì‹¤íŒ¨ ì‹œì—ë„ íŒŒì¼ ì •ë³´ëŠ” DBì— ì €ì¥ (ë¹ˆ ì„¸ê·¸ë¨¼íŠ¸ë¡œ)
                    logging.warning(f"âš ï¸ STT ì²˜ë¦¬ ì‹¤íŒ¨, íŒŒì¼ ì •ë³´ë§Œ DBì— ì €ì¥: {filename}")

                    new_row = {
                        "file_hash": file_hash,
                        "filename": filename,
                        "file_path": file_path,
                        "file_size": file_size,
                        "audio_duration": audio_duration,
                        "segments_json": json.dumps([], ensure_ascii=False),  # ë¹ˆ ì„¸ê·¸ë¨¼íŠ¸
                        "stt_service": "gemini",
                        "stt_processing_time": 0.0,
                        "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "summary": "STT ì²˜ë¦¬ ì‹¤íŒ¨",
                    }

                    history_df = load_audio_history()
                    history_df = pd.concat(
                        [history_df, pd.DataFrame([new_row])], ignore_index=True
                    )
                    save_audio_history(history_df)

                    update_progress(
                        task_id,
                        "error",
                        0,
                        "Gemini STT ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    )
                    return

                # ì´ë ¥ì— ì €ì¥
                new_row = {
                    "file_hash": file_hash,
                    "filename": filename,
                    "file_path": file_path,
                    "file_size": file_size,
                    "audio_duration": audio_duration,
                    "segments_json": json.dumps(segments, ensure_ascii=False),
                    "stt_service": "gemini",
                    "stt_processing_time": stt_processing_time,
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "summary": "",
                }

                history_df = load_audio_history()
                history_df = pd.concat(
                    [history_df, pd.DataFrame([new_row])], ignore_index=True
                )
                save_audio_history(history_df)

                # STT ì²˜ë¦¬ ì‹œê°„ ë¡œê·¸ì— ê¸°ë¡
                add_stt_processing_record(audio_duration, stt_processing_time, source_type="audio")

                # ì„¸ì…˜ì— ì €ì¥
                session_id = remote_addr + "_" + secrets.token_hex(8)
                session_data[session_id] = {
                    "segments": segments,
                    "chat_history": [],
                    "file_hash": file_hash,
                    "filename": filename,
                    "source_type": "audio",
                }

                # ì™„ë£Œ ìƒíƒœ ì €ì¥
                progress_data[task_id]["completed"] = True
                progress_data[task_id]["result"] = {
                    "success": True,
                    "source_type": "audio",
                    "file_hash": file_hash,
                    "filename": filename,
                    "file_path": file_path,
                    "file_size": file_size,
                    "audio_duration": audio_duration,
                    "segments": segments,
                    "total_segments": len(segments),
                    "stt_service": "gemini",
                    "stt_processing_time": stt_processing_time,
                    "session_id": session_id,
                    "created_at": new_row["created_at"],
                }

                logging.info(f"âœ… ë°±ê·¸ë¼ìš´ë“œ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ: {filename}")

            except Exception as e:
                import traceback

                traceback.print_exc()
                update_progress(task_id, "error", 0, f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        # STT ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        estimated_time = estimate_stt_processing_time(audio_duration)

        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘
        thread = threading.Thread(target=process_in_background)
        thread.daemon = True
        thread.start()

        # ì¦‰ì‹œ task_idì™€ ì˜ˆìƒ ì‹œê°„ ë°˜í™˜
        return jsonify(
            {
                "success": True,
                "processing": True,
                "task_id": task_id,
                "estimated_time": estimated_time,  # ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
                "audio_duration": audio_duration,  # ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
                "message": "ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ì§„í–‰ ìƒí™©ì„ í™•ì¸í•˜ì„¸ìš”.",
            }
        )

    except Exception as e:
        import traceback

        traceback.print_exc()
        return jsonify({"success": False, "error": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500


@app.route("/uploads/<path:filename>")
def serve_audio(filename):
    """ì—…ë¡œë“œëœ ì˜¤ë””ì˜¤ íŒŒì¼ ì œê³µ"""
    uploads_path = os.path.abspath(UPLOADS_FOLDER)
    return send_from_directory(uploads_path, filename)


@app.route("/mp3/<path:filename>")
def serve_mp3(filename):
    """MP3 íŒŒì¼ ì œê³µ"""
    mp3_path = os.path.abspath(MP3_FOLDER)
    return send_from_directory(mp3_path, filename)


@app.route("/api/summarize", methods=["POST"])
def summarize_transcript():
    """íšŒì˜ë¡ ìš”ì•½ API"""
    try:
        data = request.get_json()
        segments = data.get("segments")
        session_id = data.get("session_id")
        title = data.get("title")  # ì œëª© ë°›ê¸°

        if not segments and session_id and session_id in session_data:
            segments = session_data[session_id]["segments"]

        if not segments:
            return (
                jsonify({"success": False, "error": "ìš”ì•½í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}),
                400,
            )

        # ê¸°ì¡´ ìš”ì•½ì´ ìˆëŠ”ì§€ í™•ì¸ (ì¤‘ë³µ ìƒì„± ë°©ì§€)
        if session_id and session_id in session_data:
            source_type = session_data[session_id].get("source_type")
            source_id = None

            if source_type == "youtube":
                source_id = session_data[session_id].get("video_id")
            elif source_type == "audio":
                source_id = session_data[session_id].get("file_hash")

            if source_id:
                # VectorStoreì—ì„œ ê¸°ì¡´ ìš”ì•½ í™•ì¸
                existing_summary = get_summary_from_vectordb(source_id, source_type)
                if existing_summary:
                    logging.info(
                        f"âœ… ê¸°ì¡´ ìš”ì•½ ë°œê²¬ (source: {source_id}), ìƒˆë¡œ ìƒì„±í•˜ì§€ ì•ŠìŒ"
                    )
                    return jsonify(
                        {
                            "success": True,
                            "summary": existing_summary,
                            "from_cache": True,
                        }
                    )

        # segmentsë¥¼ token-based chunksë¡œ ë³€í™˜
        logging.info("ğŸ“¦ Token-based chunks ìƒì„± ì¤‘...")
        chunks = create_token_based_chunks(segments, chunk_size=500, chunk_overlap=100)
        logging.info(f"âœ… {len(chunks)}ê°œ chunks ìƒì„± ì™„ë£Œ")

        # chunksë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (chunk_id í‘œì‹œ)
        chunk_texts = []
        for chunk in chunks:
            # chunk_idë¥¼ ì‚¬ìš©í•˜ì—¬ ì²­í¬ ë²ˆí˜¸ í‘œì‹œ
            chunk_id = chunk["chunk_id"]
            chunk_text = f"[ì²­í¬ {chunk_id}]\n{chunk['text']}"
            chunk_texts.append(chunk_text)

        transcript_text = "\n\n".join(chunk_texts)

        client = get_gemini_client()

        # ì œëª©ì´ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        title_context = f"\níšŒì˜ ì œëª©: {title}\n" if title else ""

        prompt = f"""ë‹¹ì‹ ì€ ì œê³µëœ ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬, êµ¬ì¡°í™”ëœ ì£¼ì œë³„ ìš”ì•½ë³¸ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ì…ë ¥ íŒŒì¼ í˜•ì‹:**
ì…ë ¥ ë‚´ìš©ì€ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¶ì¸ ì²­í¬(chunk) í˜•íƒœì…ë‹ˆë‹¤. ê° ì²­í¬ëŠ” [ì²­í¬ X] í˜•ì‹ìœ¼ë¡œ ì²­í¬ ë²ˆí˜¸ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
ì˜ˆì‹œ: [ì²­í¬ 0], [ì²­í¬ 1], [ì²­í¬ 2] ë“±

**ì¶œë ¥ ìš”êµ¬ì‚¬í•­:**
ë‹¹ì‹ ì€ ì…ë ¥ íŒŒì¼ì„ ë‹¤ìŒê³¼ ê°™ì€ ê·œì¹™ì— ë”°ë¼ ìš”ì•½ë³¸ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.

1.  íšŒì˜ ì œëª© : {title_context}
2.  ì£¼ì œë³„ ê·¸ë£¹í™” : ìŠ¤í¬ë¦½íŠ¸ ì „ì²´ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì£¼ìš” ë…¼ì˜ ì£¼ì œë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
3.  ì†Œì£¼ì œ ì œëª© í˜•ì‹ (ì¤‘ìš”): ê° ì£¼ìš” ì£¼ì œë³„ë¡œ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ì œëª©ì„ **ë°˜ë“œì‹œ "### ì œëª©" í˜•ì‹**ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤. (ì˜ˆ: `### ëŒ€ì£¼ì£¼ ì£¼ì‹ ì–‘ë„ì„¸ ê¸°ì¤€ ë…¼ë€`)
4.  ë‚´ìš© ìš”ì•½: ê° ì£¼ì œ ì œëª© ì•„ë˜ì— ê´€ë ¨ëœ í•µì‹¬ ì£¼ì¥, ì‚¬ì‹¤, ì˜ê²¬ì„ ê¸€ë¨¸ë¦¬ ê¸°í˜¸(`*`)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½í•©ë‹ˆë‹¤.
5.  ë¬¸ì²´ ë³€í™˜: ì›ë³¸ì˜ êµ¬ì–´ì²´(ëŒ€í™”ì²´)ë¥¼ ê°„ê²°í•˜ê³  ê³µì‹ì ì¸ ì„œìˆ í˜• ë¬¸ì–´ì²´(ìš”ì•½ë¬¸ ìŠ¤íƒ€ì¼)ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
6.  í™”ì ë° êµ°ë”ë”ê¸° ì œê±°: 'A:', 'B:'ì™€ ê°™ì€ í™”ì í‘œì‹œì™€ 'ê·¸ëŸ¬ë‹ˆê¹Œ', 'ì–´,', 'ì,', '[ì›ƒìŒ]' ë“± ëŒ€í™”ì˜ êµ°ë”ë”ê¸°ë¥¼ ëª¨ë‘ ì œê±°í•˜ê³  ë‚´ìš©ë§Œ ì •ì œí•˜ì—¬ ìš”ì•½í•©ë‹ˆë‹¤.
7.  ì œëª©ê³¼ ë‚´ìš© ê°„ê²©: ì†Œì£¼ì œ ì œëª©(### ì œëª©)ê³¼ ì²« ë²ˆì§¸ ê¸€ë¨¸ë¦¬ ê¸°í˜¸(*) ì‚¬ì´ì—ëŠ” ê³µë°± ì¤„ì„ ë‘ì§€ ì•ŠìŠµë‹ˆë‹¤. ì œëª© ë°”ë¡œ ë‹¤ìŒ ì¤„ì— ë‚´ìš©ì„ ì‘ì„±í•©ë‹ˆë‹¤.
8.  ë¬¸ë‹¨ ê°„ê²©: ì„œë¡œ ë‹¤ë¥¸ ì†Œì£¼ì œ ì‚¬ì´ì—ëŠ” ì¤„ë°”ê¿ˆì„ 2ê°œ ë„£ì–´ ê°€ë…ì„±ì„ ë†’ì…ë‹ˆë‹¤.
9.  ì •í™•í•œ ì¸ìš© (í•„ìˆ˜):
    * ìš”ì•½ëœ ëª¨ë“  ë¬¸ì¥ì´ë‚˜ êµ¬ì ˆ ëì—ëŠ” ë°˜ë“œì‹œ [ì²­í¬ X]ì— í‘œì‹œëœ ì²­í¬ ë²ˆí˜¸ë¥¼ [cite: X] í˜•ì‹ìœ¼ë¡œ ì¸ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    * í•˜ë‚˜ì˜ ê¸€ë¨¸ë¦¬ ê¸°í˜¸ê°€ ì—¬ëŸ¬ ì²­í¬ì˜ ë‚´ìš©ì„ ì¢…í•©í•œ ê²½ìš°, ëª¨ë“  ê´€ë ¨ ì²­í¬ ë²ˆí˜¸ë¥¼ ì¸ìš©í•´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: `[cite: 0, 1, 2]`)
    * ì¸ìš©ì€ ìš”ì•½ëœ ë‚´ìš©ê³¼ ì›ë³¸ ì†ŒìŠ¤ ê°„ì˜ ì‚¬ì‹¤ ê´€ê³„ê°€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
    * [ì²­í¬ 0]ì˜ ë‚´ìš©ì„ ìš”ì•½í•  ë•ŒëŠ” ë°˜ë“œì‹œ [cite: 0]ìœ¼ë¡œ ì¸ìš©í•©ë‹ˆë‹¤.
    * [ì²­í¬ 1]ê³¼ [ì²­í¬ 2]ì˜ ë‚´ìš©ì„ ì¢…í•©í•  ë•ŒëŠ” [cite: 1, 2]ë¡œ ì¸ìš©í•©ë‹ˆë‹¤.

**ì¶œë ¥ ì˜ˆì‹œ:**
### ì²« ë²ˆì§¸ ì£¼ìš” ì£¼ì œ
* ì²« ë²ˆì§¸ ë…¼ì˜ ë‚´ìš© ìš”ì•½ [cite: 0]
* ë‘ ë²ˆì§¸ ë…¼ì˜ ë‚´ìš© ìš”ì•½ [cite: 1, 2]

### ë‘ ë²ˆì§¸ ì£¼ìš” ì£¼ì œ
* ê´€ë ¨ ë…¼ì˜ ë‚´ìš© ìš”ì•½ [cite: 3, 4]

ì‘ì—… ìˆ˜í–‰:
ì´ì œ ë‹¤ìŒ [ìŠ¤í¬ë¦½íŠ¸ ë‚´ìš©]ì„ ë¶„ì„í•˜ì—¬ ìœ„ì˜ ìš”êµ¬ì‚¬í•­ì„ ëª¨ë‘ ì¤€ìˆ˜í•˜ëŠ” ì£¼ì œë³„ ìš”ì•½ë³¸ì„ ìƒì„±í•´ ì£¼ì‹­ì‹œì˜¤.

[ìŠ¤í¬ë¦½íŠ¸ ë‚´ìš©]
{transcript_text}"""

        logging.info("ğŸ¤– Geminië¡œ ìš”ì•½ ìƒì„± ì¤‘...")

        response = client.models.generate_content(
            model="gemini-2.5-pro",
            contents=prompt,
        )

        summary = response.text.strip()

        # ë””ë²„ê¹…: ìš”ì•½ ë‚´ìš© ì¶œë ¥ ë° íŒŒì¼ ì €ì¥
        print("\n" + "=" * 80)
        print("ìƒì„±ëœ ìš”ì•½ ë‚´ìš©:")
        print("=" * 80)
        print(summary)
        print("=" * 80 + "\n")

        # ìš”ì•½ ë‚´ìš©ì„ íŒŒì¼ë¡œ ì €ì¥ (ë””ë²„ê¹…ìš©)
        try:
            debug_file = (
                f"data/summary_debug_{session_id[:8] if session_id else 'unknown'}.txt"
            )
            with open(debug_file, "w", encoding="utf-8") as f:
                f.write(summary)
            logging.info(f"ğŸ“„ ìš”ì•½ ë‚´ìš©ì´ ë””ë²„ê·¸ íŒŒì¼ì— ì €ì¥ë¨: {debug_file}")
        except Exception as e:
            logging.debug(f"ë””ë²„ê·¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")

        logging.info("âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ")

        # CSV ë° VectorStoreì— ìš”ì•½ ì €ì¥
        if session_id and session_id in session_data:
            source_type = session_data[session_id].get("source_type")

            if source_type == "youtube":
                video_id = session_data[session_id].get("video_id")
                if video_id:
                    try:
                        # CSVì— ì €ì¥
                        history_df = load_youtube_history()
                        mask = history_df["video_id"] == video_id
                        if mask.any():
                            history_df.loc[mask, "summary"] = summary
                            save_youtube_history(history_df)
                            logging.info(
                                f"ğŸ’¾ ìš”ì•½ì´ YouTube CSVì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (video_id: {video_id})"
                            )
                            logging.info(
                                f"â„¹ï¸ VectorStore ì €ì¥ì„ ì›í•˜ì‹œë©´ 'VectorStoreì— ì €ì¥' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”."
                            )

                        # VectorStore ìë™ ì €ì¥ ì œê±°: ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì €ì¥ ë²„íŠ¼ì„ í´ë¦­í•´ì•¼ í•¨
                        # vectordb_success = store_summary_in_vectordb(
                        #     summary=summary,
                        #     source_id=video_id,
                        #     source_type="youtube",
                        #     filename=None,
                        # )
                    except Exception as e:
                        logging.error(f"ìš”ì•½ ì €ì¥ ì˜¤ë¥˜: {e}")

            elif source_type == "audio":
                file_hash = session_data[session_id].get("file_hash")
                filename = session_data[session_id].get("filename")
                if file_hash:
                    try:
                        # CSVì— ì €ì¥
                        history_df = load_audio_history()
                        mask = history_df["file_hash"] == file_hash
                        if mask.any():
                            history_df.loc[mask, "summary"] = summary
                            save_audio_history(history_df)
                            logging.info(
                                f"ğŸ’¾ ìš”ì•½ì´ ì˜¤ë””ì˜¤ CSVì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (file_hash: {file_hash})"
                            )
                            logging.info(
                                f"â„¹ï¸ VectorStore ì €ì¥ì„ ì›í•˜ì‹œë©´ 'VectorStoreì— ì €ì¥' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”."
                            )

                        # VectorStore ìë™ ì €ì¥ ì œê±°: ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì €ì¥ ë²„íŠ¼ì„ í´ë¦­í•´ì•¼ í•¨
                        # vectordb_success = store_summary_in_vectordb(
                        #     summary=summary,
                        #     source_id=file_hash,
                        #     source_type="audio",
                        #     filename=filename,
                        # )
                    except Exception as e:
                        logging.error(f"ìš”ì•½ ì €ì¥ ì˜¤ë¥˜: {e}")

        return jsonify({"success": True, "summary": summary})

    except Exception as e:
        import traceback

        traceback.print_exc()
        return (
            jsonify({"success": False, "error": f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}),
            500,
        )


@app.route("/api/chat", methods=["POST"])
def chat_with_transcript():
    """íšŒì˜ë¡ ê¸°ë°˜ ì±„íŒ… API (RAG)"""
    try:
        data = request.get_json()
        user_message = data.get("message", "").strip()

        if not user_message:
            return jsonify({"success": False, "error": "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400

        session_id = data.get("session_id")
        chat_history = data.get("chat_history", [])

        if not session_id or session_id not in session_data:
            return jsonify({"success": False, "error": "ì„¸ì…˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 400

        session_info = session_data[session_id]
        source_type = session_info.get("source_type")
        chat_history = session_info.get("chat_history", [])

        # source_id ê°€ì ¸ì˜¤ê¸°
        if source_type == "youtube":
            source_id = session_info.get("video_id")
        elif source_type == "audio":
            source_id = session_info.get("file_hash")
        else:
            return (
                jsonify({"success": False, "error": "ì•Œ ìˆ˜ ì—†ëŠ” ì†ŒìŠ¤ íƒ€ì…ì…ë‹ˆë‹¤."}),
                400,
            )

        # VectorDBì—ì„œ ê´€ë ¨ ì„¸ê·¸ë¨¼íŠ¸ ê²€ìƒ‰ (RAG)
        logging.info(f"ğŸ” VectorDB ê²€ìƒ‰: {user_message}")
        search_results = search_vectordb(
            query=user_message,
            source_id=source_id,
            source_type=source_type,
            n_results=5,
        )

        if not search_results:
            return (
                jsonify(
                    {"success": False, "error": "ê´€ë ¨ íšŒì˜ë¡ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
                ),
                400,
            )

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
        context_text = "\n\n".join(
            [
                f"í™”ì {result['metadata']['speaker']} ({result['metadata']['start_time']:.1f}ì´ˆ): {result['document']}"
                for result in search_results
            ]
        )

        logging.info(f"ğŸ“ ê²€ìƒ‰ëœ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(search_results)}")

        # ì´ì „ ëŒ€í™” ë‚´ì—­
        history_text = ""
        if chat_history:
            history_text = "\n\nì´ì „ ëŒ€í™” ë‚´ì—­:\n"
            for hist in chat_history[-5:]:
                history_text += f"ì‚¬ìš©ì: {hist['user']}\n"
                history_text += f"AI: {hist['assistant']}\n\n"

        client = get_gemini_client()

        prompt = f"""ë‹¹ì‹ ì€ íšŒì˜ë¡ ë¶„ì„ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ íšŒì˜ë¡ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.

ê´€ë ¨ íšŒì˜ë¡ ë‚´ìš©:
{context_text}
{history_text}
ì‚¬ìš©ì ì§ˆë¬¸: {user_message}

ë‹µë³€ ì‹œ ë‹¤ìŒì„ ìœ ì˜í•´ ì£¼ì„¸ìš”:
1. ì œê³µëœ íšŒì˜ë¡ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
2. í•„ìš”í•œ ê²½ìš° í™”ìì™€ ì‹œê°„ ì •ë³´ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”.
3. ì œê³µëœ ë‚´ìš©ì— ì—†ëŠ” ê²ƒì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  "ì œê³µëœ íšŒì˜ë¡ì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
4. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."""

        logging.info(f"ğŸ¤– ì‚¬ìš©ì ì§ˆë¬¸: {user_message}")

        response = client.models.generate_content(
            model="gemini-2.5-pro",
            contents=prompt,
        )

        assistant_response = response.text.strip()
        logging.info(f"âœ… AI ì‘ë‹µ ìƒì„± ì™„ë£Œ (RAG ê¸°ë°˜)")

        chat_history.append({"user": user_message, "assistant": assistant_response})

        session_data[session_id]["chat_history"] = chat_history

        return jsonify(
            {
                "success": True,
                "response": assistant_response,
                "chat_history": chat_history,
                "search_results": len(search_results),  # ë””ë²„ê¹…ìš©
            }
        )

    except Exception as e:
        import traceback

        traceback.print_exc()
        return (
            jsonify(
                {"success": False, "error": f"ì±„íŒ… ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
            ),
            500,
        )


@app.route("/api/history", methods=["GET"])
def get_history():
    """ì²˜ë¦¬ ì´ë ¥ ì¡°íšŒ API"""
    try:
        youtube_history = load_youtube_history()
        audio_history = load_audio_history()

        # DataFrameì„ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        youtube_list = youtube_history.to_dict("records")
        audio_list = audio_history.to_dict("records")

        return jsonify(
            {
                "success": True,
                "youtube_history": youtube_list,
                "audio_history": audio_list,
                "total_youtube": len(youtube_list),
                "total_audio": len(audio_list),
            }
        )
    except Exception as e:
        logging.error(f"ì´ë ¥ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/progress/<task_id>", methods=["GET"])
def get_progress(task_id):
    """ì§„í–‰ ìƒí™© ì¡°íšŒ API"""
    try:
        if task_id not in progress_data:
            return jsonify({"success": False, "error": "ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404

        return jsonify(
            {"success": True, "task_id": task_id, "progress": progress_data[task_id]}
        )
    except Exception as e:
        logging.error(f"ì§„í–‰ ìƒí™© ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/save-to-vectorstore", methods=["POST"])
def save_to_vectorstore():
    """ì„¸ê·¸ë¨¼íŠ¸ì™€ ìš”ì•½ì„ VectorStoreì— ì €ì¥"""
    try:
        data = request.get_json()
        source_id = data.get("source_id", "").strip()
        source_type = data.get("source_type", "").strip()
        segments = data.get("segments", [])
        title = data.get("title", "").strip()
        summary = data.get("summary", "")  # strip() ì œê±°í•˜ì—¬ ì›ë³¸ ìœ ì§€
        filename = data.get("filename", None)

        # ë””ë²„ê¹…: ìš”ì•½ ìˆ˜ì‹  í™•ì¸
        logging.info(f"ğŸ“¥ ìš”ì•½ ìˆ˜ì‹  í™•ì¸: type={type(summary)}, length={len(summary) if summary else 0}")
        if summary:
            logging.info(f"ğŸ“ ìš”ì•½ ë¯¸ë¦¬ë³´ê¸° (ì²« 200ì): {summary[:200]}...")
        else:
            logging.warning("âš ï¸ ìš”ì•½ì´ ë¹„ì–´ìˆê±°ë‚˜ Noneì…ë‹ˆë‹¤!")

        if not source_id or not source_type:
            return (
                jsonify(
                    {"success": False, "error": "source_idì™€ source_typeì´ í•„ìš”í•©ë‹ˆë‹¤."}
                ),
                400,
            )

        if not segments:
            return (
                jsonify({"success": False, "error": "ì €ì¥í•  ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."}),
                400,
            )

        logging.info(f"ğŸ“¦ VectorStore ì €ì¥ ì‹œì‘: source_id={source_id}, title={title}")

        # 1. ì„¸ê·¸ë¨¼íŠ¸ë¥¼ VectorStoreì— ì €ì¥
        vectordb_success = store_segments_in_vectordb(
            segments=segments,
            source_id=source_id,
            source_type=source_type,
            filename=filename,
            title=title if title else None,
        )

        if not vectordb_success:
            return (
                jsonify({"success": False, "error": "ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}),
                500,
            )

        # 2. ìš”ì•½ì´ ìˆìœ¼ë©´ Summary VectorStoreì—ë„ ì €ì¥
        summary_saved = False
        if summary and summary.strip():  # ë¹ˆ ë¬¸ìì—´ ì²´í¬ ê°•í™”
            logging.info(f"ğŸ’¾ ìš”ì•½ ì €ì¥ ì‹œë„: {len(summary)} ë¬¸ì")
            summary_saved = store_summary_in_vectordb(
                summary=summary,
                source_id=source_id,
                source_type=source_type,
                filename=filename,
            )
            if summary_saved:
                logging.info(f"âœ… ìš”ì•½ì´ Summary VectorStoreì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                logging.error(f"âŒ ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: store_summary_in_vectordbê°€ False ë°˜í™˜")
        else:
            logging.warning(f"âš ï¸ ìš”ì•½ì´ ë¹„ì–´ìˆì–´ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (summary={repr(summary)})")

        logging.info(
            f"âœ… VectorStore ì €ì¥ ì™„ë£Œ: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸, ìš”ì•½ ì €ì¥={summary_saved}"
        )

        return jsonify(
            {
                "success": True,
                "message": f"VectorStoreì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (ì„¸ê·¸ë¨¼íŠ¸: {len(segments)}ê°œ)",
                "segments_saved": len(segments),
                "summary_saved": summary_saved,
            }
        )

    except Exception as e:
        import traceback

        traceback.print_exc()
        logging.error(f"âŒ VectorStore ì €ì¥ ì˜¤ë¥˜: {e}")
        return (
            jsonify(
                {"success": False, "error": f"VectorStore ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
            ),
            500,
        )


@app.route("/api/update-title", methods=["POST"])
def update_title():
    """VectorStoreì˜ ì„¸ê·¸ë¨¼íŠ¸ ë©”íƒ€ë°ì´í„°ì— ì œëª© ì—…ë°ì´íŠ¸"""
    try:
        data = request.get_json()
        source_id = data.get("source_id", "").strip()
        source_type = data.get("source_type", "").strip()
        title = data.get("title", "").strip()

        if not source_id or not source_type:
            return (
                jsonify(
                    {"success": False, "error": "source_idì™€ source_typeì´ í•„ìš”í•©ë‹ˆë‹¤."}
                ),
                400,
            )

        if not title:
            return jsonify({"success": False, "error": "ì œëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400

        # VectorStore ì„ íƒ
        vectorstore = (
            youtube_vectorstore if source_type == "youtube" else audio_vectorstore
        )

        if not vectorstore:
            return (
                jsonify(
                    {"success": False, "error": "VectorStoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
                ),
                500,
            )

        # í•´ë‹¹ source_idì˜ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        existing_docs = vectorstore.get(where={"source_id": source_id})

        if not existing_docs or not existing_docs["ids"]:
            return (
                jsonify(
                    {
                        "success": False,
                        "error": "í•´ë‹¹ source_idì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    }
                ),
                404,
            )

        # ê° ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ì— title ì¶”ê°€
        # LangChain ChromaëŠ” ì§ì ‘ì ì¸ metadata ì—…ë°ì´íŠ¸ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ
        # ë‚´ë¶€ _collectionì„ ì‚¬ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸
        updated_metadatas = []
        for i in range(len(existing_docs["ids"])):
            metadata = existing_docs["metadatas"][i].copy()
            metadata["title"] = title
            updated_metadatas.append(metadata)

        # Chroma collectionì˜ update ë©”ì„œë“œ ì‚¬ìš©
        vectorstore._collection.update(
            ids=existing_docs["ids"], metadatas=updated_metadatas
        )
        updated_count = len(existing_docs["ids"])

        logging.info(
            f"âœ… ì œëª© ì—…ë°ì´íŠ¸ ì™„ë£Œ: {updated_count}ê°œ ì„¸ê·¸ë¨¼íŠ¸ (source: {source_id}, title: {title})"
        )

        return jsonify(
            {
                "success": True,
                "message": f"ì œëª©ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤: {title}",
                "updated_count": updated_count,
            }
        )

    except Exception as e:
        import traceback

        traceback.print_exc()
        logging.error(f"âŒ ì œëª© ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
        return (
            jsonify(
                {"success": False, "error": f"ì œëª© ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
            ),
            500,
        )


@app.route("/api/retriever-search", methods=["POST"])
def retriever_search():
    """VectorStore ê²€ìƒ‰ API (Retriever ì‚¬ìš©) - ì²­í¬ ë°˜í™˜"""
    try:
        data = request.get_json()
        query = data.get("query", "").strip()
        source_type = data.get("source_type", None)  # Noneì´ë©´ ì „ì²´ ê²€ìƒ‰
        n_results = data.get("n_results", 5)

        if not query:
            return jsonify({"success": False, "error": "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400

        # VectorDB ê²€ìƒ‰ ìˆ˜í–‰ (chunkë§Œ ê²€ìƒ‰)
        logging.info(
            f"ğŸ” Retriever ê²€ìƒ‰: '{query}' (source_type: {source_type}, n_results: {n_results}, document_type: chunk)"
        )

        search_results = search_vectordb(
            query=query,
            source_id=None,  # íŠ¹ì • sourceë¡œ ì œí•œí•˜ì§€ ì•ŠìŒ
            source_type=source_type,  # youtube, audio ë˜ëŠ” None (ì „ì²´)
            n_results=n_results,
            document_type="chunk",  # chunkë§Œ ê²€ìƒ‰
        )

        logging.info(f"âœ… ì²­í¬ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")

        # ì²­í¬ ê²°ê³¼ë¥¼ í¬ë§·íŒ…
        chunk_results = []
        for result in search_results:
            metadata = result.get("metadata", {})
            chunk_id = metadata.get("chunk_id", 0)

            chunk_info = {
                "id": chunk_id,
                "document": result.get("document", ""),
                "metadata": metadata,
                "distance": result.get("distance", 0.0),
            }

            chunk_results.append(chunk_info)

        logging.info(f"âœ… ì²­í¬ ê²°ê³¼ í¬ë§·íŒ… ì™„ë£Œ: {len(chunk_results)}ê°œ")

        return jsonify(
            {
                "success": True,
                "results": chunk_results,
                "total": len(chunk_results),
                "query": query,
            }
        )

    except Exception as e:
        import traceback

        traceback.print_exc()
        logging.error(f"âŒ Retriever ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return jsonify({"success": False, "error": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500


@app.route("/api/ask_content", methods=["POST"])
def ask_content():
    """ë‚´ìš© ì§ˆë¬¸ API (RAG ê¸°ë°˜ - Citation ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ì¡°íšŒ)"""
    try:
        data = request.get_json()
        question = data.get("question", "").strip()
        summary_n = data.get("summary_n", 5)  # ìš”ì•½ ê²€ìƒ‰ ê°œìˆ˜

        if not question:
            return jsonify({"success": False, "error": "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400

        logging.info(f"ğŸ’¬ ë‚´ìš© ì§ˆë¬¸: '{question}' (ìš”ì•½: {summary_n}ê°œ)")

        # 1. ìš”ì•½ ê²€ìƒ‰ë§Œ ìˆ˜í–‰
        logging.info("ğŸ” ìš”ì•½ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
        summary_results = search_vectordb(
            query=question,
            source_id=None,
            source_type="summary",
            n_results=summary_n,
        )
        logging.info(f"âœ… ìš”ì•½ ê²€ìƒ‰ ì™„ë£Œ: {len(summary_results)}ê°œ")

        # 2. ìš”ì•½ì—ì„œ citation ì¶”ì¶œ (ì²­í¬ ë²ˆí˜¸ ê¸°ë°˜)
        import re
        import json
        cited_segments = []
        segment_ids_to_fetch = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ set

        for summary_result in summary_results:
            summary_text = summary_result.get("document", "")
            metadata = summary_result.get("metadata", {})
            source_id = metadata.get("source_id")
            source_type = metadata.get("source_type")

            # 1ì°¨: ë©”íƒ€ë°ì´í„°ì—ì„œ cited_chunk_ids ê°€ì ¸ì˜¤ê¸°
            cited_chunk_ids = metadata.get("cited_chunk_ids", [])

            # filter_complex_metadataëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ë¯€ë¡œ íŒŒì‹± í•„ìš”
            if isinstance(cited_chunk_ids, str):
                try:
                    cited_chunk_ids = json.loads(cited_chunk_ids)
                    logging.debug(f"ğŸ”„ cited_chunk_idsë¥¼ ë¬¸ìì—´ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜: {cited_chunk_ids}")
                except Exception as e:
                    logging.warning(f"âš ï¸ cited_chunk_ids íŒŒì‹± ì‹¤íŒ¨: {e}")
                    cited_chunk_ids = []

            if not cited_chunk_ids:
                # 2ì°¨: í…ìŠ¤íŠ¸ì—ì„œ [cite: X] ì •ê·œì‹ìœ¼ë¡œ íŒŒì‹± (fallback)
                citations = re.findall(r'\[cite:\s*(\d+(?:\s*,\s*\d+)*)\]', summary_text)

                if citations:
                    for citation in citations:
                        ids = [int(cid.strip()) for cid in citation.split(',')]
                        cited_chunk_ids.extend(ids)

            if cited_chunk_ids:
                logging.info(f"ğŸ“Œ Citation ë°œê²¬: source_id={source_id}, chunk_ids={cited_chunk_ids}")

                # 3. ê° ì²­í¬ì—ì„œ segment_ids ì¶”ì¶œ
                vectorstore = youtube_vectorstore if source_type == "youtube" else audio_vectorstore

                for chunk_id in cited_chunk_ids:
                    try:
                        # VectorStoreì—ì„œ ì²­í¬ ì¡°íšŒ
                        chunk_doc_id = f"{source_id}_chunk_{chunk_id}"
                        chunk_results = vectorstore.get(ids=[chunk_doc_id])

                        if chunk_results and chunk_results["documents"]:
                            chunk_metadata = chunk_results["metadatas"][0]
                            seg_ids = chunk_metadata.get("segment_ids", [])

                            # JSON ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
                            if isinstance(seg_ids, str):
                                try:
                                    seg_ids = json.loads(seg_ids)
                                except:
                                    seg_ids = []

                            # segment_idsë¥¼ ì¡°íšŒ ëª©ë¡ì— ì¶”ê°€
                            for seg_id in seg_ids:
                                segment_ids_to_fetch.add((source_id, source_type, seg_id))

                            logging.debug(f"âœ… ì²­í¬ {chunk_id}ì—ì„œ segment_ids ì¶”ì¶œ: {seg_ids}")
                        else:
                            logging.warning(f"âš ï¸ ì²­í¬ {chunk_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    except Exception as e:
                        logging.warning(f"âš ï¸ ì²­í¬ {chunk_id} ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # ì¤‘ë³µ ì œê±°ëœ segment_idë“¤ì„ ì¡°íšŒ
        logging.info(f"ğŸ” ì´ {len(segment_ids_to_fetch)}ê°œì˜ ê³ ìœ í•œ ì„¸ê·¸ë¨¼íŠ¸ ì¡°íšŒ í•„ìš”")

        # YouTube URL ì¡°íšŒë¥¼ ìœ„í•œ ìºì‹œ (video_id -> youtube_url)
        youtube_url_cache = {}

        for source_id, source_type, seg_id in segment_ids_to_fetch:
            vectorstore = youtube_vectorstore if source_type == "youtube" else audio_vectorstore

            segment_found = False

            # 1ì°¨ ì‹œë„: VectorStoreì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ì¡°íšŒ (ê¸°ì¡´ ë°ì´í„° í˜¸í™˜ì„±)
            if vectorstore:
                try:
                    doc_id = f"{source_id}_seg_{seg_id}"
                    results = vectorstore.get(ids=[doc_id])

                    if results and results["documents"]:
                        metadata = results["metadatas"][0]
                        start_time = metadata.get("start_time", 0)

                        segment_info = {
                            "id": seg_id,
                            "document": results["documents"][0],
                            "metadata": metadata,
                            "distance": 0.0,  # citationì´ë¯€ë¡œ ê±°ë¦¬ 0
                        }

                        # ìœ íŠœë¸Œì¸ ê²½ìš° íƒ€ì„ìŠ¤íƒ¬í”„ ë§í¬ ìƒì„±
                        if source_type == "youtube":
                            # ìºì‹œì—ì„œ URL ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ CSVì—ì„œ ì¡°íšŒ)
                            if source_id not in youtube_url_cache:
                                try:
                                    history_df = load_youtube_history()
                                    url_row = history_df[history_df["video_id"] == source_id]
                                    if not url_row.empty:
                                        youtube_url_cache[source_id] = url_row.iloc[0]["youtube_url"]
                                except Exception as e:
                                    logging.warning(f"âš ï¸ YouTube URL ì¡°íšŒ ì‹¤íŒ¨ (video_id: {source_id}): {e}")

                            youtube_url = youtube_url_cache.get(source_id)
                            if youtube_url:
                                # íƒ€ì„ìŠ¤íƒ¬í”„ ë§í¬ ìƒì„± (ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜)
                                timestamp_seconds = int(start_time)
                                timestamp_link = f"{youtube_url}&t={timestamp_seconds}s"
                                segment_info["timestamp_link"] = timestamp_link
                                logging.debug(f"ğŸ”— íƒ€ì„ìŠ¤íƒ¬í”„ ë§í¬ ìƒì„±: {timestamp_link}")

                        cited_segments.append(segment_info)
                        segment_found = True
                        logging.debug(f"âœ… Segment {seg_id} VectorStoreì—ì„œ ì¡°íšŒ ì„±ê³µ")
                except Exception as e:
                    logging.debug(f"VectorStoreì—ì„œ Segment {seg_id} ì¡°íšŒ ì‹¤íŒ¨: {e}")

            # 2ì°¨ ì‹œë„: CSVì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ì¡°íšŒ (ì²­í‚¹ ì‚¬ìš© ì‹œ)
            if not segment_found:
                try:
                    segment = get_segment_from_csv(source_id, source_type, seg_id)

                    if segment:
                        start_time = segment.get("start_time", 0)

                        segment_info = {
                            "id": seg_id,
                            "document": segment.get("text", ""),
                            "metadata": {
                                "source_id": source_id,
                                "source_type": source_type,
                                "speaker": str(segment.get("speaker", "")),
                                "start_time": float(start_time),
                                "confidence": float(segment.get("confidence", 0.0)),
                                "segment_id": seg_id,
                            },
                            "distance": 0.0,
                        }

                        # ìœ íŠœë¸Œì¸ ê²½ìš° íƒ€ì„ìŠ¤íƒ¬í”„ ë§í¬ ìƒì„±
                        if source_type == "youtube":
                            if source_id not in youtube_url_cache:
                                try:
                                    history_df = load_youtube_history()
                                    url_row = history_df[history_df["video_id"] == source_id]
                                    if not url_row.empty:
                                        youtube_url_cache[source_id] = url_row.iloc[0]["youtube_url"]
                                except Exception as e:
                                    logging.warning(f"âš ï¸ YouTube URL ì¡°íšŒ ì‹¤íŒ¨ (video_id: {source_id}): {e}")

                            youtube_url = youtube_url_cache.get(source_id)
                            if youtube_url:
                                timestamp_seconds = int(start_time)
                                timestamp_link = f"{youtube_url}&t={timestamp_seconds}s"
                                segment_info["timestamp_link"] = timestamp_link
                                logging.debug(f"ğŸ”— íƒ€ì„ìŠ¤íƒ¬í”„ ë§í¬ ìƒì„±: {timestamp_link}")

                        cited_segments.append(segment_info)
                        segment_found = True
                        logging.debug(f"âœ… Segment {seg_id} CSVì—ì„œ ì¡°íšŒ ì„±ê³µ")
                except Exception as e:
                    logging.warning(f"âš ï¸ CSVì—ì„œ Segment {seg_id} ì¡°íšŒ ì‹¤íŒ¨: {e}")

            if not segment_found:
                logging.warning(f"âš ï¸ Segment {seg_id} ì¡°íšŒ ì‹¤íŒ¨ (VectorStore ë° CSV ëª¨ë‘)")

        # cited_segmentsë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        cited_segments.sort(key=lambda x: x["metadata"].get("start_time", 0))

        logging.info(f"âœ… Citation ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ì¡°íšŒ ì™„ë£Œ: {len(cited_segments)}ê°œ")

        # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ìš”ì•½ë§Œ ì‚¬ìš©)
        summary_context = "\n\n".join(
            [
                f"[ìš”ì•½ ê²€ìƒ‰ ê²°ê³¼ {i+1}]\nì¶œì²˜: {r.get('metadata', {}).get('source_id', 'Unknown')}\nì œëª©: {r.get('metadata', {}).get('subtopic', 'ì „ì²´')}\në‚´ìš©: {r.get('document', '')}"
                for i, r in enumerate(summary_results)
            ]
        )

        # 4. RAG í”„ë¡¬í”„íŠ¸ ìƒì„± (ìš”ì•½ë§Œ ì‚¬ìš©)
        rag_prompt = f"""ì•„ë˜ íšŒì˜ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

**ì§ˆë¬¸:**
{question}

**ì°¸ê³  ìë£Œ: íšŒì˜ ìš”ì•½**
{summary_context if summary_context else "(ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)"}

**ë‹µë³€ ì‘ì„± ë°©ë²•:**
1. ì°¸ê³  ìë£Œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
2. ë¶ˆí•„ìš”í•œ ì¸ì‚¬ë§ì´ë‚˜ "AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤" ê°™ì€ ìê¸°ì†Œê°œëŠ” ìƒëµí•˜ê³ , ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ë§Œ ë°”ë¡œ ì‹œì‘í•˜ì„¸ìš”.
3. ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì‰¬ìš´ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
4. ë‹µë³€ì€ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš” (ì œëª©, ë¦¬ìŠ¤íŠ¸, ê°•ì¡° ë“± í™œìš©).
5. í•„ìš”ì‹œ ê¸€ë¨¸ë¦¬ ê¸°í˜¸(-)ë‚˜ ë²ˆí˜¸ ëª©ë¡ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”í•˜ì„¸ìš”.
6. ì¤‘ìš”í•œ ì •ë³´ëŠ” **êµµê²Œ** í‘œì‹œí•˜ì„¸ìš”.
7. ì°¸ê³  ìë£Œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´, "ì œê³µëœ ìë£Œì—ëŠ” í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ê°„ë‹¨íˆ ë‹µë³€í•˜ì„¸ìš”.

ë‹µë³€:"""

        # 5. Gemini API í˜¸ì¶œ
        logging.info("ğŸ¤– Gemini API í˜¸ì¶œ ì¤‘...")
        client = get_gemini_client()
        response = client.models.generate_content(
            model="gemini-2.5-pro", contents=rag_prompt
        )

        answer = response.text.strip()
        logging.info("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")

        return jsonify(
            {
                "success": True,
                "answer": answer,
                "question": question,
                "transcript_results_count": len(cited_segments),
                "summary_results_count": len(summary_results),
                "transcript_results": cited_segments,  # citation ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸
                "summary_results": summary_results,
            }
        )

    except Exception as e:
        import traceback

        traceback.print_exc()
        logging.error(f"âŒ ë‚´ìš© ì§ˆë¬¸ ì˜¤ë¥˜: {e}")
        return (
            jsonify({"success": False, "error": f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}),
            500,
        )


# ==================== ë°ì´í„° ê´€ë¦¬ API ====================

@app.route("/api/data-management/list", methods=["GET"])
def api_data_management_list():
    """ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
    try:
        from modules.sqlite_db import load_youtube_data, load_audio_data, get_database_stats

        # YouTube ëª©ë¡
        youtube_data = load_youtube_data()
        youtube_list = []
        for item in youtube_data:
            youtube_list.append({
                'id': item['video_id'],
                'type': 'youtube',
                'title': item['title'],
                'channel': item['channel'],
                'view_count': item['view_count'],
                'upload_date': item['upload_date'],
                'stt_service': item['stt_service'],
                'stt_time': item['stt_processing_time'],
                'segments_count': len(item['segments']),
                'created_at': item['created_at'],
                'has_summary': bool(item['summary'])
            })

        # ì˜¤ë””ì˜¤ ëª©ë¡
        audio_data = load_audio_data()
        audio_list = []
        for item in audio_data:
            audio_list.append({
                'id': item['file_hash'],
                'type': 'audio',
                'filename': item['filename'],
                'file_path': item['file_path'],
                'file_size': item['file_size'],
                'duration': item['audio_duration'],
                'stt_service': item['stt_service'],
                'stt_time': item['stt_processing_time'],
                'segments_count': len(item['segments']),
                'created_at': item['created_at'],
                'has_summary': bool(item['summary'])
            })

        # í†µê³„
        stats = get_database_stats()

        return jsonify({
            'success': True,
            'youtube': youtube_list,
            'audio': audio_list,
            'stats': stats
        })

    except Exception as e:
        logging.error(f"ë°ì´í„° ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route("/api/data-management/detail/<data_type>/<data_id>", methods=["GET"])
def api_data_management_detail(data_type, data_id):
    """ë°ì´í„° ìƒì„¸ ì¡°íšŒ"""
    try:
        from modules.sqlite_db import load_youtube_data, load_audio_data

        if data_type == 'youtube':
            data = load_youtube_data(video_id=data_id)
            if data and len(data) > 0:
                item = data[0]
                return jsonify({
                    'success': True,
                    'data': {
                        'type': 'youtube',
                        'video_id': item['video_id'],
                        'url': item['youtube_url'],
                        'title': item['title'],
                        'channel': item['channel'],
                        'view_count': item['view_count'],
                        'upload_date': item['upload_date'],
                        'mp3_path': item['mp3_path'],
                        'stt_service': item['stt_service'],
                        'stt_processing_time': item['stt_processing_time'],
                        'created_at': item['created_at'],
                        'summary': item['summary'],
                        'segments': item['segments']
                    }
                })
        elif data_type == 'audio':
            data = load_audio_data(file_hash=data_id)
            if data and len(data) > 0:
                item = data[0]
                return jsonify({
                    'success': True,
                    'data': {
                        'type': 'audio',
                        'file_hash': item['file_hash'],
                        'filename': item['filename'],
                        'file_path': item['file_path'],
                        'file_size': item['file_size'],
                        'audio_duration': item['audio_duration'],
                        'stt_service': item['stt_service'],
                        'stt_processing_time': item['stt_processing_time'],
                        'created_at': item['created_at'],
                        'summary': item['summary'],
                        'segments': item['segments']
                    }
                })

        return jsonify({'success': False, 'error': 'ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404

    except Exception as e:
        logging.error(f"ë°ì´í„° ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route("/api/data-management/delete", methods=["POST"])
def api_data_management_delete():
    """ë°ì´í„° ì‚­ì œ (SQLite + VectorStore + ì‹¤ì œ íŒŒì¼)"""
    try:
        from modules.sqlite_db import delete_youtube_by_video_id, delete_audio_by_file_hash

        data = request.get_json()
        data_type = data.get('type')
        data_id = data.get('id')

        if not data_type or not data_id:
            return jsonify({'success': False, 'error': 'íƒ€ì…ê³¼ IDê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400

        file_path = None
        deleted_count = 0

        # 1. SQLiteì—ì„œ ì‚­ì œ (íŒŒì¼ ê²½ë¡œ ë°˜í™˜)
        if data_type == 'youtube':
            db_success, file_path = delete_youtube_by_video_id(data_id)
            source_type = 'youtube'
        elif data_type == 'audio':
            db_success, file_path = delete_audio_by_file_hash(data_id)
            source_type = 'audio'
        else:
            return jsonify({'success': False, 'error': 'ì˜ëª»ëœ íƒ€ì…ì…ë‹ˆë‹¤.'}), 400

        if not db_success:
            return jsonify({'success': False, 'error': 'ì‚­ì œí•  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404

        # 2. VectorStoreì—ì„œ ì‚­ì œ
        try:
            vectorstore_success, deleted_count = delete_from_vectorstore(data_id, source_type)
            if vectorstore_success:
                logging.info(f"âœ… VectorStore ì‚­ì œ ì™„ë£Œ: {deleted_count}ê°œ ë¬¸ì„œ")
            else:
                logging.warning(f"âš ï¸ VectorStore ì‚­ì œ ì‹¤íŒ¨ (DBëŠ” ì‚­ì œë¨)")
        except Exception as vs_error:
            logging.error(f"âš ï¸ VectorStore ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {vs_error} (DBëŠ” ì‚­ì œë¨)")

        # 3. ì‹¤ì œ íŒŒì¼ ì‚­ì œ
        file_deleted = False
        if file_path:
            # Windows ê²½ë¡œë¥¼ Unix ê²½ë¡œë¡œ ì •ê·œí™” (WSL í˜¸í™˜)
            normalized_path = file_path.replace('\\', '/')

            if os.path.exists(normalized_path):
                try:
                    os.remove(normalized_path)
                    file_deleted = True
                    logging.info(f"ğŸ—‘ï¸ ì‹¤ì œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {normalized_path}")
                except Exception as file_error:
                    logging.error(f"âš ï¸ íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {file_error} (ê²½ë¡œ: {normalized_path})")
            else:
                logging.warning(f"âš ï¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {normalized_path}")

        message = f'ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. (VectorStore: {deleted_count}ê°œ ë¬¸ì„œ'
        if file_deleted:
            message += ', íŒŒì¼ ì‚­ì œ ì™„ë£Œ'
        message += ')'

        return jsonify({
            'success': True,
            'message': message
        })

    except Exception as e:
        logging.error(f"ë°ì´í„° ì‚­ì œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¬ ì˜ìƒ/ì˜¤ë””ì˜¤ ê²€ìƒ‰ ì—”ì§„ ì‹œì‘")
    print("=" * 60)
    print("URL: http://localhost:5002")
    print("=" * 60)
    print("ê¸°ëŠ¥:")
    print("  [ì˜ìƒ ê²€ìƒ‰ ì—”ì§„]")
    print("  - YouTube ì˜ìƒ ë‹¤ìš´ë¡œë“œ (mp4 í´ë”)")
    print("  - MP3 ë³€í™˜ (mp3 í´ë”)")
    print("  - STT íšŒì˜ë¡ ìƒì„±")
    print("")
    print("  [ì˜¤ë””ì˜¤ ê²€ìƒ‰ ì—”ì§„]")
    print("  - ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ (uploads í´ë”)")
    print("  - STT íšŒì˜ë¡ ìƒì„±")
    print("")
    print("  [Retriever ê²€ìƒ‰ ì—”ì§„]")
    print("  - YouTube/Audio VectorStore í†µí•© ê²€ìƒ‰")
    print("  - LangChain Retriever ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰")
    print("  - ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ì •ë³´ ì œê³µ")
    print("")
    print("  [ê³µí†µ ê¸°ëŠ¥]")
    print("  - íšŒì˜ë¡ ìš”ì•½")
    print("  - AI ì±„íŒ… (RAG ê¸°ë°˜)")
    print("  - VectorStore (LangChain + ChromaDB)")
    print("  - ì²˜ë¦¬ ì´ë ¥ ìºì‹± (SQLite)")
    print("=" * 60)

    # LangChain VectorStore ì´ˆê¸°í™”
    initialize_collections()

    app.run(host="0.0.0.0", port=5002, debug=True)
