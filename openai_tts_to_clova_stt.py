"""
íšŒì˜ë¡ TTS-STT ì •í™•ë„ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. txt íŒŒì¼ì—ì„œ íšŒì˜ë¡ ìŠ¤í¬ë¦½íŠ¸ ì½ê¸°
2. OpenAI TTS APIë¡œ ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„±
3. Clova Speech APIë¡œ ì˜¤ë””ì˜¤ ì¸ì‹í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
4. ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ì™€ ì¸ì‹ëœ ìŠ¤í¬ë¦½íŠ¸ ë¹„êµí•˜ì—¬ ì •í™•ë„ ì¸¡ì •
"""

import os
import re
import io
import json
import pandas as pd
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from openai import OpenAI
from pydub import AudioSegment
from myClovaSpeech import ClovaSpeechClient
import argparse
from difflib import SequenceMatcher


# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


# OpenAI TTS ì„¤ì •
TTS_MODEL = "tts-1-hd"  # ë˜ëŠ” "tts-1"
SPEAKER_VOICES = {
    "Speaker 1": "alloy",  # ì¤‘ì„±ì ì¸ ìŒì„±
    "Speaker 2": "echo",   # ë‚¨ì„±ì ì¸ ìŒì„±
    "Speaker 3": "fable",  # ì˜êµ­ì‹ ì–µì–‘
    "Speaker 4": "onyx",   # ê¹Šì€ ë‚¨ì„± ìŒì„±
}


def parse_meeting_text(text_content: str) -> List[Dict[str, str]]:
    """
    íšŒì˜ë¡ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ í™”ìì™€ ëŒ€í™” ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        text_content: íšŒì˜ë¡ í…ìŠ¤íŠ¸ (í˜•ì‹: "Speaker 1: ì•ˆë…•í•˜ì„¸ìš”")

    Returns:
        íŒŒì‹±ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ [{"speaker": "Speaker 1", "transcript": "ì•ˆë…•í•˜ì„¸ìš”"}, ...]
    """
    parsed_data = []
    file_like_object = io.StringIO(text_content)

    for line in file_like_object:
        clean_line = line.strip()

        # ë¹ˆ ì¤„ì€ ê±´ë„ˆëœ€
        if not clean_line:
            continue

        # "Speaker X: " í˜•ì‹ íŒŒì‹±
        parts = clean_line.split(": ", 1)

        if len(parts) == 2:
            speaker = parts[0].strip()
            transcript = parts[1].strip()
            parsed_data.append({"speaker": speaker, "transcript": transcript})

    return parsed_data


def merge_consecutive_speakers(parsed_data: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """
    ì—°ì†ì ìœ¼ë¡œ ë™ì¼í•œ í™”ìì˜ ë°œì–¸ì„ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.

    Args:
        parsed_data: íŒŒì‹±ëœ íšŒì˜ë¡ ë°ì´í„° [{"speaker": "Speaker 1", "transcript": "ì•ˆë…•"}, ...]

    Returns:
        ë³‘í•©ëœ íšŒì˜ë¡ ë°ì´í„°
    """
    if not parsed_data:
        return []

    merged_data = []
    current_item = parsed_data[0].copy()

    for next_item in parsed_data[1:]:
        if current_item['speaker'] == next_item['speaker']:
            # ë™ì¼ í™”ìë©´ transcriptë¥¼ í•©ì¹¨ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)
            current_item['transcript'] += ' ' + next_item['transcript']
        else:
            # í™”ìê°€ ë‹¤ë¥´ë©´ í˜„ì¬ í•­ëª©ì„ ê²°ê³¼ì— ì¶”ê°€í•˜ê³  ë‹¤ìŒ í•­ëª©ìœ¼ë¡œ ì´ë™
            merged_data.append(current_item)
            current_item = next_item.copy()

    # ë§ˆì§€ë§‰ í•­ëª© ì¶”ê°€
    merged_data.append(current_item)

    print(f"âœ… ì—°ì† í™”ì ë³‘í•©: {len(parsed_data)}ê°œ â†’ {len(merged_data)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
    return merged_data


def save_to_csv(parsed_data: List[Dict[str, str]], filename: str) -> pd.DataFrame:
    """
    íŒŒì‹±ëœ íšŒì˜ë¡ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        parsed_data: íŒŒì‹±ëœ íšŒì˜ë¡ ë°ì´í„°
        filename: ì €ì¥í•  CSV íŒŒì¼ëª…

    Returns:
        ì €ì¥ëœ DataFrame
    """
    df = pd.DataFrame(parsed_data)
    df.to_csv(filename, index=False, encoding="utf-8-sig")
    print(f"âœ… CSV íŒŒì¼ ìƒì„±: {filename}")
    return df


def generate_audio_from_text(conversation: str, output_file: str) -> bool:
    """
    OpenAI TTS APIë¥¼ ì‚¬ìš©í•˜ì—¬ íšŒì˜ë¡ í…ìŠ¤íŠ¸ë¥¼ ì˜¤ë””ì˜¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        conversation: íšŒì˜ë¡ í…ìŠ¤íŠ¸
        output_file: ì¶œë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ëª…

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

    # ëŒ€í™” ë‚´ìš© íŒŒì‹±
    dialogue_blocks = conversation.strip().split("\n")
    dialogue_parts = []

    for block in dialogue_blocks:
        if block.strip():
            match = re.match(r"(Speaker \d+): (.*)", block, re.DOTALL)
            if match:
                speaker = match.group(1)
                dialogue = match.group(2)
                dialogue_parts.append({"speaker": speaker, "dialogue": dialogue})

    # ê° íŒŒíŠ¸ë³„ ì˜¤ë””ì˜¤ ìƒì„±
    all_audio_segments = []

    for part in dialogue_parts:
        speaker = part["speaker"]
        dialogue = part["dialogue"]
        voice_name = SPEAKER_VOICES.get(speaker, "alloy")

        print(f"ğŸ¤ {speaker} ìŒì„± ìƒì„± ì¤‘ (voice: {voice_name})...")

        try:
            response = client.audio.speech.create(
                model=TTS_MODEL,
                voice=voice_name,
                input=dialogue,
                response_format="wav",
            )

            audio_data = response.content
            audio_segment = AudioSegment.from_file(io.BytesIO(audio_data), format="wav")
            all_audio_segments.append(audio_segment)
            print("  âœ… ì„±ê³µ")

        except Exception as e:
            print(f"  âŒ ì‹¤íŒ¨: {e}")
            return False

    # ëª¨ë“  ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ê²°í•©
    if all_audio_segments:
        print("\nğŸ”— ì˜¤ë””ì˜¤ ê²°í•© ì¤‘...")
        combined_audio = all_audio_segments[0]
        for segment in all_audio_segments[1:]:
            combined_audio += segment

        combined_audio.export(output_file, format="wav")
        print(f"âœ… ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„±: {output_file}")
        return True
    else:
        print("âŒ ì˜¤ë””ì˜¤ ìƒì„± ì‹¤íŒ¨")
        return False


def recognize_audio_with_clova(audio_file: str) -> Optional[Dict[str, Any]]:
    """
    Clova Speech APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        audio_file: ì¸ì‹í•  ì˜¤ë””ì˜¤ íŒŒì¼ëª…

    Returns:
        ì¸ì‹ ê²°ê³¼ JSON ë˜ëŠ” None
    """
    print(f"\nğŸ§ Clova Speech APIë¡œ ìŒì„± ì¸ì‹ ì¤‘: {audio_file}")

    try:
        res = ClovaSpeechClient().req_upload(
            file=audio_file,
            completion="sync",
            diarization={"enable": True}  # í™”ì ë¶„ë¦¬ í™œì„±í™”
        )

        if res.status_code == 200:
            result = res.json()
            print("âœ… ìŒì„± ì¸ì‹ ì™„ë£Œ")
            return result
        else:
            print(f"âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {res.status_code}")
            return None

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def merge_consecutive_speaker_segments(segments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ì—°ì†ì ìœ¼ë¡œ ë™ì¼í•œ í™”ìì˜ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹©ë‹ˆë‹¤.

    Args:
        segments: ì¸ì‹ëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸

    Returns:
        ë³‘í•©ëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    if not segments:
        return []

    merged_segments = []
    current_segment = segments[0].copy()

    for next_segment in segments[1:]:
        if current_segment['speaker'] == next_segment['speaker']:
            current_segment['text'] += ' ' + next_segment['text']
        else:
            merged_segments.append(current_segment)
            current_segment = next_segment.copy()

    merged_segments.append(current_segment)
    return merged_segments


def extract_recognized_segments(result: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Clova Speech API ê²°ê³¼ì—ì„œ í™”ìë³„ ì¸ì‹ ê²°ê³¼ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        result: Clova Speech API ì‘ë‹µ JSON

    Returns:
        í™”ìë³„ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    segments = result.get("segments", [])
    speaker_segments = []

    for segment in segments:
        speaker_label = segment["speaker"]["label"]
        text = segment["text"]
        confidence = segment.get("confidence", 0)
        start_time_ms = segment.get("start", 0)

        # ClovaëŠ” ë°€ë¦¬ì´ˆ(ms) ë‹¨ìœ„ì´ë¯€ë¡œ ì´ˆ(s)ë¡œ ë³€í™˜
        start_time = start_time_ms / 1000.0

        speaker_segments.append({
            "start_time": start_time,
            "confidence": confidence,
            "speaker": speaker_label,
            "text": text
        })

    # ì—°ì†ëœ ë™ì¼ í™”ì ì„¸ê·¸ë¨¼íŠ¸ ë³‘í•©
    merged_segments = merge_consecutive_speaker_segments(speaker_segments)

    return merged_segments


def calculate_accuracy(original_df: pd.DataFrame, recognized_segments: List[Dict[str, Any]]) -> Dict[str, float]:
    """
    ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ì™€ ì¸ì‹ëœ ìŠ¤í¬ë¦½íŠ¸ì˜ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        original_df: ì›ë³¸ íšŒì˜ë¡ DataFrame
        recognized_segments: ì¸ì‹ëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸

    Returns:
        ì •í™•ë„ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
    """
    # ì¸ì‹ëœ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    recognized_df = pd.DataFrame(recognized_segments)

    # í™”ì ë²ˆí˜¸ ì •ê·œí™” (Speaker ì œê±°)
    original_speakers = original_df['speaker'].str.replace("Speaker ", "", regex=False).astype(int)
    recognized_speakers = recognized_df['speaker'].astype(int)

    # í™”ì ì¸ì‹ ì •í™•ë„ (ê¸¸ì´ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìµœì†Œ ê¸¸ì´ë§Œí¼ë§Œ ë¹„êµ)
    min_len = min(len(original_speakers), len(recognized_speakers))
    speaker_accuracy = (original_speakers[:min_len] == recognized_speakers[:min_len]).sum() / min_len * 100

    # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
    text_similarities = []
    for i in range(min_len):
        original_text = original_df.iloc[i]['transcript']
        recognized_text = recognized_df.iloc[i]['text']

        # SequenceMatcherë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = SequenceMatcher(None, original_text, recognized_text).ratio() * 100
        text_similarities.append(similarity)

    avg_text_similarity = sum(text_similarities) / len(text_similarities) if text_similarities else 0

    # ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ ì¼ì¹˜ìœ¨
    segment_match_rate = min_len / max(len(original_df), len(recognized_df)) * 100

    # ì „ì²´ í…ìŠ¤íŠ¸ ë¹„êµ
    original_full_text = " ".join(original_df['transcript'].tolist())
    recognized_full_text = " ".join(recognized_df['text'].tolist())
    overall_similarity = SequenceMatcher(None, original_full_text, recognized_full_text).ratio() * 100

    # í‰ê·  ì‹ ë¢°ë„
    avg_confidence = recognized_df['confidence'].mean() if 'confidence' in recognized_df.columns else 0

    return {
        "í™”ì_ì¸ì‹_ì •í™•ë„": round(speaker_accuracy, 2),
        "í‰ê· _í…ìŠ¤íŠ¸_ìœ ì‚¬ë„": round(avg_text_similarity, 2),
        "ì „ì²´_í…ìŠ¤íŠ¸_ìœ ì‚¬ë„": round(overall_similarity, 2),
        "ì„¸ê·¸ë¨¼íŠ¸_ì¼ì¹˜ìœ¨": round(segment_match_rate, 2),
        "í‰ê· _ì‹ ë¢°ë„": round(avg_confidence, 2),
        "ì›ë³¸_ì„¸ê·¸ë¨¼íŠ¸_ìˆ˜": len(original_df),
        "ì¸ì‹_ì„¸ê·¸ë¨¼íŠ¸_ìˆ˜": len(recognized_df)
    }


def save_comparison_csv(original_df: pd.DataFrame, recognized_segments: List[Dict[str, Any]], output_file: str):
    """
    ì›ë³¸ê³¼ ì¸ì‹ ê²°ê³¼ë¥¼ ë¹„êµí•œ CSV íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        original_df: ì›ë³¸ íšŒì˜ë¡ DataFrame
        recognized_segments: ì¸ì‹ëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
        output_file: ì¶œë ¥ CSV íŒŒì¼ëª…
    """
    # ì¸ì‹ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    recognized_df = pd.DataFrame(recognized_segments)

    # ì»¬ëŸ¼ëª… ë³€ê²½
    recognized_df = recognized_df.rename(columns={
        'speaker': 'recognized_speaker',
        'text': 'recognized_text',
        'confidence': 'confidence',
        'start_time': 'start_time'
    })

    # ì›ë³¸ê³¼ ì¸ì‹ ê²°ê³¼ ë³‘í•©
    comparison_df = pd.concat([
        original_df.reset_index(drop=True),
        recognized_df.reset_index(drop=True)
    ], axis=1)

    # í™”ì ë²ˆí˜¸ ì •ê·œí™”
    comparison_df['speaker_num'] = comparison_df['speaker'].str.replace("Speaker ", "", regex=False).astype(int)

    # ì €ì¥
    comparison_df.to_csv(output_file, index=False, encoding="utf-8-sig")
    print(f"âœ… ë¹„êµ ê²°ê³¼ ì €ì¥: {output_file}")


def print_accuracy_report(accuracy_metrics: Dict[str, float]):
    """
    ì •í™•ë„ ë³´ê³ ì„œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

    Args:
        accuracy_metrics: ì •í™•ë„ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
    """
    print("\n" + "="*60)
    print("ğŸ“Š ì •í™•ë„ ë¶„ì„ ê²°ê³¼")
    print("="*60)
    print(f"í™”ì ì¸ì‹ ì •í™•ë„:      {accuracy_metrics['í™”ì_ì¸ì‹_ì •í™•ë„']:.2f}%")
    print(f"í‰ê·  í…ìŠ¤íŠ¸ ìœ ì‚¬ë„:    {accuracy_metrics['í‰ê· _í…ìŠ¤íŠ¸_ìœ ì‚¬ë„']:.2f}%")
    print(f"ì „ì²´ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„:    {accuracy_metrics['ì „ì²´_í…ìŠ¤íŠ¸_ìœ ì‚¬ë„']:.2f}%")
    print(f"ì„¸ê·¸ë¨¼íŠ¸ ì¼ì¹˜ìœ¨:       {accuracy_metrics['ì„¸ê·¸ë¨¼íŠ¸_ì¼ì¹˜ìœ¨']:.2f}%")
    print(f"í‰ê·  ì‹ ë¢°ë„:           {accuracy_metrics['í‰ê· _ì‹ ë¢°ë„']:.2f}")
    print(f"ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜:      {accuracy_metrics['ì›ë³¸_ì„¸ê·¸ë¨¼íŠ¸_ìˆ˜']}")
    print(f"ì¸ì‹ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜:      {accuracy_metrics['ì¸ì‹_ì„¸ê·¸ë¨¼íŠ¸_ìˆ˜']}")
    print("="*60)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="íšŒì˜ë¡ TTS-STT ì •í™•ë„ í…ŒìŠ¤íŠ¸")
    parser.add_argument("input_txt", help="ì…ë ¥ íšŒì˜ë¡ txt íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output-prefix", default="meeting", help="ì¶œë ¥ íŒŒì¼ëª… ì ‘ë‘ì‚¬ (ê¸°ë³¸ê°’: meeting)")

    args = parser.parse_args()

    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    input_txt = args.input_txt
    output_prefix = args.output_prefix
    csv_file = f"{output_prefix}.csv"
    audio_file = f"{output_prefix}.wav"
    comparison_file = f"{output_prefix}_comparison.csv"

    print("="*60)
    print("ğŸ¯ íšŒì˜ë¡ TTS-STT ì •í™•ë„ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*60)
    print(f"ì…ë ¥ íŒŒì¼: {input_txt}")
    print(f"ì¶œë ¥ CSV: {csv_file}")
    print(f"ì¶œë ¥ ì˜¤ë””ì˜¤: {audio_file}")
    print(f"ë¹„êµ ê²°ê³¼: {comparison_file}")
    print("="*60)

    # 1. txt íŒŒì¼ ì½ê¸°
    print("\nğŸ“– Step 1: íšŒì˜ë¡ í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°")
    if not os.path.exists(input_txt):
        print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {input_txt}")
        return

    with open(input_txt, "r", encoding="utf-8") as f:
        meeting_text = f.read()

    print(f"âœ… í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì™„ë£Œ: {len(meeting_text)} ë¬¸ì")

    # 2. í…ìŠ¤íŠ¸ íŒŒì‹± ë° CSV ì €ì¥
    print("\nğŸ“ Step 2: í…ìŠ¤íŠ¸ íŒŒì‹± ë° ì—°ì† í™”ì ë³‘í•©")
    parsed_data = parse_meeting_text(meeting_text)
    print(f"âœ… íŒŒì‹± ì™„ë£Œ: {len(parsed_data)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")

    # ì—°ì†ëœ ë™ì¼ í™”ì ë³‘í•©
    merged_data = merge_consecutive_speakers(parsed_data)

    # CSV ì €ì¥
    original_df = save_to_csv(merged_data, csv_file)

    # 3. OpenAI TTSë¡œ ì˜¤ë””ì˜¤ ìƒì„±
    print("\nğŸµ Step 3: OpenAI TTSë¡œ ì˜¤ë””ì˜¤ ìƒì„±")
    success = generate_audio_from_text(meeting_text, audio_file)
    if not success:
        print("âŒ ì˜¤ë””ì˜¤ ìƒì„± ì‹¤íŒ¨")
        return

    # 4. Clova Speech APIë¡œ ìŒì„± ì¸ì‹
    print("\nğŸ§ Step 4: Clova Speech APIë¡œ ìŒì„± ì¸ì‹")
    result = recognize_audio_with_clova(audio_file)
    if not result:
        print("âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨")
        return

    # 5. ì¸ì‹ ê²°ê³¼ ì¶”ì¶œ
    print("\nğŸ“Š Step 5: ì¸ì‹ ê²°ê³¼ ì¶”ì¶œ ë° ë³‘í•©")
    recognized_segments = extract_recognized_segments(result)
    print(f"âœ… ì¸ì‹ ì™„ë£Œ: {len(recognized_segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")

    # 6. ì •í™•ë„ ê³„ì‚°
    print("\nğŸ” Step 6: ì •í™•ë„ ê³„ì‚°")
    accuracy_metrics = calculate_accuracy(original_df, recognized_segments)

    # 7. ë¹„êµ ê²°ê³¼ ì €ì¥
    print("\nğŸ’¾ Step 7: ë¹„êµ ê²°ê³¼ ì €ì¥")
    save_comparison_csv(original_df, recognized_segments, comparison_file)

    # 8. ê²°ê³¼ ì¶œë ¥
    print_accuracy_report(accuracy_metrics)

    print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")


if __name__ == "__main__":
    main()
