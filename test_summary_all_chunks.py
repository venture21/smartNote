"""
VectorStoreì—ì„œ ìš”ì•½ì˜ ëª¨ë“  ì†Œì£¼ì œ chunk ë¡œë“œ í…ŒìŠ¤íŠ¸

ì´ì „: ì²« ë²ˆì§¸ ì†Œì£¼ì œë§Œ ë¡œë“œë¨
ìˆ˜ì • í›„: ëª¨ë“  ì†Œì£¼ì œë¥¼ ìˆœì„œëŒ€ë¡œ í•©ì³ì„œ ë¡œë“œ
"""

import logging
import sys
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# ChromaDB ì´ˆê¸°í™”
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from config import CHROMA_DB_FOLDER

# OpenAI Embeddings ì´ˆê¸°í™”
openai_api_key = os.environ.get("OPENAI_API_KEY")
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key=openai_api_key
)

# Summary VectorStore ì´ˆê¸°í™”
summary_vectorstore = Chroma(
    collection_name="summaries",
    embedding_function=embeddings,
    persist_directory=CHROMA_DB_FOLDER
)


def test_get_all_summaries():
    """ëª¨ë“  ìš”ì•½ ë°ì´í„° í™•ì¸"""
    print("=" * 60)
    print("ğŸ“Š Summary VectorStore ì „ì²´ í™•ì¸")
    print("=" * 60)

    # ì „ì²´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    all_results = summary_vectorstore.get()

    if not all_results or not all_results["documents"]:
        print("\nâš ï¸ Summary VectorStoreì— ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    total_count = len(all_results["documents"])
    print(f"\nì „ì²´ ìš”ì•½ ë¬¸ì„œ: {total_count}ê°œ")

    # source_idë³„ë¡œ ê·¸ë£¹í™”
    source_groups = {}
    for doc, meta in zip(all_results["documents"], all_results["metadatas"]):
        source_id = meta.get("source_id", "unknown")
        source_type = meta.get("source_type", "unknown")
        subtopic = meta.get("subtopic", "ì•Œ ìˆ˜ ì—†ìŒ")
        subtopic_index = meta.get("subtopic_index", 0)

        if source_id not in source_groups:
            source_groups[source_id] = {
                "type": source_type,
                "chunks": []
            }

        source_groups[source_id]["chunks"].append({
            "index": subtopic_index,
            "subtopic": subtopic,
            "content_preview": doc[:100] + "..." if len(doc) > 100 else doc
        })

    # ì¶œë ¥
    for source_id, data in source_groups.items():
        source_type = data["type"]
        chunks = sorted(data["chunks"], key=lambda x: x["index"])

        print(f"\n{'='*60}")
        print(f"ğŸ“ Source: {source_id[:16]}... ({source_type})")
        print(f"   ì†Œì£¼ì œ ìˆ˜: {len(chunks)}ê°œ")

        for chunk in chunks:
            print(f"\n   [{chunk['index']}] {chunk['subtopic']}")
            print(f"       {chunk['content_preview']}")

    print("\n" + "=" * 60)


def test_get_summary_function(source_id=None):
    """ìˆ˜ì •ëœ get_summary_from_vectordb í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ğŸ§ª get_summary_from_vectordb í•¨ìˆ˜ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    if not source_id:
        # ì²« ë²ˆì§¸ source_id ì°¾ê¸°
        all_results = summary_vectorstore.get()
        if all_results and all_results["metadatas"]:
            source_id = all_results["metadatas"][0].get("source_id")
        else:
            print("\nâš ï¸ í…ŒìŠ¤íŠ¸í•  source_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

    print(f"\ní…ŒìŠ¤íŠ¸ ëŒ€ìƒ source_id: {source_id[:16]}...")

    # ìˆ˜ì • ì „: ì²« ë²ˆì§¸ chunkë§Œ ê°€ì ¸ì˜¤ê¸°
    print("\nğŸ“Œ ìˆ˜ì • ì „ ë°©ì‹ (ì²« ë²ˆì§¸ chunkë§Œ):")
    results = summary_vectorstore.get(where={"source_id": source_id})
    if results and results["documents"]:
        first_only = results["documents"][0]
        print(f"   ë°˜í™˜ ê¸¸ì´: {len(first_only)}ì")
        print(f"   ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {first_only[:200]}...")

    # ìˆ˜ì • í›„: ëª¨ë“  chunk í•©ì¹˜ê¸°
    print("\nâœ… ìˆ˜ì • í›„ ë°©ì‹ (ëª¨ë“  chunk í•©ì¹¨):")
    if results and results["documents"]:
        documents = results["documents"]
        metadatas = results["metadatas"]

        # subtopic_indexë¡œ ì •ë ¬
        sorted_chunks = []
        for doc, meta in zip(documents, metadatas):
            subtopic_index = meta.get("subtopic_index", 0)
            subtopic = meta.get("subtopic", "")
            sorted_chunks.append((subtopic_index, subtopic, doc))

        sorted_chunks.sort(key=lambda x: x[0])

        # ëª¨ë“  ì†Œì£¼ì œ í‘œì‹œ
        print(f"   ì´ ì†Œì£¼ì œ ìˆ˜: {len(sorted_chunks)}ê°œ")
        for idx, subtopic, doc in sorted_chunks:
            print(f"   [{idx}] {subtopic} ({len(doc)}ì)")

        # í•©ì¹œ ê²°ê³¼
        combined = "\n\n".join([doc for _, _, doc in sorted_chunks])
        print(f"\n   âœ… í•©ì¹œ í›„ ì´ ê¸¸ì´: {len(combined)}ì")
        print(f"   ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:\n{combined[:500]}...")

    print("\n" + "=" * 60)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\nğŸš€ ìš”ì•½ ì†Œì£¼ì œ ì „ì²´ ë¡œë“œ í…ŒìŠ¤íŠ¸\n")

    try:
        # 1. ì „ì²´ ìš”ì•½ í™•ì¸
        test_get_all_summaries()

        # 2. í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
        test_get_summary_function()

        print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("\nğŸ’¡ ì‹¤ì œ ì‚¬ìš©:")
        print("   1. ì›¹ UIì—ì„œ ì¤‘ë³µ ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ")
        print("   2. 'ìš”ì•½' ì„¹ì…˜ì—ì„œ ëª¨ë“  ì†Œì£¼ì œê°€ í‘œì‹œë˜ëŠ”ì§€ í™•ì¸")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
