"""
ì˜ìƒ/ì˜¤ë””ì˜¤ ê²€ìƒ‰ ì—”ì§„ v0.3 (LangChain ë²„ì „)

ë‘ ê°€ì§€ ëª¨ë“œ ì œê³µ:
1. ì˜ìƒ ê²€ìƒ‰ ì—”ì§„: YouTube ë§í¬ ì…ë ¥ â†’ ë‹¤ìš´ë¡œë“œ â†’ STT â†’ íšŒì˜ë¡
2. ì˜¤ë””ì˜¤ ê²€ìƒ‰ ì—”ì§„: ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ â†’ STT â†’ íšŒì˜ë¡

ê¸°ëŠ¥:
- YouTube ì˜ìƒ ë‹¤ìš´ë¡œë“œ ë° MP3 ë³€í™˜
- ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ ì§€ì› (mp3, wav, m4a, flac, ogg)
- STT (Gemini / Clova)
- VectorStore ê¸°ë°˜ íšŒì˜ë¡ ì €ì¥ ë° ê²€ìƒ‰ (LangChain + ChromaDB + Gemini Embedding)
- íšŒì˜ë¡ ìš”ì•½ ë° AI ì±„íŒ… (RAG ê¸°ë°˜)
- CSVë¡œ ì‘ì—… ì´ë ¥ ê´€ë¦¬ (ìºì‹±)
"""

from flask import Flask, render_template, request, jsonify, send_from_directory
import os
import pandas as pd
import json
import subprocess
from werkzeug.utils import secure_filename
from dotenv import load_dotenv
from myClovaSpeech import ClovaSpeechClient
from google import genai
from google.genai import types
import secrets
import yt_dlp
import logging
from datetime import datetime
import threading
import time
import hashlib
from mutagen import File as MutagenFile

# LangChain imports
from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = Flask(__name__)
app.secret_key = secrets.token_hex(16)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)

# ì„¤ì •
MP4_FOLDER = "mp4"
MP3_FOLDER = "mp3"
CSV_FOLDER = "csv"
UPLOADS_FOLDER = "uploads"
CHROMA_DB_FOLDER = "chroma_db_langchain_v0.3"
YOUTUBE_HISTORY_CSV = os.path.join(CSV_FOLDER, "youtube_history_v0.3.csv")
AUDIO_HISTORY_CSV = os.path.join(CSV_FOLDER, "audio_history_v0.3.csv")

app.config["MAX_CONTENT_LENGTH"] = 500 * 1024 * 1024  # 500MB max

# í—ˆìš©ëœ ì˜¤ë””ì˜¤ íŒŒì¼ í™•ì¥ì
ALLOWED_AUDIO_EXTENSIONS = {
    "mp3",
    "wav",
    "m4a",
    "flac",
    "ogg",
    "mp4",
    "avi",
    "mov",
    "mkv",
}

# í´ë” ìƒì„±
for folder in [MP4_FOLDER, MP3_FOLDER, CSV_FOLDER, UPLOADS_FOLDER, CHROMA_DB_FOLDER]:
    os.makedirs(folder, exist_ok=True)

# ì„¸ì…˜ë³„ ë°ì´í„° ì €ì¥
session_data = {}

# ì§„í–‰ ìƒí™© ì €ì¥
progress_data = {}

# LangChain Embeddings ì´ˆê¸°í™”
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/text-embedding-004", google_api_key=os.environ.get("GOOGLE_API_KEY")
)

# LangChain VectorStore (YouTubeì™€ Audio ë¶„ë¦¬)
youtube_vectorstore = None
audio_vectorstore = None


def allowed_file(filename):
    """í—ˆìš©ëœ íŒŒì¼ í™•ì¥ìì¸ì§€ í™•ì¸"""
    return (
        "." in filename
        and filename.rsplit(".", 1)[1].lower() in ALLOWED_AUDIO_EXTENSIONS
    )


def calculate_file_hash(file_path):
    """íŒŒì¼ì˜ MD5 í•´ì‹œë¥¼ ê³„ì‚°í•˜ì—¬ ê³ ìœ  ID ìƒì„±"""
    hasher = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hasher.update(chunk)
    return hasher.hexdigest()


def get_audio_duration(file_path):
    """ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ê¸¸ì´ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë°˜í™˜"""
    try:
        audio = MutagenFile(file_path)
        if audio is not None and hasattr(audio.info, "length"):
            duration = audio.info.length
            logging.info(f"ğŸµ ì˜¤ë””ì˜¤ íŒŒì¼ ê¸¸ì´: {duration:.2f}ì´ˆ ({duration/60:.2f}ë¶„)")
            return duration
        else:
            logging.warning(f"âš ï¸ ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return 0.0
    except Exception as e:
        logging.error(f"âŒ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return 0.0


def update_progress(task_id, step, progress, message):
    """ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸"""
    if task_id not in progress_data:
        progress_data[task_id] = {}

    progress_data[task_id][step] = {
        "progress": progress,
        "message": message,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    }
    logging.info(f"[{task_id}] {step}: {progress}% - {message}")


# YouTube ì´ë ¥ ë¡œë“œ
def load_youtube_history():
    """CSV íŒŒì¼ì—ì„œ YouTube ë‹¤ìš´ë¡œë“œ ì´ë ¥ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if os.path.exists(YOUTUBE_HISTORY_CSV):
        try:
            df = pd.read_csv(YOUTUBE_HISTORY_CSV, encoding="utf-8-sig")
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ ë° ì¶”ê°€
            if "summary" not in df.columns:
                df["summary"] = ""
            if "stt_processing_time" not in df.columns:
                df["stt_processing_time"] = 0.0

            # NaN ê°’ ì²˜ë¦¬
            df["summary"] = df["summary"].fillna("")
            df["stt_processing_time"] = df["stt_processing_time"].fillna(0.0)
            df["view_count"] = df["view_count"].fillna(0)
            df["channel"] = df["channel"].fillna("Unknown")
            df["upload_date"] = df["upload_date"].fillna("")

            logging.info(f"ğŸ“‹ YouTube ì´ë ¥ ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í•­ëª©")
            return df
        except Exception as e:
            logging.error(f"YouTube ì´ë ¥ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return pd.DataFrame(
                columns=[
                    "youtube_url",
                    "video_id",
                    "title",
                    "channel",
                    "view_count",
                    "upload_date",
                    "mp3_path",
                    "segments_json",
                    "stt_service",
                    "stt_processing_time",
                    "created_at",
                    "summary",
                ]
            )
    else:
        return pd.DataFrame(
            columns=[
                "youtube_url",
                "video_id",
                "title",
                "channel",
                "view_count",
                "upload_date",
                "mp3_path",
                "segments_json",
                "stt_service",
                "stt_processing_time",
                "created_at",
                "summary",
            ]
        )


def save_youtube_history(df):
    """YouTube ì´ë ¥ì„ CSV íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        df.to_csv(YOUTUBE_HISTORY_CSV, index=False, encoding="utf-8-sig")
        logging.info(f"ğŸ’¾ YouTube ì´ë ¥ ì €ì¥ ì™„ë£Œ: {len(df)}ê°œ í•­ëª©")
    except Exception as e:
        logging.error(f"YouTube ì´ë ¥ ì €ì¥ ì˜¤ë¥˜: {e}")


# ì˜¤ë””ì˜¤ ì´ë ¥ ë¡œë“œ
def load_audio_history():
    """CSV íŒŒì¼ì—ì„œ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì´ë ¥ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if os.path.exists(AUDIO_HISTORY_CSV):
        try:
            df = pd.read_csv(AUDIO_HISTORY_CSV, encoding="utf-8-sig")
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ ë° ì¶”ê°€
            if "summary" not in df.columns:
                df["summary"] = ""
            if "stt_processing_time" not in df.columns:
                df["stt_processing_time"] = 0.0
            if "audio_duration" not in df.columns:
                df["audio_duration"] = 0.0

            # NaN ê°’ ì²˜ë¦¬
            df["summary"] = df["summary"].fillna("")
            df["stt_processing_time"] = df["stt_processing_time"].fillna(0.0)
            df["audio_duration"] = df["audio_duration"].fillna(0.0)

            logging.info(f"ğŸ“‹ ì˜¤ë””ì˜¤ ì´ë ¥ ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í•­ëª©")
            return df
        except Exception as e:
            logging.error(f"ì˜¤ë””ì˜¤ ì´ë ¥ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return pd.DataFrame(
                columns=[
                    "file_hash",
                    "filename",
                    "file_path",
                    "file_size",
                    "audio_duration",
                    "segments_json",
                    "stt_service",
                    "stt_processing_time",
                    "created_at",
                    "summary",
                ]
            )
    else:
        return pd.DataFrame(
            columns=[
                "file_hash",
                "filename",
                "file_path",
                "file_size",
                "audio_duration",
                "segments_json",
                "stt_service",
                "stt_processing_time",
                "created_at",
                "summary",
            ]
        )


def save_audio_history(df):
    """ì˜¤ë””ì˜¤ ì´ë ¥ì„ CSV íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        df.to_csv(AUDIO_HISTORY_CSV, index=False, encoding="utf-8-sig")
        logging.info(f"ğŸ’¾ ì˜¤ë””ì˜¤ ì´ë ¥ ì €ì¥ ì™„ë£Œ: {len(df)}ê°œ í•­ëª©")
    except Exception as e:
        logging.error(f"ì˜¤ë””ì˜¤ ì´ë ¥ ì €ì¥ ì˜¤ë¥˜: {e}")


def get_gemini_client():
    """Gemini í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    api_key = os.environ.get("GOOGLE_API_KEY")
    if api_key:
        return genai.Client(api_key=api_key)
    else:
        return genai.Client()


def initialize_vectorstores():
    """LangChain VectorStore ì´ˆê¸°í™”"""
    global youtube_vectorstore, audio_vectorstore

    try:
        # YouTube VectorStore
        youtube_vectorstore = Chroma(
            collection_name="youtube_transcripts_langchain_v0.3",
            embedding_function=embeddings,
            persist_directory=os.path.join(CHROMA_DB_FOLDER, "youtube"),
        )

        # Audio VectorStore
        audio_vectorstore = Chroma(
            collection_name="audio_transcripts_langchain_v0.3",
            embedding_function=embeddings,
            persist_directory=os.path.join(CHROMA_DB_FOLDER, "audio"),
        )

        logging.info(f"âœ… LangChain VectorStore ì´ˆê¸°í™” ì™„ë£Œ")

        # ë¬¸ì„œ ê°œìˆ˜ í™•ì¸
        try:
            youtube_count = len(youtube_vectorstore.get()["ids"])
            audio_count = len(audio_vectorstore.get()["ids"])
            logging.info(f"   - YouTube VectorStore: {youtube_count} documents")
            logging.info(f"   - Audio VectorStore: {audio_count} documents")
        except:
            logging.info(f"   - VectorStore ë¬¸ì„œ ê°œìˆ˜ í™•ì¸ ë¶ˆê°€ (ë¹ˆ ì»¬ë ‰ì…˜ì¼ ìˆ˜ ìˆìŒ)")

    except Exception as e:
        logging.error(f"âŒ LangChain VectorStore ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")


def store_segments_in_vectordb(
    segments, source_id, source_type="youtube", filename=None
):
    """
    ì„¸ê·¸ë¨¼íŠ¸ë¥¼ LangChain VectorDBì— ì €ì¥

    Args:
        segments: STTë¡œ ì¶”ì¶œëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
        source_id: YouTube video_id ë˜ëŠ” audio file_hash
        source_type: "youtube" ë˜ëŠ” "audio"
        filename: ì˜¤ë””ì˜¤ íŒŒì¼ ì´ë¦„ (ì˜¤ë””ì˜¤ íƒ€ì…ì¸ ê²½ìš°)
    """
    try:
        if not segments:
            logging.warning("âš ï¸ ì €ì¥í•  ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        # VectorStore ì„ íƒ
        vectorstore = (
            youtube_vectorstore if source_type == "youtube" else audio_vectorstore
        )

        # LangChain Document ê°ì²´ ìƒì„±
        documents = []
        for idx, segment in enumerate(segments):
            text = segment.get("text", "").strip()
            if not text:
                continue

            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                "source_id": source_id,
                "source_type": source_type,
                "segment_index": idx,
                "speaker": segment.get("speaker", "Unknown"),
                "start_time": segment.get("start", 0.0),
                "end_time": segment.get("end", 0.0),
            }

            if filename:
                metadata["filename"] = filename

            # Document ìƒì„±
            doc = Document(page_content=text, metadata=metadata)
            documents.append(doc)

        if not documents:
            logging.warning("âš ï¸ ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        # VectorStoreì— ì¶”ê°€
        logging.info(f"ğŸ“¥ LangChain VectorStoreì— {len(documents)}ê°œ ë¬¸ì„œ ì €ì¥ ì¤‘...")
        vectorstore.add_documents(documents)

        logging.info(f"âœ… VectorStore ì €ì¥ ì™„ë£Œ ({source_type}: {source_id})")
        return True

    except Exception as e:
        logging.error(f"âŒ VectorStore ì €ì¥ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return False


def search_vectordb(query, source_id, source_type="youtube", n_results=5):
    """
    LangChain VectorDBì—ì„œ ê²€ìƒ‰ (Retriever ì‚¬ìš©)

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        source_id: YouTube video_id ë˜ëŠ” audio file_hash
        source_type: "youtube" ë˜ëŠ” "audio"
        n_results: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜

    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    try:
        # VectorStore ì„ íƒ
        vectorstore = (
            youtube_vectorstore if source_type == "youtube" else audio_vectorstore
        )

        # Retriever ìƒì„± (íŠ¹ì • source_idë¡œ í•„í„°ë§)
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": n_results, "filter": {"source_id": source_id}},
        )

        # ê²€ìƒ‰ ì‹¤í–‰
        docs = retriever.get_relevant_documents(query)

        # ê²°ê³¼ í¬ë§·íŒ…
        results = []
        for doc in docs:
            results.append(
                {
                    "document": doc.page_content,
                    "metadata": doc.metadata,
                    "score": None,  # LangChainì˜ ê¸°ë³¸ retrieverëŠ” scoreë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ
                }
            )

        logging.info(f"ğŸ” ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ (query: {query[:50]}...)")
        return results

    except Exception as e:
        logging.error(f"âŒ VectorDB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return []


def search_vectordb_with_score(query, source_id, source_type="youtube", n_results=5):
    """
    LangChain VectorDBì—ì„œ ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        source_id: YouTube video_id ë˜ëŠ” audio file_hash
        source_type: "youtube" ë˜ëŠ” "audio"
        n_results: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜

    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ í¬í•¨)
    """
    try:
        # VectorStore ì„ íƒ
        vectorstore = (
            youtube_vectorstore if source_type == "youtube" else audio_vectorstore
        )

        # ìœ ì‚¬ë„ ê²€ìƒ‰ (ì ìˆ˜ í¬í•¨)
        docs_with_scores = vectorstore.similarity_search_with_score(
            query, k=n_results, filter={"source_id": source_id}
        )

        # ê²°ê³¼ í¬ë§·íŒ…
        results = []
        for doc, score in docs_with_scores:
            results.append(
                {
                    "document": doc.page_content,
                    "metadata": doc.metadata,
                    "score": float(score),  # ê±°ë¦¬ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)
                }
            )

        logging.info(
            f"ğŸ” ê²€ìƒ‰ ì™„ë£Œ (ì ìˆ˜ í¬í•¨): {len(results)}ê°œ ê²°ê³¼ (query: {query[:50]}...)"
        )
        return results

    except Exception as e:
        logging.error(f"âŒ VectorDB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return []


# STT ì²˜ë¦¬ í•¨ìˆ˜ë“¤ì€ ì›ë³¸ íŒŒì¼ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€
def process_stt_gemini(mp3_path):
    """Gemini STT ì²˜ë¦¬"""
    try:
        logging.info("ğŸ¤ Gemini STT ì‹œì‘...")
        start_time = time.time()

        client = get_gemini_client()

        # ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ
        logging.info("ğŸ“¤ ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ ì¤‘...")
        audio_file = client.files.upload(path=mp3_path)
        logging.info(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {audio_file.name}")

        # STT ì‹¤í–‰
        logging.info("ğŸ”„ STT ì²˜ë¦¬ ì¤‘...")
        prompt = """ì´ ì˜¤ë””ì˜¤ë¥¼ í•œêµ­ì–´ë¡œ ì •í™•í•˜ê²Œ ì „ì‚¬í•´ ì£¼ì„¸ìš”. 
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”:
        - í™”ì êµ¬ë¶„ì´ ê°€ëŠ¥í•˜ë©´ "í™”ì1:", "í™”ì2:" ë“±ìœ¼ë¡œ í‘œì‹œ
        - íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨
        - ë¬¸ì¥ ë‹¨ìœ„ë¡œ êµ¬ë¶„
        """

        response = client.models.generate_content(
            model="gemini-2.0-flash-exp", contents=[prompt, audio_file]
        )

        transcript_text = response.text
        elapsed_time = time.time() - start_time

        logging.info(f"âœ… Gemini STT ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")

        # ì„¸ê·¸ë¨¼íŠ¸ íŒŒì‹± (ê°„ë‹¨í•œ ë²„ì „)
        segments = []
        lines = transcript_text.split("\n")
        for i, line in enumerate(lines):
            line = line.strip()
            if line:
                segments.append(
                    {
                        "speaker": "Speaker",
                        "start": i * 5.0,  # ì„ì‹œ íƒ€ì„ìŠ¤íƒ¬í”„
                        "end": (i + 1) * 5.0,
                        "text": line,
                    }
                )

        return segments, elapsed_time

    except Exception as e:
        logging.error(f"âŒ Gemini STT ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return [], 0.0


def process_stt_clova(mp3_path):
    """Clova STT ì²˜ë¦¬"""
    try:
        logging.info("ğŸ¤ Clova STT ì‹œì‘...")
        start_time = time.time()

        invoke_url = os.environ.get("CLOVA_INVOKE_URL")
        secret_key = os.environ.get("CLOVA_SECRET_KEY")

        if not invoke_url or not secret_key:
            raise ValueError("Clova API ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")

        clova_client = ClovaSpeechClient(invoke_url, secret_key)

        # Clova STT ì‹¤í–‰ (diarization í™œì„±í™”)
        res = clova_client.req_upload(
            file=mp3_path, completion="sync", diarization={"enable": True}
        )

        elapsed_time = time.time() - start_time

        if "segments" not in res:
            raise ValueError("Clova STT ì‘ë‹µì— segmentsê°€ ì—†ìŠµë‹ˆë‹¤.")

        segments = res["segments"]
        logging.info(
            f"âœ… Clova STT ì™„ë£Œ: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)"
        )

        return segments, elapsed_time

    except Exception as e:
        logging.error(f"âŒ Clova STT ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return [], 0.0


# YouTube ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
def download_youtube_video(youtube_url, task_id):
    """YouTube ì˜ìƒ ë‹¤ìš´ë¡œë“œ ë° MP3 ë³€í™˜"""
    try:
        update_progress(task_id, "download", 10, "YouTube ì˜ìƒ ì •ë³´ í™•ì¸ ì¤‘...")

        ydl_opts = {
            "format": "bestaudio/best",
            "outtmpl": os.path.join(MP4_FOLDER, "%(id)s.%(ext)s"),
            "quiet": True,
            "no_warnings": True,
        }

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(youtube_url, download=False)
            video_id = info["id"]
            title = info.get("title", "Unknown")
            channel = info.get("uploader", "Unknown")
            view_count = info.get("view_count", 0)
            upload_date = info.get("upload_date", "")

            update_progress(task_id, "download", 30, f"'{title}' ë‹¤ìš´ë¡œë“œ ì¤‘...")

            # ë‹¤ìš´ë¡œë“œ
            ydl.download([youtube_url])

        # MP3 ë³€í™˜
        update_progress(task_id, "download", 70, "MP3 ë³€í™˜ ì¤‘...")

        downloaded_file = None
        for ext in ["webm", "m4a", "mp3", "opus"]:
            potential_file = os.path.join(MP4_FOLDER, f"{video_id}.{ext}")
            if os.path.exists(potential_file):
                downloaded_file = potential_file
                break

        if not downloaded_file:
            raise FileNotFoundError(f"ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_id}")

        mp3_path = os.path.join(MP3_FOLDER, f"{video_id}.mp3")

        subprocess.run(
            [
                "ffmpeg",
                "-y",
                "-i",
                downloaded_file,
                "-vn",
                "-ar",
                "16000",
                "-ac",
                "1",
                "-b:a",
                "128k",
                mp3_path,
            ],
            check=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )

        update_progress(task_id, "download", 100, "ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

        return {
            "video_id": video_id,
            "title": title,
            "channel": channel,
            "view_count": view_count,
            "upload_date": upload_date,
            "mp3_path": mp3_path,
        }

    except Exception as e:
        logging.error(f"YouTube ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        update_progress(task_id, "download", 0, f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise


# Flask ë¼ìš°íŠ¸ë“¤
@app.route("/")
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return render_template("index.html")


@app.route("/api/youtube/process", methods=["POST"])
def process_youtube():
    """YouTube ì˜ìƒ ì²˜ë¦¬ API"""
    try:
        data = request.get_json()
        youtube_url = data.get("youtube_url", "").strip()
        stt_service = data.get("stt_service", "gemini")

        if not youtube_url:
            return (
                jsonify({"success": False, "error": "YouTube URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}),
                400,
            )

        # Task ID ìƒì„±
        task_id = secrets.token_hex(8)
        session_id = secrets.token_hex(16)

        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
        thread = threading.Thread(
            target=process_youtube_background,
            args=(youtube_url, stt_service, task_id, session_id),
        )
        thread.start()

        return jsonify(
            {
                "success": True,
                "task_id": task_id,
                "session_id": session_id,
                "message": "ì²˜ë¦¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            }
        )

    except Exception as e:
        import traceback

        traceback.print_exc()
        return jsonify({"success": False, "error": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500


def process_youtube_background(youtube_url, stt_service, task_id, session_id):
    """YouTube ì˜ìƒ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬"""
    try:
        # 1. ì´ë ¥ í™•ì¸
        history_df = load_youtube_history()

        # video_id ì¶”ì¶œ
        import re

        video_id_match = re.search(r"(?:v=|/)([0-9A-Za-z_-]{11}).*", youtube_url)
        if not video_id_match:
            update_progress(task_id, "error", 0, "ìœ íš¨í•˜ì§€ ì•Šì€ YouTube URLì…ë‹ˆë‹¤.")
            return

        video_id = video_id_match.group(1)

        # ê¸°ì¡´ ë°ì´í„° í™•ì¸
        existing = history_df[history_df["video_id"] == video_id]

        if not existing.empty:
            # ìºì‹œëœ ë°ì´í„° ì‚¬ìš©
            update_progress(task_id, "cache", 100, "ìºì‹œëœ ë°ì´í„° ì‚¬ìš©")

            row = existing.iloc[0]
            segments = json.loads(row["segments_json"])

            session_data[session_id] = {
                "source_type": "youtube",
                "video_id": video_id,
                "title": row["title"],
                "segments": segments,
                "summary": row.get("summary", ""),
                "chat_history": [],
            }

            update_progress(task_id, "complete", 100, "ì²˜ë¦¬ ì™„ë£Œ (ìºì‹œ ì‚¬ìš©)")
            return

        # 2. ë‹¤ìš´ë¡œë“œ
        download_result = download_youtube_video(youtube_url, task_id)

        # 3. STT ì²˜ë¦¬
        update_progress(task_id, "stt", 10, f"{stt_service.upper()} STT ì²˜ë¦¬ ì¤‘...")

        if stt_service == "gemini":
            segments, stt_time = process_stt_gemini(download_result["mp3_path"])
        else:
            segments, stt_time = process_stt_clova(download_result["mp3_path"])

        update_progress(task_id, "stt", 100, "STT ì²˜ë¦¬ ì™„ë£Œ")

        # 4. VectorDB ì €ì¥
        update_progress(task_id, "vectordb", 50, "VectorDB ì €ì¥ ì¤‘...")
        store_segments_in_vectordb(segments, source_id=video_id, source_type="youtube")
        update_progress(task_id, "vectordb", 100, "VectorDB ì €ì¥ ì™„ë£Œ")

        # 5. ì´ë ¥ ì €ì¥
        new_row = {
            "youtube_url": youtube_url,
            "video_id": video_id,
            "title": download_result["title"],
            "channel": download_result["channel"],
            "view_count": download_result["view_count"],
            "upload_date": download_result["upload_date"],
            "mp3_path": download_result["mp3_path"],
            "segments_json": json.dumps(segments, ensure_ascii=False),
            "stt_service": stt_service,
            "stt_processing_time": stt_time,
            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "summary": "",
        }

        history_df = pd.concat([history_df, pd.DataFrame([new_row])], ignore_index=True)
        save_youtube_history(history_df)

        # 6. ì„¸ì…˜ ë°ì´í„° ì €ì¥
        session_data[session_id] = {
            "source_type": "youtube",
            "video_id": video_id,
            "title": download_result["title"],
            "segments": segments,
            "summary": "",
            "chat_history": [],
        }

        update_progress(task_id, "complete", 100, "ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ")

    except Exception as e:
        import traceback

        traceback.print_exc()
        update_progress(task_id, "error", 0, f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@app.route("/api/audio/process", methods=["POST"])
def process_audio():
    """ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ API"""
    try:
        if "audio_file" not in request.files:
            return jsonify({"success": False, "error": "ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}), 400

        audio_file = request.files["audio_file"]
        stt_service = request.form.get("stt_service", "gemini")

        if audio_file.filename == "":
            return (
                jsonify({"success": False, "error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}),
                400,
            )

        if not allowed_file(audio_file.filename):
            return (
                jsonify(
                    {
                        "success": False,
                        "error": f"í—ˆìš©ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. í—ˆìš© í˜•ì‹: {', '.join(ALLOWED_AUDIO_EXTENSIONS)}",
                    }
                ),
                400,
            )

        # íŒŒì¼ ì €ì¥
        filename = secure_filename(audio_file.filename)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_filename = f"{timestamp}_{filename}"
        file_path = os.path.join(UPLOADS_FOLDER, unique_filename)
        audio_file.save(file_path)

        # MP3 ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)
        if not filename.lower().endswith(".mp3"):
            mp3_path = os.path.join(
                UPLOADS_FOLDER, f"{timestamp}_{os.path.splitext(filename)[0]}.mp3"
            )
            subprocess.run(
                [
                    "ffmpeg",
                    "-y",
                    "-i",
                    file_path,
                    "-vn",
                    "-ar",
                    "16000",
                    "-ac",
                    "1",
                    "-b:a",
                    "128k",
                    mp3_path,
                ],
                check=True,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
            )
            file_path = mp3_path

        # Task ID ìƒì„±
        task_id = secrets.token_hex(8)
        session_id = secrets.token_hex(16)

        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
        thread = threading.Thread(
            target=process_audio_background,
            args=(file_path, filename, stt_service, task_id, session_id),
        )
        thread.start()

        return jsonify(
            {
                "success": True,
                "task_id": task_id,
                "session_id": session_id,
                "message": "ì²˜ë¦¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            }
        )

    except Exception as e:
        import traceback

        traceback.print_exc()
        return jsonify({"success": False, "error": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500


def process_audio_background(file_path, filename, stt_service, task_id, session_id):
    """ì˜¤ë””ì˜¤ íŒŒì¼ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬"""
    try:
        # 1. íŒŒì¼ í•´ì‹œ ê³„ì‚°
        update_progress(task_id, "hash", 10, "íŒŒì¼ í•´ì‹œ ê³„ì‚° ì¤‘...")
        file_hash = calculate_file_hash(file_path)

        # 2. ì´ë ¥ í™•ì¸
        history_df = load_audio_history()
        existing = history_df[history_df["file_hash"] == file_hash]

        if not existing.empty:
            # ìºì‹œëœ ë°ì´í„° ì‚¬ìš©
            update_progress(task_id, "cache", 100, "ìºì‹œëœ ë°ì´í„° ì‚¬ìš©")

            row = existing.iloc[0]
            segments = json.loads(row["segments_json"])

            session_data[session_id] = {
                "source_type": "audio",
                "file_hash": file_hash,
                "filename": filename,
                "segments": segments,
                "summary": row.get("summary", ""),
                "chat_history": [],
            }

            update_progress(task_id, "complete", 100, "ì²˜ë¦¬ ì™„ë£Œ (ìºì‹œ ì‚¬ìš©)")
            return

        # 3. ì˜¤ë””ì˜¤ ì •ë³´ ì¶”ì¶œ
        update_progress(task_id, "info", 30, "ì˜¤ë””ì˜¤ ì •ë³´ ì¶”ì¶œ ì¤‘...")
        file_size = os.path.getsize(file_path)
        audio_duration = get_audio_duration(file_path)

        # 4. STT ì²˜ë¦¬
        update_progress(task_id, "stt", 40, f"{stt_service.upper()} STT ì²˜ë¦¬ ì¤‘...")

        if stt_service == "gemini":
            segments, stt_time = process_stt_gemini(file_path)
        else:
            segments, stt_time = process_stt_clova(file_path)

        update_progress(task_id, "stt", 80, "STT ì²˜ë¦¬ ì™„ë£Œ")

        # 5. VectorDB ì €ì¥
        update_progress(task_id, "vectordb", 85, "VectorDB ì €ì¥ ì¤‘...")
        store_segments_in_vectordb(
            segments, source_id=file_hash, source_type="audio", filename=filename
        )
        update_progress(task_id, "vectordb", 95, "VectorDB ì €ì¥ ì™„ë£Œ")

        # 6. ì´ë ¥ ì €ì¥
        new_row = {
            "file_hash": file_hash,
            "filename": filename,
            "file_path": file_path,
            "file_size": file_size,
            "audio_duration": audio_duration,
            "segments_json": json.dumps(segments, ensure_ascii=False),
            "stt_service": stt_service,
            "stt_processing_time": stt_time,
            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "summary": "",
        }

        history_df = pd.concat([history_df, pd.DataFrame([new_row])], ignore_index=True)
        save_audio_history(history_df)

        # 7. ì„¸ì…˜ ë°ì´í„° ì €ì¥
        session_data[session_id] = {
            "source_type": "audio",
            "file_hash": file_hash,
            "filename": filename,
            "segments": segments,
            "summary": "",
            "chat_history": [],
        }

        update_progress(task_id, "complete", 100, "ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ")

    except Exception as e:
        import traceback

        traceback.print_exc()
        update_progress(task_id, "error", 0, f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@app.route("/api/transcript", methods=["POST"])
def get_transcript():
    """íšŒì˜ë¡ ì¡°íšŒ API"""
    try:
        data = request.get_json()
        session_id = data.get("session_id")

        if not session_id or session_id not in session_data:
            return jsonify({"success": False, "error": "ì„¸ì…˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 400

        session_info = session_data[session_id]
        segments = session_info.get("segments", [])

        return jsonify(
            {
                "success": True,
                "segments": segments,
                "title": session_info.get("title") or session_info.get("filename"),
                "summary": session_info.get("summary", ""),
            }
        )

    except Exception as e:
        import traceback

        traceback.print_exc()
        return (
            jsonify({"success": False, "error": f"íšŒì˜ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}),
            500,
        )


@app.route("/api/summarize", methods=["POST"])
def summarize_transcript():
    """íšŒì˜ë¡ ìš”ì•½ API"""
    try:
        data = request.get_json()
        session_id = data.get("session_id")

        if not session_id or session_id not in session_data:
            return jsonify({"success": False, "error": "ì„¸ì…˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 400

        session_info = session_data[session_id]
        segments = session_info.get("segments", [])

        if not segments:
            return jsonify({"success": False, "error": "íšŒì˜ë¡ì´ ì—†ìŠµë‹ˆë‹¤."}), 400

        # ì „ì²´ íšŒì˜ë¡ í…ìŠ¤íŠ¸ ìƒì„±
        transcript_text = "\n\n".join(
            [
                f"í™”ì {seg.get('speaker', 'Unknown')} ({seg.get('start', 0):.1f}ì´ˆ): {seg.get('text', '')}"
                for seg in segments
            ]
        )

        client = get_gemini_client()

        prompt = f"""ë‹¤ìŒ íšŒì˜ë¡ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”. ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•´ ì£¼ì„¸ìš”:

1. ì£¼ìš” ë…¼ì˜ ì£¼ì œ
2. ì£¼ìš” ê²°ì • ì‚¬í•­
3. ì•¡ì…˜ ì•„ì´í…œ (ìˆëŠ” ê²½ìš°)
4. ì¤‘ìš”í•œ ë°œì–¸ì´ë‚˜ ì˜ê²¬

íšŒì˜ë¡:
{transcript_text}

ìš”ì•½ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”."""

        logging.info("ğŸ¤– Geminië¡œ ìš”ì•½ ìƒì„± ì¤‘...")

        response = client.models.generate_content(
            model="gemini-2.5-pro",
            contents=prompt,
        )

        summary = response.text.strip()
        logging.info("âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ")

        # CSVì— ìš”ì•½ ì €ì¥
        if session_id and session_id in session_data:
            source_type = session_data[session_id].get("source_type")

            if source_type == "youtube":
                video_id = session_data[session_id].get("video_id")
                if video_id:
                    try:
                        history_df = load_youtube_history()
                        mask = history_df["video_id"] == video_id
                        if mask.any():
                            history_df.loc[mask, "summary"] = summary
                            save_youtube_history(history_df)
                            logging.info(
                                f"ğŸ’¾ ìš”ì•½ì´ YouTube CSVì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (video_id: {video_id})"
                            )
                    except Exception as e:
                        logging.error(f"ìš”ì•½ ì €ì¥ ì˜¤ë¥˜: {e}")

            elif source_type == "audio":
                file_hash = session_data[session_id].get("file_hash")
                if file_hash:
                    try:
                        history_df = load_audio_history()
                        mask = history_df["file_hash"] == file_hash
                        if mask.any():
                            history_df.loc[mask, "summary"] = summary
                            save_audio_history(history_df)
                            logging.info(
                                f"ğŸ’¾ ìš”ì•½ì´ ì˜¤ë””ì˜¤ CSVì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (file_hash: {file_hash})"
                            )
                    except Exception as e:
                        logging.error(f"ìš”ì•½ ì €ì¥ ì˜¤ë¥˜: {e}")

        return jsonify({"success": True, "summary": summary})

    except Exception as e:
        import traceback

        traceback.print_exc()
        return (
            jsonify({"success": False, "error": f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}),
            500,
        )


@app.route("/api/chat", methods=["POST"])
def chat_with_transcript():
    """íšŒì˜ë¡ ê¸°ë°˜ ì±„íŒ… API (RAG with LangChain Retriever)"""
    try:
        data = request.get_json()
        user_message = data.get("message", "").strip()

        if not user_message:
            return jsonify({"success": False, "error": "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400

        session_id = data.get("session_id")
        chat_history = data.get("chat_history", [])

        if not session_id or session_id not in session_data:
            return jsonify({"success": False, "error": "ì„¸ì…˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 400

        session_info = session_data[session_id]
        source_type = session_info.get("source_type")
        chat_history = session_info.get("chat_history", [])

        # source_id ê°€ì ¸ì˜¤ê¸°
        if source_type == "youtube":
            source_id = session_info.get("video_id")
        elif source_type == "audio":
            source_id = session_info.get("file_hash")
        else:
            return (
                jsonify({"success": False, "error": "ì•Œ ìˆ˜ ì—†ëŠ” ì†ŒìŠ¤ íƒ€ì…ì…ë‹ˆë‹¤."}),
                400,
            )

        # LangChain Retrieverë¡œ VectorDB ê²€ìƒ‰ (RAG)
        logging.info(f"ğŸ” LangChain Retriever ê²€ìƒ‰: {user_message}")
        search_results = search_vectordb_with_score(
            query=user_message,
            source_id=source_id,
            source_type=source_type,
            n_results=5,
        )

        if not search_results:
            return (
                jsonify(
                    {"success": False, "error": "ê´€ë ¨ íšŒì˜ë¡ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
                ),
                400,
            )

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
        context_text = "\n\n".join(
            [
                f"í™”ì {result['metadata']['speaker']} ({result['metadata']['start_time']:.1f}ì´ˆ): {result['document']}"
                for result in search_results
            ]
        )

        logging.info(f"ğŸ“ ê²€ìƒ‰ëœ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(search_results)}")

        # ì´ì „ ëŒ€í™” ë‚´ì—­
        history_text = ""
        if chat_history:
            history_text = "\n\nì´ì „ ëŒ€í™” ë‚´ì—­:\n"
            for hist in chat_history[-5:]:
                history_text += f"ì‚¬ìš©ì: {hist['user']}\n"
                history_text += f"AI: {hist['assistant']}\n\n"

        client = get_gemini_client()

        prompt = f"""ë‹¹ì‹ ì€ íšŒì˜ë¡ ë¶„ì„ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ íšŒì˜ë¡ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.

ê´€ë ¨ íšŒì˜ë¡ ë‚´ìš©:
{context_text}
{history_text}
ì‚¬ìš©ì ì§ˆë¬¸: {user_message}

ë‹µë³€ ì‹œ ë‹¤ìŒì„ ìœ ì˜í•´ ì£¼ì„¸ìš”:
1. ì œê³µëœ íšŒì˜ë¡ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
2. í•„ìš”í•œ ê²½ìš° í™”ìì™€ ì‹œê°„ ì •ë³´ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”.
3. ì œê³µëœ ë‚´ìš©ì— ì—†ëŠ” ê²ƒì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  "ì œê³µëœ íšŒì˜ë¡ì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
4. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."""

        logging.info(f"ğŸ¤– ì‚¬ìš©ì ì§ˆë¬¸: {user_message}")

        response = client.models.generate_content(
            model="gemini-2.5-pro",
            contents=prompt,
        )

        assistant_response = response.text.strip()
        logging.info(f"âœ… AI ì‘ë‹µ ìƒì„± ì™„ë£Œ (LangChain RAG ê¸°ë°˜)")

        chat_history.append({"user": user_message, "assistant": assistant_response})

        session_data[session_id]["chat_history"] = chat_history

        return jsonify(
            {
                "success": True,
                "response": assistant_response,
                "chat_history": chat_history,
                "search_results": len(search_results),  # ë””ë²„ê¹…ìš©
            }
        )

    except Exception as e:
        import traceback

        traceback.print_exc()
        return (
            jsonify(
                {"success": False, "error": f"ì±„íŒ… ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
            ),
            500,
        )


@app.route("/api/history", methods=["GET"])
def get_history():
    """ì²˜ë¦¬ ì´ë ¥ ì¡°íšŒ API"""
    try:
        youtube_history = load_youtube_history()
        audio_history = load_audio_history()

        # DataFrameì„ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        youtube_list = youtube_history.to_dict("records")
        audio_list = audio_history.to_dict("records")

        return jsonify(
            {
                "success": True,
                "youtube_history": youtube_list,
                "audio_history": audio_list,
                "total_youtube": len(youtube_list),
                "total_audio": len(audio_list),
            }
        )
    except Exception as e:
        logging.error(f"ì´ë ¥ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/progress/<task_id>", methods=["GET"])
def get_progress(task_id):
    """ì§„í–‰ ìƒí™© ì¡°íšŒ API"""
    try:
        if task_id not in progress_data:
            return jsonify({"success": False, "error": "ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404

        return jsonify(
            {"success": True, "task_id": task_id, "progress": progress_data[task_id]}
        )
    except Exception as e:
        logging.error(f"ì§„í–‰ ìƒí™© ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¬ ì˜ìƒ/ì˜¤ë””ì˜¤ ê²€ìƒ‰ ì—”ì§„ v0.3 (LangChain) ì‹œì‘")
    print("=" * 60)
    print("URL: http://localhost:5002")
    print("=" * 60)
    print("ê¸°ëŠ¥:")
    print("  [ì˜ìƒ ê²€ìƒ‰ ì—”ì§„]")
    print("  - YouTube ì˜ìƒ ë‹¤ìš´ë¡œë“œ (mp4 í´ë”)")
    print("  - MP3 ë³€í™˜ (mp3 í´ë”)")
    print("  - STT íšŒì˜ë¡ ìƒì„±")
    print("")
    print("  [ì˜¤ë””ì˜¤ ê²€ìƒ‰ ì—”ì§„]")
    print("  - ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ (uploads í´ë”)")
    print("  - STT íšŒì˜ë¡ ìƒì„±")
    print("")
    print("  [ê³µí†µ ê¸°ëŠ¥]")
    print("  - LangChain VectorStore (ChromaDB + Gemini Embeddings)")
    print("  - LangChain Retrieverë¡œ RAG ê²€ìƒ‰")
    print("  - íšŒì˜ë¡ ìš”ì•½")
    print("  - AI ì±„íŒ…")
    print("  - ì²˜ë¦¬ ì´ë ¥ ìºì‹± (CSV)")
    print("=" * 60)

    # VectorStore ì´ˆê¸°í™”
    initialize_vectorstores()

    app.run(host="0.0.0.0", port=5002, debug=True)
