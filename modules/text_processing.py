"""
í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ì²­í‚¹ ëª¨ë“ˆ
"""
import re
import json
import logging
from langchain_text_splitters import RecursiveCharacterTextSplitter


def merge_consecutive_speaker_segments(segments):
    """ì—°ì†ì ìœ¼ë¡œ ë™ì¼í•œ í™”ìì˜ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹©ë‹ˆë‹¤."""
    if not segments:
        return []

    merged_segments = []
    current_segment = segments[0].copy()

    for next_segment in segments[1:]:
        if current_segment["speaker"] == next_segment["speaker"]:
            current_segment["text"] += " " + next_segment["text"]
        else:
            merged_segments.append(current_segment)
            current_segment = next_segment.copy()

    merged_segments.append(current_segment)
    return merged_segments


def get_segment_from_csv(source_id, source_type, segment_id):
    """
    CSVì—ì„œ íŠ¹ì • ì„¸ê·¸ë¨¼íŠ¸ë¥¼ segment_idë¡œ ì¡°íšŒ

    Args:
        source_id: YouTube video_id ë˜ëŠ” audio file_hash
        source_type: "youtube" ë˜ëŠ” "audio"
        segment_id: ì„¸ê·¸ë¨¼íŠ¸ ID

    Returns:
        segment dict ë˜ëŠ” None
    """
    try:
        # ìˆœí™˜ import ë°©ì§€ë¥¼ ìœ„í•´ í•¨ìˆ˜ ë‚´ì—ì„œ import
        from modules.database import load_youtube_history, load_audio_history

        if source_type == "youtube":
            history_df = load_youtube_history()
            row = history_df[history_df["video_id"] == source_id]
        else:  # audio
            history_df = load_audio_history()
            row = history_df[history_df["file_hash"] == source_id]

        if row.empty:
            logging.warning(f"âš ï¸ CSVì—ì„œ source_id={source_id} ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None

        # segments_json íŒŒì‹±
        segments_json_str = row.iloc[0].get("segments_json", "[]")
        segments = json.loads(segments_json_str)

        # segment_idë¡œ ê²€ìƒ‰
        for seg in segments:
            if seg.get("id") == segment_id:
                return seg

        logging.warning(f"âš ï¸ CSVì—ì„œ segment_id={segment_id} ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return None

    except Exception as e:
        logging.error(f"âŒ CSVì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ì¡°íšŒ ì‹¤íŒ¨ (source_id={source_id}, segment_id={segment_id}): {e}")
        return None


def create_token_based_chunks(segments, chunk_size=500, chunk_overlap=100):
    """
    í† í° ê¸°ë°˜ ì²­í‚¹: í™”ì ë¶„ë¦¬ëœ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ê³ ì • í¬ê¸°ì˜ chunkë¡œ ì¬êµ¬ì„±

    Args:
        segments: ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ [{"id": 1, "speaker": "1", "start_time": 0.0, "text": "...", "confidence": 0.95}, ...]
        chunk_size: chunkë‹¹ ìµœëŒ€ ë¬¸ì ìˆ˜ (í† í° ê·¼ì‚¬ì¹˜)
        chunk_overlap: chunk ê°„ ì¤‘ë³µ ë¬¸ì ìˆ˜

    Returns:
        chunks: [{"chunk_id": 0, "text": "...", "segment_ids": [1, 2, 3], "start_time": 0.0, "end_time": 30.5, "speakers": ["1", "2"]}, ...]
    """
    try:
        if not segments:
            logging.warning("âš ï¸ create_token_based_chunks: ë¹ˆ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸")
            return []

        # ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë§ˆì»¤ì™€ í•¨ê»˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        full_text_with_markers = ""
        segment_map = {}  # ë§ˆì»¤ ìœ„ì¹˜ â†’ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ë§¤í•‘

        for seg in segments:
            marker = f"[SEG_{seg['id']}]"
            full_text_with_markers += marker + seg["text"] + " "
            segment_map[seg['id']] = seg

        # RecursiveCharacterTextSplitterë¡œ ì²­í‚¹
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", "ã€‚ ", "! ", "? ", ", ", " ", ""],  # í•œêµ­ì–´/ì˜ì–´ ë¬¸ì¥ êµ¬ë¶„ì
            length_function=len,
        )

        chunks_text = splitter.split_text(full_text_with_markers)
        logging.info(f"ğŸ“¦ ì²­í‚¹ ì™„ë£Œ: {len(chunks_text)}ê°œ chunk ìƒì„± (chunk_size={chunk_size}, overlap={chunk_overlap})")

        # chunkì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ID ì¶”ì¶œ ë° ë©”íƒ€ë°ì´í„° êµ¬ì„±
        chunks = []

        for chunk_idx, chunk_text in enumerate(chunks_text):
            # [SEG_X] ë§ˆì»¤ì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ID ì¶”ì¶œ
            seg_ids = [int(x) for x in re.findall(r'\[SEG_(\d+)\]', chunk_text)]

            if not seg_ids:
                # ë§ˆì»¤ê°€ ì—†ëŠ” ê²½ìš° (ë“œë¬¼ì§€ë§Œ ì²˜ë¦¬)
                logging.warning(f"âš ï¸ Chunk {chunk_idx}: ì„¸ê·¸ë¨¼íŠ¸ ID ì—†ìŒ")
                continue

            # ë§ˆì»¤ ì œê±°í•˜ì—¬ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            clean_text = re.sub(r'\[SEG_\d+\]', '', chunk_text).strip()

            # chunk ì‹œì‘ ë¶€ë¶„ì˜ ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì œê±° (ì²« ë²ˆì§¸ chunk ì œì™¸)
            if chunk_idx > 0:
                # ë¬¸ì¥ ë íŒ¨í„´ ì°¾ê¸°: ". ", "ã€‚ ", "! ", "? " ë‹¤ìŒë¶€í„° ì‹œì‘
                sentence_end_match = re.search(r'[.ã€‚!?]\s+', clean_text)
                if sentence_end_match:
                    # ë¬¸ì¥ ë ë‹¤ìŒë¶€í„° ì‹œì‘ (overlapìœ¼ë¡œ í¬í•¨ëœ ì´ì „ ë¬¸ì¥ ì œê±°)
                    clean_text = clean_text[sentence_end_match.end():].strip()

            # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            cited_segments = [segment_map[sid] for sid in seg_ids if sid in segment_map]

            if not cited_segments:
                logging.warning(f"âš ï¸ Chunk {chunk_idx}: ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ ì—†ìŒ")
                continue

            # ì‹œì‘/ì¢…ë£Œ ì‹œê°„ ê³„ì‚°
            start_time = min(seg["start_time"] for seg in cited_segments)

            # end_time ê³„ì‚°: ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ì˜ end_time (ì—†ìœ¼ë©´ ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ì˜ start_time)
            last_seg = cited_segments[-1]
            last_seg_id = last_seg["id"]

            # ì›ë³¸ segmentsì—ì„œ last_seg_id ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì°¾ê¸°
            last_seg_idx = next((i for i, s in enumerate(segments) if s["id"] == last_seg_id), None)
            if last_seg_idx is not None and last_seg_idx + 1 < len(segments):
                end_time = segments[last_seg_idx + 1]["start_time"]
            else:
                # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ì¸ ê²½ìš° end_timeì€ None
                end_time = None

            # í™”ì ëª©ë¡ ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
            speakers = sorted(list(set(seg["speaker"] for seg in cited_segments)))

            # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
            avg_confidence = sum(seg.get("confidence", 0.0) for seg in cited_segments) / len(cited_segments)

            chunks.append({
                "chunk_id": chunk_idx,
                "text": clean_text,
                "segment_ids": seg_ids,  # ì¸ìš©ëœ ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ID ë¦¬ìŠ¤íŠ¸
                "start_time": float(start_time),
                "end_time": float(end_time) if end_time is not None else None,
                "speakers": speakers,  # ë³µìˆ˜ í™”ì ê°€ëŠ¥
                "confidence": float(avg_confidence),
            })

        logging.info(f"âœ… ì²­í‚¹ ê²°ê³¼: {len(chunks)}ê°œ chunk, í‰ê·  ê¸¸ì´: {sum(len(c['text']) for c in chunks) / len(chunks):.0f}ì")
        return chunks

    except Exception as e:
        logging.error(f"âŒ create_token_based_chunks ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return []


def extract_citations(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ [cite: X, Y, Z] í˜•ì‹ì˜ citationì„ ì¶”ì¶œí•˜ì—¬ segment_id ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    Args:
        text: citationì´ í¬í•¨ëœ í…ìŠ¤íŠ¸

    Returns:
        list: ì¶”ì¶œëœ segment_id ë¦¬ìŠ¤íŠ¸ (ì •ìˆ˜)
    """
    # [cite: 1, 2, 3] ë˜ëŠ” [cite: 1] í˜•ì‹ì˜ citation ì°¾ê¸°
    citations = re.findall(r'\[cite:\s*(\d+(?:\s*,\s*\d+)*)\]', text)

    segment_ids = []
    for citation in citations:
        # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ segment_idë“¤ì„ ì¶”ì¶œ
        ids = [int(sid.strip()) for sid in citation.split(',')]
        segment_ids.extend(ids)

    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    segment_ids = sorted(list(set(segment_ids)))

    return segment_ids


def parse_summary_by_subtopics(summary):
    """
    ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ìš”ì•½ì„ ì†Œì£¼ì œë³„ë¡œ íŒŒì‹±í•˜ê³  citation ì •ë³´ ì¶”ì¶œ

    ë‹¤ì–‘í•œ í˜•ì‹ì„ ì§€ì›:
    1. ### ì œëª© (ë§ˆí¬ë‹¤ìš´ í—¤ë”©)
    2. **ì œëª©** (ë³¼ë“œ)
    3. ë¹ˆ ì¤„ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ì§§ì€ í…ìŠ¤íŠ¸ (ì¼ë°˜ í…ìŠ¤íŠ¸ ì œëª©)

    Args:
        summary: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ìš”ì•½ í…ìŠ¤íŠ¸

    Returns:
        list: [{"title": "ì†Œì£¼ì œ ì œëª©", "content": "ì†Œì£¼ì œ ë‚´ìš©", "cited_segment_ids": [1, 2, 3]}, ...]
    """
    if not summary or not summary.strip():
        logging.warning("âš ï¸ ìš”ì•½ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return []

    lines = summary.split("\n")
    subtopics = []
    current_title = None
    current_content = []

    logging.info(f"ğŸ“ ìš”ì•½ íŒŒì‹± ì‹œì‘ (ì´ {len(lines)}ì¤„)")

    for idx, line in enumerate(lines):
        stripped = line.strip()

        # ë‹¤ì–‘í•œ í—¤ë” í˜•ì‹ ì§€ì›
        # 1. **ì œëª©** íŒ¨í„´ (ë§ˆí¬ë‹¤ìš´ ë³¼ë“œ)
        bold_match = re.match(r"^\*\*(.+?)\*\*\s*$", stripped)
        # 2. ### ì œëª© íŒ¨í„´ (ë§ˆí¬ë‹¤ìš´ í—¤ë”© 3)
        h3_match = re.match(r"^###[\s]+(.+?)[\s]*$", stripped)
        # 3. ## ì œëª© íŒ¨í„´ (ë§ˆí¬ë‹¤ìš´ í—¤ë”© 2)
        h2_match = re.match(r"^##[\s]+(.+?)[\s]*$", stripped)
        # 4. # ì œëª© íŒ¨í„´ (ë§ˆí¬ë‹¤ìš´ í—¤ë”© 1)
        h1_match = re.match(r"^#[\s]+(.+?)[\s]*$", stripped) if len(subtopics) > 0 else None

        # 5. ì¼ë°˜ í…ìŠ¤íŠ¸ ì œëª© ê°ì§€ (íœ´ë¦¬ìŠ¤í‹±)
        is_potential_title = False
        if stripped and len(stripped) < 100 and not stripped.startswith('*') and not stripped.startswith('[cite'):
            # ì´ì „ ì¤„ê³¼ ë‹¤ìŒ ì¤„ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            prev_line_empty = (idx == 0) or (idx > 0 and not lines[idx-1].strip())
            next_line_empty = (idx == len(lines)-1) or (idx < len(lines)-1 and not lines[idx+1].strip())

            # ë¬¸ì¥ ë¶€í˜¸ ì²´í¬
            ends_with_punct = stripped.endswith(('.', ',', '!', ':', ';')) or '[cite:' in stripped[-20:]

            if prev_line_empty and next_line_empty and not ends_with_punct:
                is_potential_title = True

        # í—¤ë” ë§¤ì¹­ ìš°ì„ ìˆœìœ„: h3 > h2 > bold > h1 > ì¼ë°˜ í…ìŠ¤íŠ¸
        title_match = h3_match or h2_match or bold_match or h1_match

        if title_match or is_potential_title:
            # ì´ì „ ì†Œì£¼ì œ ì €ì¥
            if current_title is not None:
                content_str = "\n".join(current_content).strip()
                if content_str:  # ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ ì €ì¥
                    # Citation ì¶”ì¶œ ([cite: 1, 2, 3] í˜•ì‹)
                    cited_segment_ids = extract_citations(content_str)

                    subtopics.append(
                        {
                            "title": current_title,
                            "content": content_str,
                            "cited_segment_ids": cited_segment_ids,
                        }
                    )

            # ìƒˆ ì†Œì£¼ì œ ì‹œì‘
            if title_match:
                current_title = title_match.group(1).strip()
            else:
                current_title = stripped
            current_content = []

        elif current_title is not None and stripped:
            # í˜„ì¬ ì†Œì£¼ì œì˜ ë‚´ìš© ì¶”ê°€ (ë¹ˆ ì¤„ì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
            current_content.append(line)

    # ë§ˆì§€ë§‰ ì†Œì£¼ì œ ì €ì¥
    if current_title is not None:
        content_str = "\n".join(current_content).strip()
        if content_str:
            cited_segment_ids = extract_citations(content_str)
            subtopics.append({
                "title": current_title,
                "content": content_str,
                "cited_segment_ids": cited_segment_ids,
            })

    logging.info(f"âœ… íŒŒì‹± ì™„ë£Œ: {len(subtopics)}ê°œì˜ ì†Œì£¼ì œ ë°œê²¬")

    return subtopics
