"""
VectorStore ê´€ë ¨ ê¸°ëŠ¥

LangChain ChromaDBë¥¼ ì‚¬ìš©í•œ ë²¡í„° ì €ì¥ ë° ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import logging
import os
from datetime import datetime

from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from langchain_community.vectorstores.utils import filter_complex_metadata

import config
from modules.text_processing import parse_summary_by_subtopics


# =============================================================================
# ì „ì—­ ë³€ìˆ˜
# =============================================================================
embeddings = None
youtube_vectorstore = None
audio_vectorstore = None
summary_vectorstore = None


# =============================================================================
# VectorStore ì´ˆê¸°í™”
# =============================================================================
def initialize_collections():
    """LangChain VectorStore ì´ˆê¸°í™” (OpenAI Embeddings ì‚¬ìš©)"""
    global embeddings, youtube_vectorstore, audio_vectorstore, summary_vectorstore

    try:
        # OpenAI Embeddings ì´ˆê¸°í™”
        openai_api_key = os.environ.get("OPENAI_API_KEY")
        embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small", openai_api_key=openai_api_key
        )
        logging.info("âœ… OpenAI Embeddings ì‚¬ìš©")

        # YouTube VectorStore
        youtube_vectorstore = Chroma(
            collection_name="youtube_transcripts",
            embedding_function=embeddings,
            persist_directory=config.CHROMA_DB_FOLDER,
        )

        # Audio VectorStore
        audio_vectorstore = Chroma(
            collection_name="audio_transcripts",
            embedding_function=embeddings,
            persist_directory=config.CHROMA_DB_FOLDER,
        )

        # Summary VectorStore
        summary_vectorstore = Chroma(
            collection_name="summaries",
            embedding_function=embeddings,
            persist_directory=config.CHROMA_DB_FOLDER,
        )

        logging.info("âœ… LangChain VectorStore ì´ˆê¸°í™” ì™„ë£Œ")
        logging.info("   - YouTube VectorStore ì´ˆê¸°í™”ë¨")
        logging.info("   - Audio VectorStore ì´ˆê¸°í™”ë¨")
        logging.info("   - Summary VectorStore ì´ˆê¸°í™”ë¨")
    except Exception as e:
        logging.error(f"âŒ LangChain VectorStore ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


# =============================================================================
# VectorStore ì €ì¥ ê¸°ëŠ¥
# =============================================================================
def store_segments_in_vectordb(
    segments, source_id, source_type="youtube", filename=None, title=None, use_chunking=True, chunk_size=500, chunk_overlap=100
):
    """
    ì„¸ê·¸ë¨¼íŠ¸ë¥¼ VectorDBì— ì €ì¥ (LangChain ë°©ì‹)

    Args:
        segments: STTë¡œ ì¶”ì¶œëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
        source_id: YouTube video_id ë˜ëŠ” audio file_hash
        source_type: "youtube" ë˜ëŠ” "audio"
        filename: ì˜¤ë””ì˜¤ íŒŒì¼ëª… (ì˜¤ë””ì˜¤ì¼ ê²½ìš°)
        title: ì œëª© (ì‚¬ìš©ì ì…ë ¥ ë˜ëŠ” ìë™ ì¶”ì¶œ)
        use_chunking: Trueì´ë©´ í† í° ê¸°ë°˜ ì²­í‚¹ ì‚¬ìš©, Falseì´ë©´ ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥ (ê¸°ë³¸ê°’: True)
        chunk_size: ì²­í‚¹ ì‹œ chunkë‹¹ ìµœëŒ€ ë¬¸ì ìˆ˜ (ê¸°ë³¸ê°’: 500)
        chunk_overlap: ì²­í‚¹ ì‹œ chunk ê°„ ì¤‘ë³µ ë¬¸ì ìˆ˜ (ê¸°ë³¸ê°’: 100)
    """
    try:
        from modules.text_processing import create_token_based_chunks

        vectorstore = (
            youtube_vectorstore if source_type == "youtube" else audio_vectorstore
        )

        if not vectorstore:
            logging.error("âŒ LangChain VectorStoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False

        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ê°™ì€ source_id)
        try:
            # LangChain Chromaì—ì„œ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            existing_docs = vectorstore.get(where={"source_id": source_id})
            if existing_docs and existing_docs["ids"]:
                vectorstore.delete(ids=existing_docs["ids"])
                logging.info(
                    f"ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ: {len(existing_docs['ids'])}ê°œ ë¬¸ì„œ"
                )
        except Exception as e:
            logging.warning(f"ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

        # ì²­í‚¹ ì—¬ë¶€ì— ë”°ë¼ ì²˜ë¦¬
        if use_chunking:
            logging.info(f"ğŸ“¦ í† í° ê¸°ë°˜ ì²­í‚¹ ì‹œì‘ (chunk_size={chunk_size}, overlap={chunk_overlap})...")
            chunks = create_token_based_chunks(segments, chunk_size=chunk_size, chunk_overlap=chunk_overlap)

            if not chunks:
                logging.warning("âš ï¸ ì²­í‚¹ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ, ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤.")
                use_chunking = False  # í´ë°±: ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥
            else:
                # LangChain Document ê°ì²´ ìƒì„± (ì²­í¬ ê¸°ë°˜)
                documents = []
                doc_ids = []

                for chunk in chunks:
                    # Document (content)
                    text = chunk["text"]

                    # Metadata
                    metadata = {
                        "source_id": source_id,
                        "source_type": source_type,
                        "document_type": "chunk",  # ì²­í¬ì„ì„ í‘œì‹œ
                        "chunk_id": int(chunk["chunk_id"]),
                        "segment_ids": chunk["segment_ids"],  # ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ID ë¦¬ìŠ¤íŠ¸ (ë³µì¡í•œ ë©”íƒ€ë°ì´í„°)
                        "speakers": chunk["speakers"],  # í™”ì ë¦¬ìŠ¤íŠ¸ (ë³µì¡í•œ ë©”íƒ€ë°ì´í„°)
                        "start_time": float(chunk["start_time"]),
                        "end_time": float(chunk["end_time"]) if chunk["end_time"] is not None else None,
                        "confidence": float(chunk["confidence"]),
                    }

                    # ì œëª© ì¶”ê°€
                    if title:
                        metadata["title"] = title

                    if source_type == "audio" and filename:
                        metadata["filename"] = filename

                    # ID: source_id + chunk_id
                    doc_id = f"{source_id}_chunk_{chunk['chunk_id']}"
                    doc_ids.append(doc_id)

                    # LangChain Document ìƒì„±
                    doc = Document(page_content=text, metadata=metadata)
                    documents.append(doc)

                # ë³µì¡í•œ ë©”íƒ€ë°ì´í„° í•„í„°ë§ (segment_ids, speakersëŠ” ë¦¬ìŠ¤íŠ¸)
                logging.info(f"ğŸ”§ ë³µì¡í•œ ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì¤‘... (Document ìˆ˜: {len(documents)})")
                filtered_documents = filter_complex_metadata(documents)

                # LangChain VectorStoreì— ì €ì¥ (ìë™ìœ¼ë¡œ ì„ë² ë”© ìƒì„±ë¨)
                vectorstore.add_documents(
                    documents=filtered_documents,
                    ids=doc_ids,
                )

                logging.info(
                    f"âœ… VectorDB ì €ì¥ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ (ì›ë³¸ {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸, source: {source_id})"
                )
                return True

        # ì²­í‚¹ ë¯¸ì‚¬ìš© ë˜ëŠ” í´ë°±: ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥
        if not use_chunking:
            documents = []

            for idx, segment in enumerate(segments):
                # Document (content)
                text = segment["text"]

                # end_time ê³„ì‚° (ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ì˜ start_time ë˜ëŠ” None)
                if idx < len(segments) - 1:
                    end_time = float(segments[idx + 1]["start_time"])
                else:
                    # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ëŠ” end_timeì´ ì—†ìŒ (None)
                    end_time = None

                # Metadata
                metadata = {
                    "source_id": source_id,
                    "source_type": source_type,
                    "document_type": "segment",  # ëª…ì‹œì ìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ì„ì„ í‘œì‹œ
                    "speaker": str(segment["speaker"]),
                    "start_time": float(segment["start_time"]),
                    "end_time": end_time,
                    "confidence": float(segment.get("confidence", 0.0)),
                    "segment_id": int(segment["id"]),
                }

                # ì œëª© ì¶”ê°€
                if title:
                    metadata["title"] = title

                if source_type == "audio" and filename:
                    metadata["filename"] = filename

                # ID: source_id + segment_id
                doc_id = f"{source_id}_seg_{segment['id']}"

                # LangChain Document ìƒì„±
                doc = Document(page_content=text, metadata=metadata)
                documents.append(doc)

            # LangChain VectorStoreì— ì €ì¥ (ìë™ìœ¼ë¡œ ì„ë² ë”© ìƒì„±ë¨)
            vectorstore.add_documents(
                documents=documents,
                ids=[f"{source_id}_seg_{seg['id']}" for seg in segments],
            )

            logging.info(
                f"âœ… VectorDB ì €ì¥ ì™„ë£Œ: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ (source: {source_id})"
            )
            return True

    except Exception as e:
        logging.error(f"âŒ VectorDB ì €ì¥ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return False


def store_summary_in_vectordb(summary, source_id, source_type="youtube", filename=None):
    """
    ìš”ì•½ì„ ì†Œì£¼ì œë³„ë¡œ ë¶„í• í•˜ì—¬ Summary VectorDBì— ì €ì¥

    Args:
        summary: ìƒì„±ëœ ìš”ì•½ í…ìŠ¤íŠ¸ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)
        source_id: YouTube video_id ë˜ëŠ” audio file_hash
        source_type: "youtube" ë˜ëŠ” "audio"
        filename: ì˜¤ë””ì˜¤ íŒŒì¼ëª… (ì˜¤ë””ì˜¤ì¼ ê²½ìš°)
    """
    try:
        # ë””ë²„ê¹…: ì…ë ¥ íŒŒë¼ë¯¸í„° í™•ì¸
        logging.info(f"ğŸ“¥ store_summary_in_vectordb í˜¸ì¶œë¨ - source_id: {source_id}, source_type: {source_type}")
        logging.debug(f"ìš”ì•½ íƒ€ì…: {type(summary)}, ê¸¸ì´: {len(summary) if summary else 0}")
        logging.debug(f"ìš”ì•½ ë¯¸ë¦¬ë³´ê¸°: {summary[:200] if summary else 'None'}...")

        if not summary_vectorstore:
            logging.error("âŒ Summary VectorStoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False

        # ê¸°ì¡´ ìš”ì•½ ë°ì´í„° ì‚­ì œ (ê°™ì€ source_idì˜ summary)
        try:
            existing_docs = summary_vectorstore.get(where={"source_id": source_id})
            if existing_docs and existing_docs["ids"]:
                summary_vectorstore.delete(ids=existing_docs["ids"])
                logging.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ìš”ì•½ ì‚­ì œ: {len(existing_docs['ids'])}ê°œ")
        except Exception as e:
            logging.warning(f"ê¸°ì¡´ ìš”ì•½ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

        # ìš”ì•½ì„ ì†Œì£¼ì œë³„ë¡œ ë¶„í• 
        logging.info("ğŸ” ì†Œì£¼ì œ íŒŒì‹± ì‹œì‘...")
        subtopics = parse_summary_by_subtopics(summary)
        logging.info(f"ğŸ” ì†Œì£¼ì œ íŒŒì‹± ê²°ê³¼: {len(subtopics) if subtopics else 0}ê°œ")

        if not subtopics:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ì €ì¥ (fallback)
            logging.warning("âš ï¸ ì†Œì£¼ì œ íŒŒì‹± ì‹¤íŒ¨, ì „ì²´ ìš”ì•½ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ì €ì¥í•©ë‹ˆë‹¤.")
            metadata = {
                "source_id": source_id,
                "source_type": source_type,
                "document_type": "summary",
                "subtopic": "ì „ì²´",
                "subtopic_index": 0,
                "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }
            if source_type == "audio" and filename:
                metadata["filename"] = filename

            doc = Document(page_content=summary, metadata=metadata)
            doc_id = f"{source_id}_summary_0"

            # ë³µì¡í•œ ë©”íƒ€ë°ì´í„° í•„í„°ë§ (ì¼ê´€ì„±ì„ ìœ„í•´)
            filtered_docs = filter_complex_metadata([doc])
            summary_vectorstore.add_documents(documents=filtered_docs, ids=[doc_id])
            logging.info(
                f"âœ… ìš”ì•½ Summary VectorDB ì €ì¥ ì™„ë£Œ (ì „ì²´, source: {source_id})"
            )
            return True

        # ê° ì†Œì£¼ì œë¥¼ ë³„ë„ì˜ Documentë¡œ ì €ì¥
        documents = []
        doc_ids = []

        for idx, subtopic in enumerate(subtopics):
            # cited_chunk_ids ì¶”ì¶œ
            cited_chunk_ids = subtopic.get("cited_chunk_ids", [])

            metadata = {
                "source_id": source_id,
                "source_type": source_type,
                "document_type": "summary",
                "subtopic": subtopic["title"],
                "subtopic_index": idx,
                "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "cited_chunk_ids": cited_chunk_ids,  # citation ì •ë³´ ì €ì¥ (ì²­í¬ ë²ˆí˜¸)
            }

            if source_type == "audio" and filename:
                metadata["filename"] = filename

            # ì†Œì£¼ì œ ì œëª© + ë‚´ìš©ì„ í•¨ê»˜ ì €ì¥ (ê²€ìƒ‰ ì‹œ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€)
            content = f"**{subtopic['title']}**\n\n{subtopic['content']}"
            doc = Document(page_content=content, metadata=metadata)
            documents.append(doc)

            doc_id = f"{source_id}_summary_{idx}"
            doc_ids.append(doc_id)

            logging.debug(f"ğŸ“Œ ì†Œì£¼ì œ '{subtopic['title']}' - cited_chunk_ids: {cited_chunk_ids}")

        # ë³µì¡í•œ ë©”íƒ€ë°ì´í„° í•„í„°ë§ (ë¦¬ìŠ¤íŠ¸, ë”•ì…”ë„ˆë¦¬ ë“±ì„ ë¬¸ìì—´ë¡œ ë³€í™˜)
        logging.info(f"ğŸ”§ ë³µì¡í•œ ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì¤‘... (Document ìˆ˜: {len(documents)})")
        filtered_documents = filter_complex_metadata(documents)
        logging.info(f"âœ… ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì™„ë£Œ")

        # Summary VectorStoreì— ì¼ê´„ ì €ì¥
        summary_vectorstore.add_documents(documents=filtered_documents, ids=doc_ids)

        logging.info(
            f"âœ… ìš”ì•½ Summary VectorDB ì €ì¥ ì™„ë£Œ ({len(subtopics)}ê°œ ì†Œì£¼ì œ, source: {source_id})"
        )
        return True

    except Exception as e:
        logging.error(f"âŒ ìš”ì•½ VectorDB ì €ì¥ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return False


# =============================================================================
# VectorStore ì¡°íšŒ ê¸°ëŠ¥
# =============================================================================
def get_summary_from_vectordb(source_id, source_type="youtube"):
    """
    ë³„ë„ì˜ Summary VectorDBì—ì„œ ì €ì¥ëœ ìš”ì•½ ê°€ì ¸ì˜¤ê¸° (ëª¨ë“  ì†Œì£¼ì œ í¬í•¨)

    Args:
        source_id: YouTube video_id ë˜ëŠ” audio file_hash
        source_type: "youtube" ë˜ëŠ” "audio"

    Returns:
        ìš”ì•½ í…ìŠ¤íŠ¸ (ëª¨ë“  ì†Œì£¼ì œ í•©ì³ì§„ ê²ƒ) ë˜ëŠ” None (ì—†ìœ¼ë©´)
    """
    try:
        if not summary_vectorstore:
            logging.error("âŒ Summary VectorStoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None

        # ìš”ì•½ ë¬¸ì„œ ê²€ìƒ‰ (source_id ì¼ì¹˜)
        results = summary_vectorstore.get(where={"source_id": source_id})

        if results and results["documents"] and len(results["documents"]) > 0:
            # ëª¨ë“  ì†Œì£¼ì œë¥¼ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ í•©ì¹˜ê¸°
            documents = results["documents"]
            metadatas = results["metadatas"]

            # subtopic_indexë¡œ ì •ë ¬ (ì €ì¥ ìˆœì„œ ìœ ì§€)
            sorted_chunks = []
            for doc, meta in zip(documents, metadatas):
                subtopic_index = meta.get("subtopic_index", 0)
                sorted_chunks.append((subtopic_index, doc))

            sorted_chunks.sort(key=lambda x: x[0])

            # ëª¨ë“  ì†Œì£¼ì œë¥¼ í•©ì³ì„œ ë°˜í™˜
            summary = "\n\n".join([doc for _, doc in sorted_chunks])

            logging.info(
                f"âœ… Summary VectorDBì—ì„œ ìš”ì•½ ë¡œë“œ ì™„ë£Œ (source: {source_id}, {len(documents)}ê°œ ì†Œì£¼ì œ)"
            )
            return summary
        else:
            logging.info(
                f"â„¹ï¸ Summary VectorDBì— ì €ì¥ëœ ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤ (source: {source_id})"
            )
            return None

    except Exception as e:
        logging.error(f"âŒ Summary VectorDB ìš”ì•½ ë¡œë“œ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return None


def delete_from_vectorstore(source_id, source_type="youtube"):
    """
    VectorStoreì—ì„œ íŠ¹ì • source_idì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ (ì„¸ê·¸ë¨¼íŠ¸ + ìš”ì•½)

    Args:
        source_id: YouTube video_id ë˜ëŠ” audio file_hash
        source_type: "youtube" ë˜ëŠ” "audio"

    Returns:
        (ì„±ê³µ ì—¬ë¶€, ì‚­ì œëœ ë¬¸ì„œ ìˆ˜)
    """
    try:
        total_deleted = 0

        # 1. ì„¸ê·¸ë¨¼íŠ¸ ì‚­ì œ (youtube_vectorstore ë˜ëŠ” audio_vectorstore)
        vectorstore = youtube_vectorstore if source_type == "youtube" else audio_vectorstore

        if vectorstore:
            try:
                existing_docs = vectorstore.get(where={"source_id": source_id})
                if existing_docs and existing_docs["ids"]:
                    vectorstore.delete(ids=existing_docs["ids"])
                    deleted_count = len(existing_docs["ids"])
                    total_deleted += deleted_count
                    logging.info(f"ğŸ—‘ï¸ {source_type} VectorStoreì—ì„œ {deleted_count}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì‚­ì œ")
            except Exception as e:
                logging.warning(f"âš ï¸ {source_type} VectorStore ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")

        # 2. ìš”ì•½ ì‚­ì œ (summary_vectorstore)
        if summary_vectorstore:
            try:
                existing_summary = summary_vectorstore.get(where={"source_id": source_id})
                if existing_summary and existing_summary["ids"]:
                    summary_vectorstore.delete(ids=existing_summary["ids"])
                    summary_count = len(existing_summary["ids"])
                    total_deleted += summary_count
                    logging.info(f"ğŸ—‘ï¸ Summary VectorStoreì—ì„œ {summary_count}ê°œ ìš”ì•½ ì‚­ì œ")
            except Exception as e:
                logging.warning(f"âš ï¸ Summary VectorStore ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")

        logging.info(f"âœ… VectorStore ì‚­ì œ ì™„ë£Œ: ì´ {total_deleted}ê°œ ë¬¸ì„œ ì‚­ì œë¨")
        return True, total_deleted

    except Exception as e:
        logging.error(f"âŒ VectorStore ì‚­ì œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False, 0


def search_vectordb(query, source_id=None, source_type=None, n_results=5, document_type=None):
    """
    VectorDBì—ì„œ ê²€ìƒ‰ (LangChain Retriever ì‚¬ìš©)

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        source_id: íŠ¹ì • sourceë¡œ ì œí•œ (ì„ íƒ)
        source_type: "youtube", "audio", "summary" ë˜ëŠ” None (ì„ íƒ)
        n_results: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        document_type: "chunk", "segment" ë˜ëŠ” None (ì„ íƒ, chunkë§Œ ê²€ìƒ‰í•˜ë ¤ë©´ "chunk" ì§€ì •)

    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    try:
        # ê²€ìƒ‰í•  VectorStore ê²°ì •
        vectorstores_to_search = []
        if source_type == "youtube":
            vectorstores_to_search = [youtube_vectorstore]
            logging.info(f"ğŸ” ê²€ìƒ‰ ëŒ€ìƒ: YouTube VectorStoreë§Œ")
        elif source_type == "audio":
            vectorstores_to_search = [audio_vectorstore]
            logging.info(f"ğŸ” ê²€ìƒ‰ ëŒ€ìƒ: Audio VectorStoreë§Œ")
        elif source_type == "summary":
            vectorstores_to_search = [summary_vectorstore]
            logging.info(f"ğŸ” ê²€ìƒ‰ ëŒ€ìƒ: Summary VectorStoreë§Œ")
        else:
            vectorstores_to_search = [youtube_vectorstore, audio_vectorstore]
            logging.info(f"ğŸ” ê²€ìƒ‰ ëŒ€ìƒ: YouTube + Audio VectorStore (ì „ì²´ ê²€ìƒ‰)")

        all_results = []

        for idx, vectorstore in enumerate(vectorstores_to_search):
            if not vectorstore:
                logging.warning(f"âš ï¸ VectorStore #{idx}ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue

            # VectorStoreì— ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ í™•ì¸
            try:
                collection = vectorstore._collection
                total_docs = collection.count()
                logging.info(f"ğŸ“Š VectorStore #{idx} ë¬¸ì„œ ìˆ˜: {total_docs}ê°œ")
            except Exception as e:
                logging.warning(f"âš ï¸ VectorStore #{idx} ë¬¸ì„œ ìˆ˜ í™•ì¸ ì‹¤íŒ¨: {e}")

            # where í•„í„° êµ¬ì„±
            search_kwargs = {"k": n_results}
            filter_dict = {}

            if source_id:
                filter_dict["source_id"] = source_id

            if document_type:
                filter_dict["document_type"] = document_type
                logging.info(f"ğŸ“‹ document_type í•„í„°: {document_type}")

            if filter_dict:
                search_kwargs["filter"] = filter_dict

            # LangChain Retriever ìƒì„± ë° ê²€ìƒ‰
            retriever = vectorstore.as_retriever(
                search_type="similarity", search_kwargs=search_kwargs
            )

            # ê²€ìƒ‰ ìˆ˜í–‰ (similarity_search_with_score ì‚¬ìš©)
            logging.info(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{query}', k={n_results}, í•„í„°: {filter_dict if filter_dict else 'None'}")
            docs_with_scores = vectorstore.similarity_search_with_score(
                query=query, k=n_results, filter=search_kwargs.get("filter")
            )
            logging.info(
                f"âœ… VectorStore #{idx}ì—ì„œ {len(docs_with_scores)}ê°œ ê²°ê³¼ ë°œê²¬"
            )

            # ê²°ê³¼ íŒŒì‹±
            for doc, score in docs_with_scores:
                all_results.append(
                    {
                        "id": doc.metadata.get("segment_id", ""),
                        "document": doc.page_content,
                        "metadata": doc.metadata,
                        "distance": score,  # LangChainì€ ê±°ë¦¬(ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)ë¥¼ ë°˜í™˜
                    }
                )

        # ê±°ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)
        all_results.sort(key=lambda x: x.get("distance", float("inf")))

        # ìƒìœ„ n_resultsê°œë§Œ ë°˜í™˜
        return all_results[:n_results]

    except Exception as e:
        logging.error(f"âŒ VectorDB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return []


def update_title_in_vectorstore(source_id, source_type, title):
    """
    VectorStoreì˜ ì„¸ê·¸ë¨¼íŠ¸ ë©”íƒ€ë°ì´í„°ì— ì œëª© ì—…ë°ì´íŠ¸

    Args:
        source_id: YouTube video_id ë˜ëŠ” audio file_hash
        source_type: "youtube" ë˜ëŠ” "audio"
        title: ì—…ë°ì´íŠ¸í•  ì œëª©

    Returns:
        (ì„±ê³µ ì—¬ë¶€, ì—…ë°ì´íŠ¸ëœ ë¬¸ì„œ ìˆ˜)
    """
    try:
        # VectorStore ì„ íƒ
        vectorstore = (
            youtube_vectorstore if source_type == "youtube" else audio_vectorstore
        )

        if not vectorstore:
            logging.error("âŒ VectorStoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False, 0

        # í•´ë‹¹ source_idì˜ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        existing_docs = vectorstore.get(where={"source_id": source_id})

        if not existing_docs or not existing_docs["ids"]:
            logging.warning(f"âš ï¸ í•´ë‹¹ source_idì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_id}")
            return False, 0

        # ê° ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ì— title ì¶”ê°€
        # LangChain ChromaëŠ” ì§ì ‘ì ì¸ metadata ì—…ë°ì´íŠ¸ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ
        # ë‚´ë¶€ _collectionì„ ì‚¬ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸
        updated_metadatas = []
        for i in range(len(existing_docs["ids"])):
            metadata = existing_docs["metadatas"][i].copy()
            metadata["title"] = title
            updated_metadatas.append(metadata)

        # Chroma collectionì˜ update ë©”ì„œë“œ ì‚¬ìš©
        vectorstore._collection.update(
            ids=existing_docs["ids"], metadatas=updated_metadatas
        )
        updated_count = len(existing_docs["ids"])

        logging.info(
            f"âœ… ì œëª© ì—…ë°ì´íŠ¸ ì™„ë£Œ: {updated_count}ê°œ ì„¸ê·¸ë¨¼íŠ¸ (source: {source_id}, title: {title})"
        )

        return True, updated_count

    except Exception as e:
        logging.error(f"âŒ ì œëª© ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False, 0
